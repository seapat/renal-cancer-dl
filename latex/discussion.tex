%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Diskussion und Ausblick
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Discussion and Outlook}
  \label{Discussion}
  
\section{Findings}

\subsection{Single layer predictions on WSIs}

In Figure \ref{fig:1FCmodel} we see an evaluation of the expressive power of our model. On the reduced dataset, we clearly see that the model is unable to generalise well to the validation data. Although, the model does not achieve a validation loss similar to the training one, we see a clear improvement when utilizing the whole dataset. However, interesting is the behaviour in the first 50 epochs of training. In both plots, we notice an initial decrease in loss over the first 50 epochs. In figure \ref{fig:1fc_normal}, the validation loss perfectly tracks the training loss. In the overfitting experiment, we see an initial learning of the model. In general, this behaviour is to be expected, as the model has not started to 'memorise' the training data yet. However, this could be further facilitated due to the structure of the dataset and because of how the split into training and validation set is performed. Although each WSI is a different slide and also stained differently, a single patient contributes multiple images. Because of that, these images often share a similar shape. Since, we do not control whether images from one patient end up in both the training and the validation set, this could affect the performance of the network. As the network is learning with the training data, it might optimise for the corresponding images in the validation set. With that being said, we do not believe that this is the sole or the most significant contributor to the networks' performance. If that were the case, we would expect it to perform even better on the validation set, especially on the reduced dataset. If the data in the validation set were too similar, we would not be able to induce overfitting. Furthermore, we see that the overfitting stops together with the convergence of the training as expected.

\subsection{Unimodal Networks}

\subsubsection{EfficientNet versus ResNet} As seen in the results section, although the original version of EfficientNet managed to outperform ResNet, ours did not. While the concordance index might be higher on the validation set, it is not for the training data and neither are the losses better than the ResNet model. This raises the question which metric is the most relevant one and which one we should optimise for. Liao \textit{et al.} argue that a combination of characteristics should be considered. \cite{Liao2022Empirical} Hence, we decided to conduct consecutive experiments using the ResNet based approach. In the following paragraph, we discuss possible reasons for the differences in performance.
For once, we applied the exact same hyperparameters as in our most successful experiment of ResNet (learning rate 0.01, 12 gradient accumulations). Obviously, what works well with ResNet does not necessarily work well for other models. To better compare both, we would need to evaluate EfficientNet with different hyperparameters, like we did with the ResNet model. In favour of other experiments and the main aim of this work, we did not follow through on this endeavour. 
This brings us to another suitable aspect for optimisation. We modified both ResNet and EfficientNet, but not in the same manner. While we add two fully connected layers, one for feature extraction and the other for prediction, we only add one for EfficientNet and modify the last convolutional layer instead. Hence, the fully connected feature extraction and prediction happens both in the last layer simultaneously. While we could not find a significant difference in performance between using one or two fully connected layers in ResNet, this could very well be the case for EfficientNet. Furthermore, EfficientNet has less learnable parameters, but the overall number of layers is higher. As this network architecture has been thoroughly optimised for a completely different task, that being classification on small images, it might simply not be suited for our aims. Perhaps the fact that the ResNet architecture did not undergo such tunings is the reason it performs better. An interesting observation shown in figure \ref{fig:Effnet} is that towards the end of the training period, the training loss seems to be catching up with that of ResNet. It would have been interesting to see how this development behaves at later epochs. However, the authors claim that their network also converges faster, \cite{Tan2021EfficientNetV2} a finding that we could not confirm. Despite that, the training speed was indeed faster. While each ResNet experiment required about 15 hours to process the 500 epochs, the EfficientNet experiments concluded around 3 hours earlier.
The initial idea behind using EfficientNetV2 was the promise to better handle large input images. ResNet is trained to use images of 224x224 pixels, but our input is multiple magnitudes larger. Due to the average pooling layers both ResNet and EfficientNet employ, this does not pose an issue from an implementation perspective, but could very well degrade prediction performance. As the largest variant of EfficientNetV2 is designed and pre-trained for inputs of size 480x480 pixels, we assume that it would be better suited for our data, since this increases the level of detail that is directly put into the network. However, using the larger version was not possible due to the memory limitations of the hardware. The small version of EfficientNetV2 uses an input size of 384 pixels along width and height, which is still more than ResNet. But that did yield a benefit in the end.
Concluding this analysis, probably a proper hyperparameter hyper and model optimization is needed to gain useful insights on the cause of the divergence of expectation and result. 

\subsubsection{ResNet: learning rate and gradient accumulations}

We explored the effect that alterations of single hyperparameters have on the performance of the neural network. First, we lowered the learning rate by one decimal position from 0.01 to 0.001. The results of this can be seen in \ref{fig:resnet_lr}. As we can see, a lower learning rate leads to a worse concordance index and a worse overall loss. Furthermore, we see that the generalisation on the validation set is worse. The CI at \(lr=0.001\) is lower than the one for the training dataset, while the opposite is the case at \(lr=0.01\). Usually, there are two reasons why a model performs better on the validation data than on the training data. For once, we disable regularisation techniques such as dropout during validation. Doing the same for training data would also produce better results. Furthermore, the validation is always run after the full iteration on the training dataset. Hence, the full learning progress during an epoch is already reflected at validation time but can only be observed on the next epoch for the training dataset. 

Also interesting is the development of training and validation losses. We can see that at around 200 epochs, the validation loss starts stagnating for the lower learning rate. At the higher learning rate, the descent only slows down, parallel to the training loss. Hence, we see a clear divergence with the lower learning rate, which means that the generalisation of the model is getting worse, as the training loss keeps decreasing. 
What raises questions is the stagnation of the validation loss. Normally, one would expect that lower learning rates lead to a slower convergence speed, and thus are only necessary if a minimum would be overstepped otherwise. A possible reason for the validation loss may be that the parameters of the network are “stuck” at a local minimum, which the model cannot “leave” because of the small learning rate. In such cases, a higher learning rate helps to escape local minima or to avoid entering them in the first place.
The fact that the increase in accumulations does not have a meaningful effect is less surprising. Since we use a batch size of 3 and train the network using Distributed Data Parallel on 4 GPUs, we end up at an effective batch size of 144 already, which is roughly 20.4\% of our dataset. Perhaps, this is already enough information to make reasonable weight adjustments and doubling this number (effectively) does not yield a worthwhile benefit.

\subsubsection{SNN: Gene expression networks}

We see in figures \ref{fig:snn_fsz} that the number of learned features does only slightly impact the training loss, while the differences are more noticeable on the validation set. The same is true for the corresponding concordance indices. 

The effect of the dropout probability is more noticeable, as can be seen in figure \ref{fig:snn_dropout}. With a probability of 0.5, hence there is a 50\% chance that nodes are set to zero during training, the loss stays unchanged except for a short decrease in the very early epochs. On the validation set, this leads to a steady increase over time without any signs of convergence. Decreasing the probability also decreases the lowest training loss achieved within 1000 epochs. For the validation set, a probability of 0.25 leads to loss first increasing, but after around 500 epochs it plateaus briefly, after which it declines. However, as in all experiments, the loss starts to increase again towards the end of the training time. With a probability of 0.1 the early increase is way less noticeable; however, the same increase sets in earlier, at around 600 epochs, and is much steeper. That means that the network is overfitting earlier and stronger.
The concordance indices both reach high values of above 0.9, with the \(p=0.1\) leading to an almost perfect score. Furthermore, the lower the dropout rate, the faster the convergence of the CI. Even at a rate of 0.5, the CI is slowly increasing. The dropout rate seems to have less impact on the final CI, under the condition that it is low enough to facilitate learning on the training set.

However, one thing is common to all experiments. Towards the end of the training period, the validation loss increases. Hence, all models start overfitting eventually, and none of the parameters we altered seem to change that. By performing a full and extensive hyperparameter search, perhaps a setting can be found for which the model does not overfit. However, we already show that the model stops learning at all if regularisation is too high. Hence, if such a window exists, it is very small. In comparison, we were able to show that a simple linear model manages to adjust almost perfectly, at least as far as the concordance index is concerned. As deep neural networks are known to be more data hungry than conventional machine learning methods, another conclusion becomes apparent. That is, that we simply do not have enough data samples to train a complex “big data” model that generalises well on unseen data. 
However, we also show that our approach is feasible, as our model manage to reach a concordance index of almost 1.0 with the right hyperparameters. This is further supported by our findings in figure \ref{fig:snn_lr_loss}. A simpler model, with only one layer, produces worse results in the sense that the minimum loss is higher, and the overfitting starts sooner. This is the case due to our use of regularisation techniques (self-normalisation) with each layer, and hence the overall regularisation is stronger by adding more layers. We hypothesise that more available data would allow significant simplifications of the SNN model. Furthermore, we show that a learning rate of 0.00001 is necessary. Increasing it leads to a dramatic acceleration of the rise of the validation loss. Meanwhile, lowering the learning rate slows down the progress to the point that improvement in metrics is barely observable. This supports our theory that the window of optimal parameters, where no overfitting occurs, is very narrow, if it exists at all. 

\subsection{Fusion networks}

The different fusion experiments were run as described in section \ref{MetMat}. Furthermore, we used a learning rate of 0.01 and 0.00001 for the WSI and gene expression networks, respectively. As we could show that these parameters work well in the respective unimodal settings, we assume them to be good starting points for the fusion network as well. The learning rate for the remaining layers, which were the fusion layers, if applicable, and the prediction layers, was set to 0.001. The fused feature representation was processed by a single block of a fully connected layer, dropout with a rate of 0.25 and ReLU as the activation function. This was followed by a single fully connected layer performing the prediction of hazards by returning a single value.
Also due to the time constraints, we were not able to conduct an experiment that employs uses the element-wise product as its fusion method.

\subsubsection{Element-wise Summation}

In our case, the summation approach could work well, as an empty genomic vector will forward the unimodal WSI vector to the prediction. However, since we do not adjust the data after summation, we alter the range of values the vector can take. Thus, we subdivide the dataset unintentionally into two distributions, one with genomic data available and one where the fused feature vector is equal to the unimodal input of WSI features.
The results somewhat confirm our expectations, that this method indeed lets the network improve iteratively. However, especially the validation loss seems to decrease very slow. A possible explanation could be the aforementioned division of the dataset.
It would be interesting to take the mean with respect to the number of modalities preliminary to performing the prediction. However, due to the low number of genomic data, we would not expect a big difference from the unimodal image-processing. 

\subsubsection{Element-wise Maximisation}

Analogously to the summation approach, we expect this method to work rather well. If genomic information is missing. the unimodal vector from the CNN should be largely unaffected and thus return comparable results. We can see that to some degree in the results, although based on the loss, the learning seems to be slower. A reason could be the additional fully connected layer that is situated after the fusion and the prediction step within the network. Because this layer employs dropout with a probability of 0.25, learning could be slower, hopefully at the benefit of better generalisation later on. With that being said, it might make sense to adjust the learning rate and try to improve the convergence of the network.

\subsubsection{Concatenation}

The advantage that concatenation brings, is that no unimodal information is sacrificed. As all feature representations are simply appended to each other, the joint representation still contains all the raw values from each modality. This way, no bias is introduced based on the ordering of values or from stochastic selection processes. The only caveat is that the joint representation does not contain any clear separation of modalities. This can become relevant at least if the modalities differ significantly, as in our case; it might cause issues as the statistics of the respective feature representations can differ. For example, our SNN is designed to maintain zero mean and unit variance, but the CNN does not adhere to such constraints.
Despite that, the initial results look promising, as the loss was steadily decreasing, while the concordance indices both were rising synchronously. Hence, it would be worthwhile to investigate this method with a longer runtime.

\subsubsection{Kronecker Product}

In most cases, the genomic vector is set to zero due to the missing data. It can be assumed that that will push the weights of the attention based gating towards zero. As the Kronecker product is a simple multiplication of every pair of values in two vectors, of which one will often contain these values that are close to zero, the fused output most probably will not be able to provide valuable information. The results somewhat confirm our expectations. Although, the initial metrics indicate a good performance, the best in fact, the network barely learns anything from the data. Thus, the loss and concordance index stays almost unchanged over the entire run. There is a chance, that the learning rate is the cause of this behaviour, but the overall lack of data is more likely. Another reason could be the fact that we had to downsample the size of each feature representation. This loss of information did probably hamper the learning as well. The only way to overcome this issue, is by increasing the memory capacity of hardware.

\subsubsection{Attention}

Attention based methods are very attractive as they allow the networks to “decide for themselves” what they should care about, which makes the job of the researcher a bit easier. Although, this is probably not always the reality, it appears to have worked well in our case, but only partially. The loss curve shows the steepest decrease, hence the fastest learning. While this is a desirable property, we also notice that the network already starts to converge towards the end. This raises the question if it could be outperformed by the over networks over the long run. This is a good opportunity to consider the absolute values of the loss curves rather than their behaviour. As we can see, the only network that achieves comparable validation losses, is the one based on the Kronecker fusion. But, as we already established, this network fails to learn anything meaningful. 
Interesting however is that the Kronecker approach also employs an attention mechanism on its own. This might justify further investigation into attention-based methods in general.
Although the loss seems promising, the c-index does improve very slowly. As the loss is already starting to converge, it is unclear if it will reach similarly good values as the unimodal CNN approaches. This exposes the non-linear relationship between the log-likelihood loss and the concordance index, which we will discuss further in the one of the following sections.

\subsubsection{EmbraceNet}

The EmbraceNet approach showed to be the most unstable. As the sampling of values from the respective feature representations is a stochastic process, this does make sense. An improvement of this approach could be to move our approach closer to the original EmbraceNet implementation. There, docking layers perform a pre-filtering of values that is not bound to specific indices. \cite{Choi2019EmbraceNet} Hence, the network is able to pre-select valuable features and the random selection is less likely to pick counterproductive features. Despite that, we can still observe a slow improvement of metrics over time and again changing the hyperparameters may be worthwhile.

\subsubsection{Conclusion of Fusion}

Especially the maximisation, the attention and the EmbraceNet approach are impacted by the ordering of values. As each of these methods perform some form of selection with respect to a shared index, the results of the selection could vary widely if the feature vectors were ordered differently with respect to each other.
Broadly, we can conclude that the precise method of fusion is of considerable importance to the performance of the network. Unfortunately, this is not as apparent in the literature. The only publication that performs similar comparisons is the one discussing the MultiSurv architecture \cite{ValeSilva2021Long}, but even they do not discuss their findings and only report on the best performing method. 
In this section, we have shed light on the importance of this particular part of multimodal neural networks and revealed a promising aspect for future research.

\subsection{Interpretability methods}

% 001 = HE-Elastika
% 016 = CD68 (Makrophagen)
% 019 = CD204 (TAM)
% 041 = CD8_CD20
% 042 = CD4_FoxP3

The interpretability methods that we used both show interesting confirmatory results. Figure \ref{fig:case111b} shows the results of the more straightforward methods. As noted in \ref{Background}, Saliency and Input*Gradient are closely related, the only difference is the multiplication of the gradient with the input. Therefore, the respective results are generally very similar.
Interestingly, attribution is mostly focused on the borders of the tissue, furthermore, most of the regions do not belong to any tumour annotations. The only exceptions are the regions on the upper left edge of the tissue. However, all attributed regions correspond to areas that are already notable in the regular slide image. We clearly notice that attribution focuses on regions that are adjacent to tumour areas. We notice that areas with high content of H\&E stained cells have high attribution. Furthermore, the “dark spot” on the right side of the tissue seems to get more attention as well. The tissue border just below appears to have pixels of similar intensity. Then, the occlusion map does not appear to be very conclusive, as the calculated attribution inside the tissue appears to be just as high for some background patches. 

Figure \ref{fig:case111a} visualises more promising methods that were already employed for similar tasks in the past. \cite{Chen2022Pathomic, Chen2021Multimodal} For the attribution calculated using IG, we can clearly see that regions annotated with “Tumour Regression” receive high values. Moreover, we notice that regions adjacent to the tumour tissue receive noteworthy attribution and there appears to be some overlap with the saliency-based results. Noteworthy is the tumour free area in the middle of vital tumour tissue, as well as an area of high eosinophilic objects towards the lower right of the tissue. At the lower end of the tissue, there is a tumour-free region that received high attribution by IG, but only some with the saliency methods. The last employed approach is Guided GradCAM. We separated the attribution values by their sign, which was done to better visualise the positive attributions, as they are smaller than the negative ones but more interesting. This was done to maintain a consistent method for calculations of the colour bar. As there is no clear interpretation of positive or negative attribution for networks that return a single value (regression or binary classification), we chose to visualise the unsigned attributions for all other methods. 
The GradCAM results confirm our findings. Again, regions with regressed tumour receive high attention. Furthermore, the regions attended outside the tumour area are the same as in IG and the saliency map, with stronger correlation with the IG results.

Another example is case 13 which is shown in figures \ref{fig:case13a} and \ref{fig:case13b}. This case is particularly interesting as there is only one tumour annotation and hence less given information for the network to rely upon. We notice that the saliency methods mostly attribute the tissue border, in areas marked as tumour. However, there is a small area, where we can see a cluster of either CD8 or CD20 positive cells, we know the cell type thanks to the particular stain used. The occlusion map appears to have a high attribution to the same area. However, other regions show similarly high values. 
Both IG and GradCAM reveal the same and additional, similar, clusters of CD8\textsuperscript{+}/CD20\textsuperscript{+} cells. Furthermore, regions between the tumour annotations or at its border receive high attribution. This behaviour is similar to that for case 111 \ref{fig:case111a}, with the difference that these regions were marked as regressed tumours in the other case. 
Figure \ref{fig:case13c} shows the results for case 13 as well, but for a differently stained sample. This stain reveals cells positive for CD4 or FoxP3. We chose not to show the results for Input*Gradient, occlusion and the negative GradCAM attribution as they do not present new insights, similar to Figures \ref{fig:case13b} and \ref{fig:case111b}. We observe results that are analogous to the other stain. Although the attributed areas are larger for saliency and IG, they are qualitatively similar. The only notable difference is that the attribution now focuses on cell clusters corresponding to the current stain.

Figure \ref{fig:case129} shows the results for case 129 for the CD204 revealing stain. The most interesting aspect is that, especially GradCAM and IG (saliency to a lesser extent) attribute the tumour necrosis almost exclusively. This appears not always to be the case, especially when these regions are large (see figure \ref{fig:case181} and figure \ref{fig:case121} in the Appendix).

From these results, we can conclude that the networks seem to benefit greatly from the included annotations. Furthermore, we notice the differences between the stains. The network recognizes the specific cells and appears to pay attention to them. Because the attribution maps differ by stain, this suggests that utilising multiple slides per stains  adds value to the dataset. It would be worthwhile to train the network with limited access to masks and evaluate the differences in performance and attribution.
The cell clusters adjacent to the tumour clusters are also noteworthy. We already know from the MHH that the immune cells that flock near the tumour are of particular interest. We conclude that thorough analysis by a domain expert is necessary to determine which of these regions are particularly interesting for the characterisation of new biomarkers.

\subsection{Validity of the C-statistic}

As seen in Chapter \ref{Results}, we achieved quite high concordance indices for the WSI-related models on both the training and the validation data set. Although this may cause initial euphoria, caution is advised.
First, it should be remembered that this metric does not include any measure of distance between prediction and truth. Because of this, the predicted hazard could still be far from the true risk. Such a prediction will still perform well as long as the pair-wise relationship between patients holds. 
In general, our results show that the relationship between the negative log-likelihood loss and the c-index is non-linear. We see cases where the validation loss is worse, but its corresponding CI is better than the respective values for the training dataset. \ref{fig:Effnet} Or the while the validation loss deteriorates, the CI is seemingly unaffected \ref{fig:resnet_lr}. 

That being said, our results are still quite high in comparison to the literature, as these constraints apply there, too. To our knowledge, Ning \textit{et al.} achieved the best-performing model on a ccRCC dataset in 2020. They reported a C-index of 0.808 with data from a cohort of 209 patients. Their model is multimodal as well and is trained with two CNNs for histopathology and tomography images, use weighted gene co-expression network analysis for RNA-Seq expression data. \cite{Ning2020Integrative}. Their approach differs from ours substantially, as the image processing was batch based and as the RNA counts were not processed using deep learning. All these differences make a direct comparison difficult. 
While these comparisons to literature are interesting, a proper comparison would involve reproducing other approaches on our data. It would also be interesting to apply our approach to a publicly available dataset, so that we can perform better compared to previous literature.

\section{Outlook}

An obvious issue with the approach presented here is in its nature. Although the images have been heavily downsampled already, they are still quite large. This leads to a higher minimum amount of memory required when compared to a patch-based approach. In return, this means that, by upgrading the infrastructure, better results can be achievable. 
Togelius and Yannakakis argue that this path is a hopeless one, at least for academia. Currently, profit-driven companies will always have a higher budget than research institutes or universities. \cite{Togelius2023Choose} Hence, the race towards bigger models is already lost. The authors lay out multiple options where academia can compete or outshine industry. One recommendation of theirs, is to focus on questions that have less competition or deep learning models have not yet been applied to. \cite{Togelius2023Choose} While this is certainly not the case for digital medicine, we have not yet seen a fully commercialised model that reaches the popularity that generative or large language models have seen recently. While such models are surely in the works, there is still time to contribute to the state of the art or perhaps shape the direction the development will take. 

More interestingly, the authors also recommend that academia should focus on finding new, efficient, or improved solutions.  They argue that the private sector is often more concerned with scaling existing models instead of trying new approaches. \cite{Togelius2023Choose}
In the following section, we want to mention different directions that could have been taken instead or can be in the future. We want to discuss their advantages and disadvantages. 

\subsection{Stain Normalisation}

Stain normalisation is a technique that showed to improve WSI based deep learning models.
Such methods usually employ what is called colour deconvolution. These normalisation techniques require prior knowledge about the dyes used in the WSI, from which stain vectors are extracted from the images. They can then be used to obtain a superior directive for normalising WSIs. Such methods are usually called for due to variations and inconsistencies between different WSIs despite following similar standardised protocols. Inconsistencies in saturation, intensity, or brightness are usually caused by different scanners and dye concentrations, as well as by various other factors. \cite{GutierrezPerez2022StainCUT} Previous research was able to demonstrate that the application of stain normalisation methods leads to overall improvements in performance for deep neural networks. 
 \cite{Howard2021impact, Ciompi2017importance, Bejnordi2016Stain}
Despite these results, we chose not to use stain normalisation. These processes are alien to the neural network itself, but impact its behaviour and performance significantly. This leads to such normalization methods becoming a strictly necessary preprocessing step that has to be performed preliminary to the main analysis. In the scenario of diagnosis in the hospital, this only causes difficulties for the clinician.  Moreover, these methods are not without fault and also have their own limitations. \cite{Bejnordi2016Stain, Landini2021Colour} Thus, the practical benefit needs to be properly evaluated, for which, there was no time in this thesis.
In addition, these methods can require considerable computing resources and increase the time required before meaningful results can be retrieved. \cite{GutierrezPerez2022StainCUT} Likewise, they often require the choice of a target slide, for which the others are normalised. \cite{Reinhard2001Color, Macenko2009method} Then, all applications of the trained network model have to be preprocessed with respect to a single WSI that must be included in the distribution of the algorithm. Otherwise, sub-par results need to be accepted; or expensive retraining, both in terms of time and energy, is necessary. On top of that, if the method is to be adapted to another type of carcinoma, a new reference image has to be selected and again included in the distribution. Although this may not seem like a big problem for a professional software engineer, it will most likely be a considerable challenge in the field of medical research. Unfortunately, the scientific community in the medical sector has not yet caught up in terms of reproducibility and deployment strategies. Therefore, any software that does not consist of a single binary is at risk of being misused or not used at all. Probably due to a perceived lack of convenience. Luckily, there are methods being developed that do not rely on such reference images, so they have the potential to alleviate these issues in the future. \cite{GutierrezPerez2022StainCUT} 
Furthermore, since this work uses a combination of H\&E and IHC stained images, the use of multiple normalisation methods would have been necessary. One issue that this introduces is that equal performance of these methods is not ensured. Thus, if one stain normalization performs better than the other, a bias towards that subset of the data is introduced.
As described in Chapter \ref{MetMat}, the images were instead normalised using more general methods and then combined with random augmentations. This approach was chosen to stimulate the network itself to factor in and compensate for such alterations in the appearance of WSIs despite them using the same dyes. 
In summary, preprocessing WSIs in order to obtain preferable results hinders the ability of neural networks to generalize properly to real world data. The focus should be put on development of robust and more widely deployable models instead.

\subsection{Image Registration}
Another interesting preprocessing technique that we were unable to explore is image registration. This method can be described as a two-step process. First, the corresponding features or pixels between a \textit{target} and a \textit{source} image are searched. In the second step, a set of one or multiple transformations is applied to the source to bring it to the spatial location of the target.

A common procedure applies two transformations in succession. In the first step, affine transformations are applied to bring both images into the same frame. These are rigid and act on the image globally. Usually this is done by a combination of translation and rotation. This is then followed by non-rigid transforms, such as the local B-spline transformation. This second transform is performed to deal with local deformation and other deviations in specific samples, which can occur during slide preparation. \cite{Kondo2022Two}
However, before such transformations can be applied, the corresponding points across two images first have to be found. Approaches that do this either try to detect features present in both images or use the intensity values of the image. The former approach uses various methods,  Scale Invariant Feature Transform (SIFT) for example, to find distinct patterns and then uses a spatial correlation between these features to find the transformation.
The choice of a method depends on multiple factors, such as the desired resolution and the quality of the available data. \cite{Solorzano2019Whole}
Especially for WSIs, this is an active field of research, and many different and new directions are explored. Some of them also use deep learning. \cite{Solorzano2019Whole, Levy2020PathFlow, Chiaruttini2022Open, Hoque2022Whole, Kondo2022Two}

For our case, this would have been an interesting idea to pursue. Since the dataset contains multiple images of similar appearance but with different staining, we assume that high-quality registrations are possible and could serve as input to the neural network. This has some potential advantages. By registering images, we could correlate related images before providing them as input to the neural network. If we consider the different stains to be separate modalities, we can even define this as a form of early fusion. This allows us to rule out any bias between the training set and the validation set, which is not the case for our current technique. In the approach we have taken, each image is passed independently with no information provided to the network about the patient they belong to. Hence, the network is forced to predict based on a single image. As the intensities of these images and their spatial concentrations varies drastically due to the stain, the worst case scenario is that the network learns to simply ignore them in the first place. However, we want the network to incorporate this information. The stained cells are of interest as potential biomarkers; ideally, the network learns a correlation between them and the patient survival. Furthermore, correlating the images this way eases the cognitive task for the neural network.

Despite these promising aspects, we decided against implementing WSI registration as part of our preprocessing, for which have some conceptual and some very practical reasons. 
For once, most image registration methods always require a target image. For our data set, that would be one image per patient, while the other corresponding images act as sources. The first issue is that one has to decide whether always the same stain should be used or if the best performing image, in terms of registration quality, is chosen as the target on a case-by-case basis. The latter is valid, as consistency for different groups of images is not guaranteed. Even in the former case, an informed decision is still needed on which stain to choose. These necessary considerations should already make it obvious that image registration itself is another optimisation problem. The method adds additional parameters that need to be picked carefully, such as the correlation method used based on features or intensities, and the transformation strategy itself. 
Even if we manage to find a well performing registration pipeline, we probably still lose some information before being able to use the data for training. Because of that, we also need to evaluate whether image registration even improves prediction performance in practise. Moreover, the use of a target image also introduces a bias towards that image in that case. Since the source image stays unchanged, the previously mentioned loss of information will only affect the respective set of source images. 

In addition, there are technical reasons that make this approach infeasible. 
As image registration is by no means a cheap operation \cite{Levy2020PathFlow}, and we already are limited by the hardware, fusing multiple images would increase the memory to the point that we cannot load enough pictures at once to compute a loss. Even if we only supply one set of masks per stack of registered images, the dimensionality of an input will still be larger. Furthermore, the result of image registration will require extra storage space. This is something we have been trying to avoid so far. 
By using images as independent input, our data set is quite large and consists of images with strong and slight changes, which are good starting conditions to acquire a well generalising model. Registering and fusing images significantly reduces the size of the dataset, thus requiring a more elaborate generalisation strategy such as stronger random transformations. Furthermore, the increased complexity of the fused input perhaps requires a more complex model as well. The last issue is the imbalance of images available per patient. As explained in chapter \ref{MetMat}, we were forced to remove several images from the data set. The direct consequence of that is an uneven number of images per patient. For the few affected cases, a suitable solution must be found. Although we use an all-zero input for single masks, when the image did not have the annotation in question, such an approach might lead to serious disturbance of the training process. A missing mask tells the network exactly what it is supposed to learn: the image does not contain the respective annotation. However, when fusing registered images, how should the network interpret a blacked-out WSI?
On top of all these downsides is the issue of practicability. As we already discuss in the previous section, this is a serious preprocessing step that increases the burden of the potential end-user of the algorithm.

\subsection{Image compression}

As we mentioned previously, while MIL approaches are a valid strategy, incorporating an image as a whole is preferable. Since we manage to hit hardware requirements very fast with such a requirement, the next obvious step, is to downscale the image. However, by simply downsampling images using conventional methods, we lose potentially important information. Another approach would be to use deep learning to compress images, and thus make the compression learnable. 
This allows to learn a representation of the image that still contains relevant information and discards anything not important to the task of the network. A CNN does exactly that. It learns to reduce image data to a one-dimensional vector. Thanks to backpropagation, these networks do that in a way that is beneficial to the downstream task.
However, doing so in an end-to-end network makes it difficult to benefit from the potential savings, because every sample of the batch still has to fit onto memory at its original resolution. 
An interesting strategy would be to step away from the end-to-end approach in order to store learnt representation either in abundant CPU memory or directly to disk. This can be done in an unsupervised manner using transfer learning with a pre-trained network \cite{Chen2021Multimodal} or training a network specifically for the purpose of producing condensed representations. This has been done many times using autoencoders and other generative networks. \cite{Celik2021Extracting, Tellez2019Neural, Quan2022Global, Hecht2020Disentangled, Celik2021Extracting, Sun2022Deep, Roy2021Convolutional}

Because our approach is more demanding in terms of memory and because of the stacking of mask images, this becomes especially interesting. The main drawback we suffer from is the low number of samples we process per iteration (batch size). It has been shown in the past that larger batch sizes are to be preferred and allow faster convergence, i.e. the networks need fewer epochs and less time overall for convergence. \cite{Smith2017Dont}
A particular interesting technique for generating these compressed representations are Vector Quantised-Variational AutoEncoders (VQ-VAE). They differ from regular VAEs in the sense that the latent space is represented by a codebook containing discrete vectors. The outputs of the encoder are fit to the closest index in the codebook, which is continuously updated with each iteration. \cite{Oord2017Neural} Especially the improved version of this architecture has been shown to work well for compression. Here, multiple codebooks are learnt from each other, with the first being built from the input image. This allows us to capture image features at different scales, thus producing more complete compressions. \cite{Razavi2019Generating} To the best of our knowledge, this has not yet been applied to WSIs for the purpose of compression. However, there has been successful work that incorporates VQ-VAEs for compression of medical images, although for X-Ray images. \cite{Rajpurkar2017CheXNet} There is another publication that uses a VQ-VAE on WSIs for generation of new image patches. \cite{Chen2022Fasta} Their results indicate that this approach is quite promising. We assume that a good performance of the generative aspect of the model indicates a comparably good compression performance, and hypothesise that such an implementation is worth pursuing for the purpose of alle\textit{via}ting our current hardware limitations. 

An interesting alternative to image compression techniques was presented by Pinckaers \textit{et al.} By exploiting the fact that CNN consists of operations that only act locally, they achieve end-to-end training on images of any size by streaming the local data to the accelerator as it is needed. They show improvements on the dataset of the CAMELYON17 challenge and argue that their approach simplifies the necessary pre-processing compared to patch-based networks. However, they recognize the increased processing times stemming from the streaming process. Due to these limitations, they only tested their technique on images of 8192 × 8192 pixels \cite{Pinckaers2019Streaming}, which is a few magnitudes smaller than our WSIs at full resolution.

\subsection{On the value of hyperparameter optimisation}

As shown in chapter \ref{Results}, we only explored possible hyperparameters in a manner similar to that of A/B testing. In this way, we manually altered a single specific hyperparameter while keeping the others fixed to some base configuration. However, such manual modifications are not a \textit{via}ble strategy for proper optimisation. In recent years, hyperparameter optimisation became more relevant, as new achievements are often first achieved by upscaling and iterative improvements of existing models first, while fundamental changes come second. \cite{Yu2020Hyper}
Furthermore, a primitive approach as ours can be less effective and more time-consuming than exploring a larger space of hyperparameters automatically using specific algorithms that have been designed for that purpose. This is facilitated by so-called sampling and pruning strategies. 

Sampling refers to methods where the outcome of one trial decides on the parameters that shall be explored next. This allows iterative narrowing of the search space, thus allowing quicker approximation of an optimum; compared to systematic trial of possible combinations chosen from pre-defined values per parameter. \cite{Akiba2019Optuna} Examples of such algorithms are \textit{random search}, \textit{grid search}, and evolutionary strategies such as \textit{CMA-ES}. \cite{Hansen2016CMA, Feurer2019Hyperparameter} These algorithms are also referred to as black-box hyperparameter optimisation. \cite{Feurer2019Hyperparameter}
Pruning in optimisation tasks is comparable to early stopping during network training. Essentially, these methods detect when a set of parameters is unlikely to lead to an improvement of performance, and thus terminate the trial early, in order to progress to the next set of hyperparameters more quickly. \cite{Akiba2019Optuna} Such techniques are also referred to as multi-fidelity optimisation, because these methods start by using only small subsets of the data (low fidelity) to find a suitable set of parameters and increase the data size later to confirm that the parameter optimality still holds. Examples are \textit{Hyperband} and \textit{successive halving}. \cite{Feurer2019Hyperparameter}
Interestingly, pruning also refers to the optimisation of the model itself by dynamically removing nodes from the network that are not being used. \cite{Liao2022Empirical} 

Hyperparameter optimisation requires its own considerations. For once, Liao \textit{et al.} showed that optimising for a single metric, loss, or accuracy, can be suboptimal. They note that a mix might be more beneficial and that other characteristics such as convergence time or FLOPS should be considered as well. \cite{Liao2022Empirical} In addition, Hertel \textit{et al.} make the point that choosing the correct optimisation strategy is a problem on its own. \cite{Hertel2020Quantity}

Due to time constraints, we were unable to employ any automated hyperparameter optimisation algorithms. However, this certainly is the most obvious next step to try to improve the performance of our current unimodal approaches.

\subsection{Discrete time models}

As mentioned above, the use of the concordance index as the singular performance metric can be problematic. In addition to what has been mentioned before, there is still concern of the proportionality assumption of the Cox proportional hazards model. Due to this assumption, the survival curves of all patients are assumed to have the same shape and only differ by the factors of the covariates. In addition, the CPH model is actually expected to be computed over the full dataset. As SGD-inspired methods operate on batches of data, the resulting loss will always be just an approximation for the full data set. \cite{Gensheimer2019scalable} Hence, alternatives have been proposed. 
Discrete-time survival models operate by defining time intervals over the data set. Then, for each time interval, either constant or dynamic hazards can be estimated. Gensheimer \textit{et al.} propose such a model, where the baseline hazard rate is time-varying and hazards are non-proportional. The model predicts the hazard rate for each time interval separately. It can be interpreted as the hazard for the event that occurs during that specific interval. \cite{Gensheimer2019scalable} This way, a survival curve can be predicted with as many points as there are time intervals.
Another approach is proposed by Vale-Silva \textit{et al.} with MultiSurv. Follow-up times are assumed to be discrete, so that prediction is limited to the hazard of the patient observing the event in the current time interval, i.e., the event occurred by the date of the next follow-up. Their loss consists of two terms; one encourages increasing hazards with each interval for patients who died during that interval, while the second term increases the predicted survival probability for the remaining patients who survived the interval. \cite{ValeSilva2021Long}
While these methods do not fully remove the proportionality assumption, they reduce the problem to each time interval, varying the baseline and impact of the data with each interval. Thus, these models reduce the impact and lessen the chance of violations. 

A point of concern with time discrete models is the implicit division of the data. As, the prediction of hazards of later time intervals only depends on the patients that survived until the last interval. Because of this, the dataset needs to be large enough so that decent predictions are still possible for the last interval. As we only have 141 patients, this would be difficult. When adopting this approach, we would either need to define large enough intervals or accept that later intervals may only contain a few patients and thus are unreliable. As we currently obtain predictions from the perspective of time point zero, essentially using what would be a single time interval, a time-discrete model could still yield better overall performance.

Likewise, this approach requires the selection of an interval scheme. By doing so, we potentially introduce a bias towards our data set. The intervals might be less well suited when transferred to new data, hampering the transferability. Either way, the choice of time interval is an optimisation problem that can either be defined by the amount of intervals or by a fixed duration each interval should have. In general, the time-discrete approach seems to provide a potential advantage as far as prediction is concerned. The interpretability methods would need to be evaluated in terms of how well they would perform per time interval, when used on a model returning multiple unbound decimal values.  


