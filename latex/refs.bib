@Article{Kvamme2019Time,
  author           = {Kvamme, H{\aa}vard and Borgan, {\O}rnulf and Scheel, Ida},
  journal          = {Journal of Machine Learning Research},
  title            = {Time-to-event prediction with neural networks and cox regression},
  year             = {2019},
  number           = {129},
  pages            = {1--30},
  volume           = {20},
  modificationdate = {2023-04-15T18:59:24},
  url              = {http://jmlr.org/papers/v20/18-424.html},
}

@Article{Lemsara2020PathME,
  author           = {Lemsara, Amina and Ouadfel, Salima and Fr{\"o}hlich, Holger},
  journal          = {{BMC} bioinformatics},
  title            = {{PathME}: pathway based multi-modal sparse autoencoders for clustering of patient-level multi-omics data},
  year             = {2020},
  issn             = {1471-2105},
  month            = apr,
  note             = {19 citations (Crossref) [2022-11-29]},
  number           = {1},
  pages            = {146},
  volume           = {21},
  abstract         = {{BACKGROUND}: Recent years have witnessed an increasing interest in multi-omics data, because these data allow for better understanding complex diseases such as cancer on a molecular system level. In addition, multi-omics data increase the chance to robustly identify molecular patient sub-groups and hence open the door towards a better personalized treatment of diseases. Several methods have been proposed for unsupervised clustering of multi-omics data. However, a number of challenges remain, such as the magnitude of features and the large difference in dimensionality across different omics data sources.
{RESULTS}: We propose a multi-modal sparse denoising autoencoder framework coupled with sparse non-negative matrix factorization to robustly cluster patients based on multi-omics data. The proposed model specifically leverages pathway information to effectively reduce the dimensionality of omics data into a pathway and patient specific score profile. In consequence, our method allows us to understand, which pathway is a feature of which particular patient cluster. Moreover, recently proposed machine learning techniques allow us to disentangle the specific impact of each individual omics feature on a pathway score. We applied our method to cluster patients in several cancer datasets using gene expression, {miRNA} expression, {DNA} methylation and {CNVs}, demonstrating the possibility to obtain biologically plausible disease subtypes characterized by specific molecular features. Comparison against several competing methods showed a competitive clustering performance. In addition, post-hoc analysis of somatic mutations and clinical data provided supporting evidence and interpretation of the identified clusters.
{CONCLUSIONS}: Our suggested multi-modal sparse denoising autoencoder approach allows for an effective and interpretable integration of multi-omics data on pathway level while addressing the high dimensional character of omics data. Patient specific pathway score profiles derived from our model allow for a robust identification of disease subgroups.},
  doi              = {10.1186/s12859-020-3465-2},
  keywords         = {Deep learning, Humans, Algorithms, Cluster Analysis, Computational Biology, Data Analysis, Multi-omics, Neoplasms, Patient clustering},
  modificationdate = {2023-04-15T18:59:24},
  pmcid            = {PMC7161108},
  pmid             = {32299344},
  shortjournal     = {{BMC} Bioinformatics},
  shorttitle       = {{PathME}},
}

@Article{Nam2022Understanding,
  author           = {Nam, Seojin and Kim, Donghun and Jung, Woojin and Zhu, Yongjun},
  journal          = {Journal of Medical Internet Research},
  title            = {Understanding the Research Landscape of Deep Learning in Biomedical Science: Scientometric Analysis},
  year             = {2022},
  month            = apr,
  note             = {0 citations (Crossref) [2022-11-28] Company: Journal of Medical Internet Research Distributor: Journal of Medical Internet Research Institution: Journal of Medical Internet Research Label: Journal of Medical Internet Research Publisher: {JMIR} Publications Inc., Toronto, Canada},
  number           = {4},
  pages            = {e28114},
  volume           = {24},
  abstract         = {Background: Advances in biomedical research using deep learning techniques have generated a large volume of related literature. However, there is a lack of scientometric studies that provide a bird{\rq}s-eye view of them. This absence has led to a partial and fragmented understanding of the field and its progress.
Objective: This study aimed to gain a quantitative and qualitative understanding of the scientific domain by analyzing diverse bibliographic entities that represent the research landscape from multiple perspectives and levels of granularity.
Methods: We searched and retrieved 978 deep learning studies in biomedicine from the {PubMed} database. A scientometric analysis was performed by analyzing the metadata, content of influential works, and cited references.
Results: In the process, we identified the current leading fields, major research topics and techniques, knowledge diffusion, and research collaboration. There was a predominant focus on applying deep learning, especially convolutional neural networks, to radiology and medical imaging, whereas a few studies focused on protein or genome analysis. Radiology and medical imaging also appeared to be the most significant knowledge sources and an important field in knowledge diffusion, followed by computer science and electrical engineering. A coauthorship analysis revealed various collaborations among engineering-oriented and biomedicine-oriented clusters of disciplines.
Conclusions: This study investigated the landscape of deep learning research in biomedicine and confirmed its interdisciplinary nature. Although it has been successful, we believe that there is a need for diverse applications in certain areas to further boost the contributions of deep learning in addressing biomedical research problems. We expect the results of this study to help researchers and communities better align their present and future work.},
  doi              = {10.2196/28114},
  modificationdate = {2023-04-15T18:59:24},
  shorttitle       = {Understanding the Research Landscape of Deep Learning in Biomedical Science},
  url              = {https://www.jmir.org/2022/4/e28114},
  urldate          = {2022-11-28},
}

@Article{Kiemen2022CODA,
  author           = {Kiemen, Ashley L. and Braxton, Alicia M. and Grahn, Mia P. and Han, Kyu Sang and Babu, Jaanvi Mahesh and Reichel, Rebecca and Jiang, Ann C. and Kim, Bridgette and Hsu, Jocelyn and Amoa, Falone and Reddy, Sashank and Hong, Seung-Mo and Cornish, Toby C. and Thompson, Elizabeth D. and Huang, Peng and Wood, Laura D. and Hruban, Ralph H. and Wirtz, Denis and Wu, Pei-Hsun},
  journal          = {Nature Methods},
  title            = {{CODA}: quantitative 3D reconstruction of large tissues at cellular resolution},
  year             = {2022},
  issn             = {1548-7105},
  month            = nov,
  note             = {3 citations (Crossref) [2022-11-28] Number: 11 Publisher: Nature Publishing Group},
  number           = {11},
  pages            = {1490--1499},
  volume           = {19},
  abstract         = {A central challenge in biology is obtaining high-content, high-resolution information while analyzing tissue samples at volumes relevant to disease progression. We address this here with {CODA}, a method to reconstruct exceptionally large (up to multicentimeter cubed) tissues at subcellular resolution using serially sectioned hematoxylin and eosin-stained tissue sections. Here we demonstrate {CODA}{\rq}s ability to reconstruct three-dimensional (3D) distinct microanatomical structures in pancreas, skin, lung and liver tissues. {CODA} allows creation of readily quantifiable tissue volumes amenable to biological research. As a testbed, we assess the microanatomy of the human pancreas during tumorigenesis within the branching pancreatic ductal system, labeling ten distinct structures to examine heterogeneity and structural transformation during neoplastic progression. We show that pancreatic precancerous lesions develop into distinct 3D morphological phenotypes and that pancreatic cancer tends to spread far from the bulk tumor along collagen fibers that are highly aligned to the 3D curves of ductal, lobular, vascular and neural structures. Thus, {CODA} establishes a means to transform broadly the structural study of human diseases through exploration of exhaustively labeled 3D microarchitecture.},
  doi              = {10.1038/s41592-022-01650-9},
  keywords         = {Cancer imaging, Machine learning, Cancer microenvironment, Imaging},
  langid           = {english},
  modificationdate = {2023-04-15T18:59:24},
  rights           = {2022 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  shortjournal     = {Nat Methods},
  shorttitle       = {{CODA}},
  url              = {https://www.nature.com/articles/s41592-022-01650-9},
  urldate          = {2022-11-28},
}

@Article{Wang2022Survey,
  author           = {Wang, Ching-Wei and Khalil, Muhammad-Adil and Firdi, Nabila Puspita},
  journal          = {Diagnostics},
  title            = {A Survey on Deep Learning for Precision Oncology},
  year             = {2022},
  issn             = {2075-4418},
  month            = jun,
  note             = {0 citations (Crossref) [2022-11-28] Number: 6 Publisher: Multidisciplinary Digital Publishing Institute},
  number           = {6},
  pages            = {1489},
  volume           = {12},
  abstract         = {Precision oncology, which ensures optimized cancer treatment tailored to the unique biology of a patient{\rq}s disease, has rapidly developed and is of great clinical importance. Deep learning has become the main method for precision oncology. This paper summarizes the recent deep-learning approaches relevant to precision oncology and reviews over 150 articles within the last six years. First, we survey the deep-learning approaches categorized by various precision oncology tasks, including the estimation of dose distribution for treatment planning, survival analysis and risk estimation after treatment, prediction of treatment response, and patient selection for treatment planning. Secondly, we provide an overview of the studies per anatomical area, including the brain, bladder, breast, bone, cervix, esophagus, gastric, head and neck, kidneys, liver, lung, pancreas, pelvis, prostate, and rectum. Finally, we highlight the challenges and discuss potential solutions for future research directions.},
  doi              = {10.3390/diagnostics12061489},
  keywords         = {deep learning, cancer treatment, only-Image, precision oncology, review, therapy, treatment planning},
  langid           = {english},
  modificationdate = {2023-04-28T08:29:44},
  rights           = {http://creativecommons.org/licenses/by/3.0/},
  url              = {https://www.mdpi.com/2075-4418/12/6/1489},
  urldate          = {2022-11-28},
}

@Article{Kim2022Application,
  author           = {Kim, Inho and Kang, Kyungmin and Song, Youngjae and Kim, Tae-Jung},
  journal          = {Diagnostics},
  title            = {Application of Artificial Intelligence in Pathology: Trends and Challenges},
  year             = {2022},
  issn             = {2075-4418},
  month            = nov,
  note             = {0 citations (Crossref) [2022-11-28] Number: 11 Publisher: Multidisciplinary Digital Publishing Institute},
  number           = {11},
  pages            = {2794},
  volume           = {12},
  abstract         = {Given the recent success of artificial intelligence ({AI}) in computer vision applications, many pathologists anticipate that {AI} will be able to assist them in a variety of digital pathology tasks. Simultaneously, tremendous advancements in deep learning have enabled a synergy with artificial intelligence ({AI}), allowing for image-based diagnosis on the background of digital pathology. There are efforts for developing {AI}-based tools to save pathologists time and eliminate errors. Here, we describe the elements in the development of computational pathology ({CPATH}), its applicability to {AI} development, and the challenges it faces, such as algorithm validation and interpretability, computing systems, reimbursement, ethics, and regulations. Furthermore, we present an overview of novel {AI}-based approaches that could be integrated into pathology laboratory workflows.},
  doi              = {10.3390/diagnostics12112794},
  keywords         = {deep learning, artificial intelligence, computational pathology, digital pathology, Simple, histopathology image analysis},
  langid           = {english},
  modificationdate = {2023-04-15T18:59:24},
  rights           = {http://creativecommons.org/licenses/by/3.0/},
  shorttitle       = {Application of Artificial Intelligence in Pathology},
  url              = {https://www.mdpi.com/2075-4418/12/11/2794},
  urldate          = {2022-11-28},
}

@InProceedings{Zuo2022Identify,
  author           = {Zuo, Yingli and Wu, Yawen and Lu, Zixiao and Zhu, Qi and Huang, Kun and Zhang, Daoqiang and Shao, Wei},
  booktitle        = {Medical Image Computing and Computer Assisted Intervention -- {MICCAI} 2022},
  title            = {Identify Consistent Imaging Genomic Biomarkers for Characterizing the Survival-Associated Interactions Between Tumor-Infiltrating Lymphocytes and Tumors},
  year             = {2022},
  address          = {Cham},
  editor           = {Wang, Linwei and Dou, Qi and Fletcher, P. Thomas and Speidel, Stefanie and Li, Shuo},
  note             = {0 citations (Crossref) [2022-11-28]},
  pages            = {222--231},
  publisher        = {Springer Nature Switzerland},
  series           = {Lecture Notes in Computer Science},
  abstract         = {The tumor-infiltrating lymphocytes ({TILs}) and its correlation with tumors play a critical role in the development and progression of breast cancer. Existing studies have demonstrated that the combination of the whole-slide pathological images ({WSIs}) and genomic data can better characterize the immunological mechanisms of {TILs} and assess the prognostic outcome in breast cancer. However, it is still very challenging to characterize the intersections between {TILs} and tumors in {WSIs} because of their large size and heterogeneity patterns, and the high dimensional genomic data also brings difficulty for the integrative analysis with {WSIs} data. To address the above challenges, in this paper, we propose an interpretable multi-modal fusion framework, {IMGFN}, that can fuse the interaction information between {TILs} and tumors with the genomic data via an attention mechanism for prognosis predictions of breast cancer. Specifically, for {WSIs} data, we use the graph attention network (i.e., {GAT}) to describe the spatial interactions of {TILs} and tumor regions across {WSIs}. As to genomic data, we use co-expression network analysis algorithms to cluster genes into co-expressed modules followed by applying the Concrete Autoencoders to select survival-associated modules. Finally, a self-attention layer is adopted to combine both the imaging and genomic features for the prognosis prediction of breast cancer. The experimental results on The Cancer Genome Atlas({TCGA}) dataset suggest that the proposed {IMGFN} can not only achieve better prognosis results than the comparing methods but also identify consistent survival-associated imaging and genomic biomarkers correlated strongly with the interaction between {TILs} and tumors.},
  doi              = {10.1007/978-3-031-16434-7_22},
  isbn             = {978-3-031-16434-7},
  keywords         = {Breast cancer, Concrete Autoencoders, Graph attention network, Prognosis prediction, Tumor-infiltrating lymphocytes},
  langid           = {english},
  modificationdate = {2023-04-28T08:30:12},
}

@InProceedings{Lu2022mathrm,
  author           = {Lu, Zilin and Lu, Mengkang and Xia, Yong},
  booktitle        = {Multiscale Multimodal Medical Imaging},
  title            = {\$\${\textbackslash}mathrm \{M{\textasciicircum}\{2\}F\}\$\$: A Multi-modal and Multi-task Fusion Network for Glioma Diagnosis and Prognosis},
  year             = {2022},
  address          = {Cham},
  editor           = {Li, Xiang and Lv, Jinglei and Huo, Yuankai and Dong, Bin and Leahy, Richard M. and Li, Quanzheng},
  note             = {0 citations (Crossref) [2022-11-28]},
  pages            = {1--10},
  publisher        = {Springer Nature Switzerland},
  series           = {Lecture Notes in Computer Science},
  abstract         = {Clinical decision of oncology comes from multi-modal information, such as morphological information from histopathology and molecular profiles from genomics. Most of the existing multi-modal learning models achieve better performance than single-modal models. However, these multi-modal models only focus on the interactive information between modalities, which ignore the internal relationship between multiple tasks. Both survival analysis task and tumor grading task can provide reliable information for pathologists in the diagnosis and prognosis of cancer. In this work, we present a Multi-modal and Multi-task Fusion (\$\${\textbackslash}mathrm \{M{\textasciicircum}\{2\}F\}\$\$M2F) model to make use of the potential connection between modalities and tasks. The co-attention module in multi-modal transformer extractor can excavate the intrinsic information between modalities more effectively than the original fusion methods. Joint training of tumor grading branch and survival analysis branch, instead of separating them, can make full use of the complementary information between tasks to improve the performance of the model. We validate our \$\${\textbackslash}mathrm \{M{\textasciicircum}\{2\}F\}\$\$M2Fmodel on glioma datasets from the Cancer Genome Atlas ({TCGA}). Experiment results show our \$\${\textbackslash}mathrm \{M{\textasciicircum}\{2\}F\}\$\$M2Fmodel is superior to existing multi-modal models, which proves the effectiveness of our model.},
  doi              = {10.1007/978-3-031-18814-5_1},
  isbn             = {978-3-031-18814-5},
  keywords         = {Survival analysis, attention, Multi-modal learning, Multi-task, Tumor grading},
  langid           = {english},
  modificationdate = {2023-04-15T18:59:24},
  shorttitle       = {\$\${\textbackslash}mathrm \{M{\textasciicircum}\{2\}F\}\$\$},
}

@Article{Ren2018Recurrence,
  author           = {Ren, Jian and Karagoz, Kubra and Gatza, Michael L. and Singer, Eric A. and Sadimin, Evita and Foran, David J. and Qi, Xin},
  journal          = {Journal of Medical Imaging},
  title            = {Recurrence analysis on prostate cancer patients with Gleason score 7 using integrated histopathology whole-slide images and genomic data through deep neural networks},
  year             = {2018},
  issn             = {2329-4302},
  month            = oct,
  note             = {5 citations (Crossref) [2022-11-15] Publisher: {SPIE}},
  number           = {4},
  pages            = {047501},
  volume           = {5},
  doi              = {10.1117/1.JMI.5.4.047501},
  keywords         = {look for followup},
  modificationdate = {2023-04-15T18:59:25},
  shortjournal     = {Journal of Medical Imaging},
  url              = {https://rutgers-staging.pure.elsevier.com/en/publications/recurrence-analysis-on-prostate-cancer-patients-with-gleason-scor},
  urldate          = {2022-11-15},
}

@Article{Pei2021Deep,
  author           = {Pei, Linmin and Jones, Karra A. and Shboul, Zeina A. and Chen, James Y. and Iftekharuddin, Khan M.},
  journal          = {Frontiers in Oncology},
  title            = {Deep Neural Network Analysis of Pathology Images With Integrated Molecular Data for Enhanced Glioma Classification and Grading},
  year             = {2021},
  issn             = {2234-943X},
  volume           = {11},
  abstract         = {Gliomas are primary brain tumors that originate from glial cells. Classification and grading of these tumors is critical to prognosis and treatment planning. The current criteria for glioma classification in central nervous system ({CNS}) was introduced by World Health Organization ({WHO}) in 2016. This criteria for glioma classification requires the integration of histology with genomics. In 2017, the Consortium to Inform Molecular and Practical Approaches to {CNS} Tumor Taxonomy ({cIMPACT}-{NOW}) was established to provide up-to-date recommendations for {CNS} tumor classification, which in turn the {WHO} is expected to adopt in its upcoming edition. In this work, we propose a novel glioma analytical method that, for the first time in the literature, integrates a cellularity feature derived from the digital analysis of brain histopathology images integrated with molecular features following the latest {WHO} criteria. We first propose a novel over-segmentation strategy for region-of-interest ({ROI}) selection in large histopathology whole slide images ({WSIs}). A Deep Neural Network ({DNN})-based classification method then fuses molecular features with cellularity features to improve tumor classification performance. We evaluate the proposed method with 549 patient cases from The Cancer Genome Atlas ({TCGA}) dataset for evaluation. The cross validated classification accuracies are 93.81\% for lower-grade glioma ({LGG}) and high-grade glioma ({HGG}) using a regular {DNN}, and 73.95\% for {LGG} {II} and {LGG} {III} using a residual neural network ({ResNet}) {DNN}, respectively. Our experiments suggest that the type of deep learning has a significant impact on tumor subtype discrimination between {LGG} {II} vs. {LGG} {III}. These results outperform state-of-the-art methods in classifying {LGG} {II} vs. {LGG} {III} and offer competitive performance in distinguishing {LGG} vs. {HGG} in the literature. In addition, we also investigate molecular subtype classification using pathology images and cellularity information. Finally, for the first time in literature this work shows promise for cellularity quantification to predict brain tumor grading for {LGGs} with {IDH} mutations.},
  doi              = {10.3389/fonc.2021.668694},
  modificationdate = {2023-04-15T18:59:24},
  urldate          = {2022-11-08},
}

@Report{karthicsonia2022Automatic,
  abstract         = {Abstract
          Osteosarcoma is the most frequent primary malignant bone tumour. Computer-aided detection ({CAD}) and diagnosis are being used to enhance osteosarcoma detection and diagnosis . The use of machine learning and deep learning algorithms may save up surgeons' time while also improving patient outcomes. An enormous quantity of data must be fed into the classifier for it to become more accurate. Adapted to a public dataset of osteosarcoma histology pictures, a mix of machine and deep learning is used in this work to distinguish between necrotic and healthy tissue images. First, the dataset was preprocessed, and contour based threshold segmentation techniques are applied. Then, Stochastic linear embedding based Feature extraction is used for extracting the abnormal features. Finally, the proposed multilayer grid {XG} Boost classifier is trained on stained images in order to increase the output accuracy. The experimental findings indicate that the proposed classifier has the greatest accuracy of any illness classification approach currently available. Our finetuned model showed superior performance in identifying osteosarcoma malignancy using H and E stained pictures.},
  author           = {karthicsonia, B. and Vanitha, M.},
  doi              = {10.21203/rs.3.rs-1360315/v1},
  modificationdate = {2023-04-15T18:59:24},
  month            = feb,
  school           = {In Review},
  title            = {Automatic Osteosarcoma Classification System Based Multilayer Grid {XG} Boost Architecture},
  type             = {preprint},
  url              = {https://www.researchsquare.com/article/rs-1360315/v1},
  urldate          = {2022-11-08},
  year             = {2022},
}

@Article{Rajpurkar2021Deep,
  author           = {Rajpurkar, Aparna R. and Mateo, Leslie J. and Murphy, Sedona E. and Boettiger, Alistair N.},
  journal          = {Nature Communications},
  title            = {Deep learning connects {DNA} traces to transcription to reveal predictive features beyond enhancer--promoter contact},
  year             = {2021},
  issn             = {2041-1723},
  month            = jun,
  note             = {Number: 1 Publisher: Nature Publishing Group},
  number           = {1},
  pages            = {3423},
  volume           = {12},
  abstract         = {Chromatin architecture plays an important role in gene regulation. Recent advances in super-resolution microscopy have made it possible to measure chromatin 3D structure and transcription in thousands of single cells. However, leveraging these complex data sets with a computationally unbiased method has been challenging. Here, we present a deep learning-based approach to better understand to what degree chromatin structure relates to transcriptional state of individual cells. Furthermore, we explore methods to ``unpack the black box'' to determine in an unbiased manner which structural features of chromatin regulation are most important for gene expression state. We apply this approach to an Optical Reconstruction of Chromatin Architecture dataset of the Bithorax gene cluster in Drosophila and show it outperforms previous contact-focused methods in predicting expression state from 3D structure. We find the structural information is distributed across the domain, overlapping and extending beyond domains identified by prior genetic analyses. Individual enhancer-promoter interactions are a minor contributor to predictions of activity.},
  doi              = {10.1038/s41467-021-23831-4},
  keywords         = {Machine learning, Data mining, Image processing, Nuclear organization, Super-resolution microscopy},
  langid           = {english},
  modificationdate = {2023-04-15T18:59:25},
  rights           = {2021 The Author(s)},
  shortjournal     = {Nat Commun},
  url              = {https://www.nature.com/articles/s41467-021-23831-4},
  urldate          = {2022-11-08},
}

@Article{Lu2021BrcaSeg,
  author           = {Lu, Zixiao and Zhan, Xiaohui and Wu, Yi and Cheng, Jun and Shao, Wei and Ni, Dong and Han, Zhi and Zhang, Jie and Feng, Qianjin and Huang, Kun},
  journal          = {Genomics, Proteomics \& Bioinformatics},
  title            = {{BrcaSeg}: A Deep Learning Approach for Tissue Quantification and Genomic Correlations of Histopathological Images},
  year             = {2021},
  issn             = {1672-0229},
  month            = dec,
  number           = {6},
  pages            = {1032--1042},
  volume           = {19},
  abstract         = {Epithelial and stromal tissues are components of the tumor microenvironment and play a major role in tumor initiation and progression. Distinguishing stroma from epithelial tissues is critically important for spatial characterization of the tumor microenvironment. Here, we propose {BrcaSeg}, an image analysis pipeline based on a convolutional neural network ({CNN}) model to classify epithelial and stromal regions in whole-slide hematoxylin and eosin (H\&E) stained histopathological images. The {CNN} model is trained using well-annotated breast cancer tissue microarrays and validated with images from The Cancer Genome Atlas ({TCGA}) Program. {BrcaSeg} achieves a classification accuracy of 91.02\%, which outperforms other state-of-the-art methods. Using this model, we generate pixel-level epithelial/stromal tissue maps for 1000 {TCGA} breast cancer slide images that are paired with gene expression data. We subsequently estimate the epithelial and stromal ratios and perform correlation analysis to model the relationship between gene expression and tissue ratios. Gene Ontology ({GO}) enrichment analyses of genes that are highly correlated with tissue ratios suggest that the same tissue is associated with similar biological processes in different breast cancer subtypes, whereas each subtype also has its own idiosyncratic biological processes governing the development of these tissues. Taken all together, our approach can lead to new insights in exploring relationships between image-based phenotypes and their underlying genomic events and biological processes for all types of solid tumors. {BrcaSeg} can be accessed at https://github.com/Serian1992/{ImgBio}.},
  doi              = {10.1016/j.gpb.2020.06.026},
  keywords         = {Deep learning, Breast cancer, Computational pathology, Integrative genomics, Whole-slide tissue image},
  langid           = {english},
  modificationdate = {2023-04-15T18:59:24},
  shortjournal     = {Genomics, Proteomics \& Bioinformatics},
  shorttitle       = {{BrcaSeg}},
  url              = {https://www.sciencedirect.com/science/article/pii/S1672022921001522},
  urldate          = {2022-11-08},
}

@Book{Klein2003Survival,
  author           = {Klein, John P. and Moeschberger, Melvin L.},
  publisher        = {Springer},
  title            = {Survival analysis: techniques for censored and truncated data},
  year             = {2003},
  volume           = {1230},
  localfile        = {John P. Klein, Melvin L. Moeschberger, Survival Analysis - Techniques for Censored and Truncated Data (2005).pdf},
  modificationdate = {2023-04-13T22:01:18},
  x-fetchedfrom    = {Google Scholar},
}

@Article{Cox1972Regression,
  author           = {D. R. Cox},
  journal          = {Journal of the Royal Statistical Society. Series B (Methodological)},
  title            = {Regression Models and Life-Tables},
  year             = {1972},
  number           = {2},
  pages            = {187--220},
  volume           = {34},
  abstract         = {The analysis of censored failure times is considered. It is assumed that on each individual are available values of one or more explanatory variables. The hazard function (age-specific failure rate) is taken to be a function of the explanatory variables and unknown regression coefficients multiplied by an arbitrary and unknown function of time. A conditional likelihood is obtained, leading to inferences about the unknown regression coefficients. Some generalizations are outlined.},
  modificationdate = {2023-04-15T18:54:45},
  publisher        = {[Royal Statistical Society, Wiley]},
  url              = {http://www.jstor.org/stable/2985181},
  urldate          = {2023-04-12},
}

@Article{Izmailov2018Averaging,
  author           = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  journal          = {arXiv preprint arXiv:1803.05407},
  title            = {Averaging weights leads to wider optima and better generalization},
  year             = {2018},
  eprint           = {1803.05407},
  modificationdate = {2023-04-15T18:57:05},
  x-fetchedfrom    = {Google Scholar},
}

@Article{Duchi2011Adaptive,
  author           = {Duchi, John and Hazan, Elad and Singer, Yoram},
  journal          = {J. Mach. Learn. Res.},
  title            = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  year             = {2011},
  issn             = {1532-4435},
  month            = jul,
  number           = {null},
  pages            = {2121--2159},
  volume           = {12},
  abstract         = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
  issue_date       = {2/1/2011},
  modificationdate = {2023-04-13T22:01:18},
  numpages         = {39},
  publisher        = {JMLR.org},
}

@Article{Hinton2012RMSProp,
  author           = {Hinton, G. and Tieleman, T.},
  title            = {RMSProp, COURSERA: Neural Networks for Machine Learning},
  year             = {2012},
  modificationdate = {2023-04-13T22:01:18},
  type             = {Technical report},
}

@Article{Kleinbaum2012Cox,
  author           = {Kleinbaum, David G. and Klein, Mitchel and Kleinbaum, David G. and Klein, Mitchel},
  journal          = {Survival analysis: a self-learning text},
  title            = {The Cox proportional hazards model and its characteristics},
  year             = {2012},
  pages            = {97--159},
  modificationdate = {2023-04-13T22:01:18},
  publisher        = {Springer},
  x-fetchedfrom    = {Google Scholar},
}

@Manual{Aldus1992TIFF,
  title            = {TIFF File Specification},
  author           = {Aldus, Developers Desk},
  edition          = {Revision 6.0},
  month            = jun,
  year             = {1992},
  abstract         = {This document describes TIFF, a tag-based file format for storing and interchanging raster images.},
  journal          = {Technical Manual},
  localfile        = {tiff6.pdf},
  modificationdate = {2023-04-16T00:12:39},
  publisher        = {Aldus Corporation},
  url              = {https://www.itu.int/itudoc/itu-t/com16/tiff-fx/docs/tiff6.pdf},
}

@Manual{OpenMicroscopyEnvironment2022OME,
  title            = {OME-TIFF specification},
  author           = {Open Microscopy Environment, The},
  edition          = {2018},
  month            = may,
  note             = {https://docs.openmicroscopy.org/ome-model/5.6.3/index.html\#},
  year             = {2022},
  booktitle        = {OME Data Model and File Formats 5.6.3 documentation},
  journal          = {Technical Documentation},
  modificationdate = {2023-04-16T00:12:01},
  publisher        = {The Open Microscopy Environment},
  url              = {https://docs.openmicroscopy.org/ome-model/6.3.1/ome-tiff/specification.html},
  urldate          = {2023-04-15},
}

@InProceedings{Loshchilov2017Decoupled,
  author           = {Loshchilov, Ilya and Hutter, Frank},
  booktitle        = {ICLR (Poster)},
  title            = {Decoupled Weight Decay Regularization.},
  year             = {2019},
  publisher        = {OpenReview.net},
  added-at         = {2019-07-25T00:00:00.000+0200},
  interhash        = {3ecf6f0fa41db2b64213a90038dfb9e3},
  intrahash        = {fdfb79ea341d185e3f9a1ad92d4ca5a3},
  modificationdate = {2023-04-18T08:09:22},
  timestamp        = {2019-07-26T11:39:47.000+0200},
  url              = {https://openreview.net/forum?id=Bkg6RiCqY7; https://www.bibsonomy.org/bibtex/2fdfb79ea341d185e3f9a1ad92d4ca5a3/dblp},
  x-fetchedfrom    = {Bibsonomy},
}

@Article{Chen2022Fast,
  author           = {Chen, Chengkuan and Lu, Ming Y. and Williamson, Drew F. K. and Chen, Tiffany Y. and Schaumberg, Andrew J. and Mahmood, Faisal},
  journal          = {Nature Biomedical Engineering},
  title            = {Fast and scalable search of whole-slide images via self-supervised deep learning},
  year             = {2022},
  number           = {12},
  pages            = {1420--1434},
  volume           = {6},
  modificationdate = {2023-04-13T22:01:18},
  publisher        = {Nature Publishing Group UK London},
  x-fetchedfrom    = {Google Scholar},
}

@Article{Kingma2019Introduction,
  author           = {Kingma, Diederik P. and Welling, Max},
  journal          = {Foundations and Trends in Machine Learning},
  title            = {An Introduction to Variational Autoencoders.},
  year             = {2019},
  number           = {4},
  pages            = {307--392},
  volume           = {12},
  added-at         = {2019-12-16T00:00:00.000+0100},
  biburl           = {https://www.bibsonomy.org/bibtex/229757d2cf614686777ddef2bc70fb38a/dblp},
  doi              = {10.1561/2200000056},
  eprint           = {1906.02691},
  interhash        = {b487efb36758d1cec7b53dec08a17d78},
  intrahash        = {29757d2cf614686777ddef2bc70fb38a},
  modificationdate = {2023-04-15T18:59:24},
  timestamp        = {2019-12-17T11:40:09.000+0100},
  x-fetchedfrom    = {Bibsonomy},
}

@Article{Lipkova2022Artificial,
  author           = {Lipkova, Jana and Chen, Richard J. and Chen, Bowen and Lu, Ming Y. and Barbieri, Matteo and Shao, Daniel and Vaidya, Anurag J. and Chen, Chengkuan and Zhuang, Luoting and Williamson, Drew F. K. and Shaban, Muhammad and Chen, Tiffany Y. and Mahmood, Faisal},
  journal          = {{Cancer Cell}},
  title            = {Artificial intelligence for multimodal data integration in oncology.},
  year             = {2022},
  month            = oct,
  number           = {10},
  pages            = {1095--1110},
  volume           = {40},
  abstract         = {In oncology, the patient state is characterized by a whole spectrum of modalities, ranging from radiology, histology, and genomics to electronic health records. Current artificial intelligence (AI) models operate mainly in the realm of a single modality, neglecting the broader clinical context, which inevitably diminishes their potential. Integration of different data modalities provides opportunities to increase robustness and accuracy of diagnostic and prognostic models, bringing AI closer to clinical practice. AI models are also capable of discovering novel patterns within and across modalities suitable for explaining differences in patient outcomes or treatment resistance. The insights gleaned from such models can guide exploration studies and contribute to the discovery of novel biomarkers and therapeutic targets. To support these advances, here we present a synopsis of AI methods and strategies for multimodal data fusion and association discovery. We outline approaches for AI interpretability and directions for AI-driven exploration through multimodal data interconnections. We examine challenges in clinical adoption and discuss emerging solutions.},
  doi              = {10.1016/j.ccell.2022.09.012},
  modificationdate = {2023-04-13T22:01:18},
  nlmuniqueid      = {101130617},
  pii              = {S1535-6108(22)00441-X},
  pubmed           = {36220072},
  x-fetchedfrom    = {PubMed},
}

@Article{Katzman2018DeepSurv,
  author           = {Katzman, Jared L. and Shaham, Uri and Cloninger, Alexander and Bates, Jonathan and Jiang, Tingting and Kluger, Yuval},
  journal          = {{BMC Med Res Methodol}},
  title            = {DeepSurv: personalized treatment recommender system using a Cox proportional hazards deep neural network.},
  year             = {2018},
  month            = feb,
  number           = {1},
  pages            = {24},
  volume           = {18},
  abstract         = {Medical practitioners use survival models to explore and understand the relationships between patients' covariates (e.g. clinical and genetic features) and the effectiveness of various treatment options. Standard survival models like the linear Cox proportional hazards model require extensive feature engineering or prior medical knowledge to model treatment interaction at an individual level. While nonlinear survival methods, such as neural networks and survival forests, can inherently model these high-level interaction terms, they have yet to be shown as effective treatment recommender systems. We introduce DeepSurv, a Cox proportional hazards deep neural network and state-of-the-art survival method for modeling interactions between a patient's covariates and treatment effectiveness in order to provide personalized treatment recommendations. We perform a number of experiments training DeepSurv on simulated and real survival data. We demonstrate that DeepSurv performs as well as or better than other state-of-the-art survival models and validate that DeepSurv successfully models increasingly complex relationships between a patient's covariates and their risk of failure. We then show how DeepSurv models the relationship between a patient's features and effectiveness of different treatment options to show how DeepSurv can be used to provide individual treatment recommendations. Finally, we train DeepSurv on real clinical studies to demonstrate how it's personalized treatment recommendations would increase the survival time of a set of patients. The predictive and modeling capabilities of DeepSurv will enable medical researchers to use deep neural networks as a tool in their exploration, understanding, and prediction of the effects of a patient's characteristics on their risk of failure.},
  doi              = {10.1186/s12874-018-0482-1},
  modificationdate = {2023-04-13T22:01:18},
  nlmuniqueid      = {100968545},
  pii              = {10.1186/s12874-018-0482-1},
  pmc              = {PMC5828433},
  pubmed           = {29482517},
  x-fetchedfrom    = {PubMed},
}

@InProceedings{Lee2018DeepHit,
  author           = {Lee, Changhee and Zame, William R. and Yoon, Jinsung and van der Schaar, Mihaela},
  booktitle        = {AAAI},
  title            = {DeepHit: A Deep Learning Approach to Survival Analysis With Competing Risks.},
  year             = {2018},
  editor           = {McIlraith, Sheila A. and Weinberger, Kilian Q.},
  pages            = {2314--2321},
  publisher        = {AAAI Press},
  added-at         = {2018-10-22T00:00:00.000+0200},
  biburl           = {https://www.bibsonomy.org/bibtex/2a82f4b87a4225ab28156311a368f8da0/dblp},
  ee               = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16160},
  interhash        = {8738768a51827edd1c8b6584b4af173b},
  intrahash        = {a82f4b87a4225ab28156311a368f8da0},
  modificationdate = {2023-04-14T23:04:44},
  timestamp        = {2018-10-24T11:38:24.000+0200},
  x-fetchedfrom    = {Bibsonomy},
}

@Article{LopezBeltran20062004,
  author           = {Lopez-Beltran, Antonio and Scarpelli, Marina and Montironi, Rodolfo and Kirkali, Ziya},
  journal          = {Eur. Urol.},
  title            = {2004 WHO Classification of the Renal Tumors of the Adults},
  year             = {2006},
  issn             = {0302-2838},
  month            = may,
  number           = {5},
  pages            = {798--805},
  volume           = {49},
  doi              = {10.1016/j.eururo.2005.11.035},
  modificationdate = {2023-04-13T22:01:18},
  publisher        = {Elsevier},
}

@Article{Hsieh2017Renal,
  author           = {Hsieh, James J. and Purdue, Mark P. and Signoretti, Sabina and Swanton, Charles and Albiges, Laurence and Schmidinger, Manuela and Heng, Daniel Y. and Larkin, James and Ficarra, Vincenzo},
  journal          = {Nat. Rev. Dis. Primers},
  title            = {Renal cell carcinoma},
  year             = {2017},
  issn             = {2056-676X},
  month            = mar,
  number           = {17009},
  pages            = {1--19},
  volume           = {3},
  doi              = {10.1038/nrdp.2017.9},
  modificationdate = {2023-04-13T22:01:18},
  publisher        = {Nature Publishing Group},
}

@InCollection{Padala2022Clear,
  author           = {Padala, Sandeep A. and Kallam, Avyakta},
  booktitle        = {StatPearls [Internet]},
  publisher        = {StatPearls Publishing},
  title            = {Clear Cell Renal Carcinoma},
  year             = {2022},
  month            = may,
  modificationdate = {2023-04-13T22:01:18},
  url              = {https://www.ncbi.nlm.nih.gov/books/NBK557644},
}

@Article{Reinhard2001Color,
  author           = {Reinhard, E. and Adhikhmin, M. and Gooch, B. and Shirley, P.},
  journal          = {IEEE Computer Graphics and Applications},
  title            = {Color transfer between images},
  year             = {2001},
  number           = {5},
  pages            = {34--41},
  volume           = {21},
  doi              = {10.1109/38.946629},
  modificationdate = {2023-04-13T22:01:18},
}

@Article{Landini2021Colour,
  author           = {Landini, Gabriel and Martinelli, Giovanni and Piccinini, Filippo},
  journal          = {Bioinformatics},
  title            = {Colour deconvolution: stain unmixing in histological imaging},
  year             = {2021},
  issn             = {1367-4803},
  month            = may,
  number           = {10},
  pages            = {1485--1487},
  volume           = {37},
  doi              = {10.1093/bioinformatics/btaa847},
  modificationdate = {2023-04-13T22:01:18},
  publisher        = {Oxford Academic},
}

@Article{Howard2021impact,
  author           = {Howard, Frederick M. and Dolezal, James and Kochanny, Sara and Schulte, Jefree and Chen, Heather and Heij, Lara and Huo, Dezheng and Nanda, Rita and Olopade, Olufunmilayo I. and Kather, Jakob N. and Cipriani, Nicole and Grossman, Robert L. and Pearson, Alexander T.},
  journal          = {Nat. Commun.},
  title            = {The impact of site-specific digital histology signatures on deep learning model accuracy and bias},
  year             = {2021},
  issn             = {2041-1723},
  month            = jul,
  number           = {4423},
  pages            = {1--13},
  volume           = {12},
  doi              = {10.1038/s41467-021-24698-1},
  modificationdate = {2023-04-13T22:01:18},
  publisher        = {Nature Publishing Group},
}

@InBook{Macenko2009method,
  author           = {Macenko, Marc and Niethammer, Marc and Marron, J. and Borland, David and Woosley, John and Guan, Xiaojun and Schmitt, Charles and Thomas, Nancy},
  publisher        = {IEEE},
  title            = {A method for normalizing histology slides for quantitative analysis},
  year             = {2009},
  month            = jun,
  booktitle        = {2009 IEEE International Symposium on Biomedical Imaging: From Nano to Macro},
  doi              = {10.1109/isbi.2009.5193250},
  modificationdate = {2023-04-15T18:59:24},
}

@Misc{Kingma2014Adam,
  author           = {Kingma, Diederik P. and Ba, Jimmy},
  title            = {Adam: A Method for Stochastic Optimization},
  year             = {2014},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  doi              = {10.48550/ARXIV.1412.6980},
  keywords         = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  modificationdate = {2023-04-15T18:59:24},
  publisher        = {arXiv},
}

@Article{Yang2022FastCPH,
  author           = {Yang, Xuelin and Abraham, Louis and Kim, Sejin and Smirnov, Petr and Ruan, Feng and Haibe-Kains, Benjamin and Tibshirani, Robert},
  journal          = {arXiv preprint arXiv:2208.09793},
  title            = {FastCPH: Efficient Survival Analysis for Neural Networks},
  year             = {2022},
  month            = aug,
  abstract         = {The Cox proportional hazards model is a canonical method in survival analysis for prediction of the life expectancy of a patient given clinical or genetic covariates -- it is a linear model in its original form. In recent years, several methods have been proposed to generalize the Cox model to neural networks, but none of these are both numerically correct and computationally efficient. We propose FastCPH, a new method that runs in linear time and supports both the standard Breslow and Efron methods for tied events. We also demonstrate the performance of FastCPH combined with LassoNet, a neural network that provides interpretability through feature sparsity, on survival datasets. The final procedure is efficient, selects useful covariates and outperforms existing CoxPH approaches.},
  archiveprefix    = {arXiv},
  copyright        = {Creative Commons Attribution 4.0 International},
  doi              = {10.48550/ARXIV.2208.09793},
  eprint           = {2208.09793},
  file             = {:http\://arxiv.org/pdf/2208.09793v1:PDF},
  keywords         = {Machine Learning (stat.ML), Artificial Intelligence (cs.AI), Applications (stat.AP), FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:30:12},
  primaryclass     = {stat.ML},
  publisher        = {arXiv},
  x-fetchedfrom    = {Google Scholar},
}

@Article{Yang2022Unbox,
  author           = {Yang, Guang and Ye, Qinghao and Xia, Jun},
  journal          = {Information Fusion},
  title            = {Unbox the black-box for the medical explainable {AI} via multi-modal and multi-centre data fusion: A mini-review, two showcases and beyond},
  year             = {2022},
  issn             = {1566-2535},
  month            = jan,
  note             = {85 citations (Crossref) [2022-11-28]},
  pages            = {29--52},
  volume           = {77},
  abstract         = {Explainable Artificial Intelligence ({XAI}) is an emerging research topic of machine learning aimed at unboxing how {AI} systems{\rq} black-box choices are made. This research field inspects the measures and models involved in decision-making and seeks solutions to explain them explicitly. Many of the machine learning algorithms cannot manifest how and why a decision has been cast. This is particularly true of the most popular deep neural network approaches currently in use. Consequently, our confidence in {AI} systems can be hindered by the lack of explainability in these black-box models. The {XAI} becomes more and more crucial for deep learning powered applications, especially for medical and healthcare studies, although in general these deep neural networks can return an arresting dividend in performance. The insufficient explainability and transparency in most existing {AI} systems can be one of the major reasons that successful implementation and integration of {AI} tools into routine clinical practice are uncommon. In this study, we first surveyed the current progress of {XAI} and in particular its advances in healthcare applications. We then introduced our solutions for {XAI} leveraging multi-modal and multi-centre data fusion, and subsequently validated in two showcases following real clinical scenarios. Comprehensive quantitative and qualitative analyses can prove the efficacy of our proposed {XAI} solutions, from which we can envisage successful applications in a broader range of clinical questions.},
  doi              = {10.1016/j.inffus.2021.07.016},
  keywords         = {Explainable {AI}, Information fusion, Medical image analysis, Multi-domain information fusion, Weakly supervised learning},
  langid           = {english},
  modificationdate = {2023-04-28T08:30:12},
  publisher        = {Elsevier {BV}},
  shortjournal     = {Information Fusion},
  shorttitle       = {Unbox the black-box for the medical explainable {AI} via multi-modal and multi-centre data fusion},
  url              = {https://www.sciencedirect.com/science/article/pii/S1566253521001597},
  urldate          = {2022-11-28},
}

@Article{Walter2022Artificial,
  author           = {Walter, Wencke and Pohlkamp, Christian and Meggendorfer, Manja and Nadarajah, Niroshan and Kern, Wolfgang and Haferlach, Claudia and Haferlach, Torsten},
  journal          = {Blood Reviews},
  title            = {Artificial intelligence in hematological diagnostics: Game changer or gadget?},
  year             = {2022},
  issn             = {0268-960X},
  month            = oct,
  note             = {0 citations (Crossref) [2022-11-28]},
  pages            = {101019},
  volume           = {58},
  abstract         = {The future of clinical diagnosis and treatment of hematologic diseases will inevitably involve the integration of artificial intelligence ({AI})-based systems into routine practice to support the hematologists' decision making. Several studies have shown that {AI}-based models can already be used to automatically differentiate cells, reliably detect malignant cell populations, support chromosome banding analysis, and interpret clinical variants, contributing to early disease detection and prognosis. However, even the best tool can become useless if it is misapplied or the results are misinterpreted. Therefore, in order to comprehensively judge and correctly apply newly developed {AI}-based systems, the hematologist must have a basic understanding of the general concepts of machine learning. In this review, we provide the hematologist with a comprehensive overview of various machine learning techniques, their current implementations and approaches in different diagnostic subfields (e.g., cytogenetics, molecular genetics), and the limitations and unresolved challenges of the systems.},
  doi              = {10.1016/j.blre.2022.101019},
  keywords         = {Machine learning, Artificial intelligence, Simple, Hematological diagnostics, Precision medicine, Review{\textbackslash}, Transfer learning},
  langid           = {english},
  modificationdate = {2023-04-28T08:29:08},
  publisher        = {Elsevier {BV}},
  shortjournal     = {Blood Reviews},
  shorttitle       = {Artificial intelligence in hematological diagnostics},
  url              = {https://www.sciencedirect.com/science/article/pii/S0268960X22000935},
  urldate          = {2022-11-28},
}

@Article{ValeSilva2021Long,
  author           = {Vale-Silva, Lu{\'\i}s A. and Rohr, Karl},
  journal          = {{Sci Rep}},
  title            = {Long-term cancer survival prediction using multimodal deep learning},
  year             = {2021},
  issn             = {2045-2322},
  month            = jun,
  note             = {24 citations (Crossref) [2022-12-09] Number: 1 Publisher: Nature Publishing Group},
  number           = {1},
  pages            = {13505},
  volume           = {11},
  abstract         = {The age of precision medicine demands powerful computational techniques to handle high-dimensional patient data. We present {MultiSurv}, a multimodal deep learning method for long-term pan-cancer survival prediction. {MultiSurv} uses dedicated submodels to establish feature representations of clinical, imaging, and different high-dimensional omics data modalities. A data fusion layer aggregates the multimodal representations, and a prediction submodel generates conditional survival probabilities for follow-up time intervals spanning several decades. {MultiSurv} is the first non-linear and non-proportional survival prediction method that leverages multimodal data. In addition, {MultiSurv} can handle missing data, including single values and complete data modalities. {MultiSurv} was applied to data from 33 different cancer types and yields accurate pan-cancer patient survival curves. A quantitative comparison with previous methods showed that Multisurv achieves the best results according to different time-dependent metrics. We also generated visualizations of the learned multimodal representation of {MultiSurv}, which revealed insights on cancer characteristics and heterogeneity.},
  date             = {2021-06-29},
  doi              = {10.1038/s41598-021-92799-4},
  journaltitle     = {Scientific Reports},
  keywords         = {Cancer, Cancer genomics, Cancer imaging, Cancer models, Computational biology and bioinformatics, Computational models, Data integration},
  langid           = {english},
  modificationdate = {2023-04-13T22:01:18},
  nlmuniqueid      = {101563288},
  pii              = {10.1038/s41598-021-92799-4},
  pmc              = {PMC8242026},
  publisher        = {Springer Science and Business Media {LLC}},
  pubmed           = {34188098},
  rights           = {2021 The Author(s)},
  shortjournal     = {Sci Rep},
  url              = {https://www.nature.com/articles/s41598-021-92799-4},
  urldate          = {2022-12-09},
  x-fetchedfrom    = {PubMed},
}

@Article{Tellez2019Neural,
  author           = {Tellez, David and Litjens, Geert and van der Laak, Jeroen and Ciompi, Francesco},
  journal          = {IEEE transactions on pattern analysis and machine intelligence},
  title            = {Neural Image Compression for Gigapixel Histopathology Image Analysis},
  year             = {2019},
  month            = feb,
  number           = {2},
  pages            = {567--578},
  volume           = {43},
  doi              = {10.1109/tpami.2019.2936841},
  journaltitle     = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
  modificationdate = {2023-04-28T08:30:10},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
  x-fetchedfrom    = {Google Scholar},
}

@Article{Tan2022multi,
  author           = {Tan, Kaiwen and Huang, Weixian and Liu, Xiaofeng and Hu, Jinlong and Dong, Shoubin},
  journal          = {Artificial Intelligence in Medicine},
  title            = {A multi-modal fusion framework based on multi-task correlation learning for cancer prognosis prediction},
  year             = {2022},
  issn             = {0933-3657},
  month            = apr,
  note             = {2 citations (Crossref) [2022-11-28]},
  pages            = {102260},
  volume           = {126},
  abstract         = {Morphological attributes from histopathological images and molecular profiles from genomic data are important information to drive diagnosis, prognosis, and therapy of cancers. By integrating these heterogeneous but complementary data, many multi-modal methods are proposed to study the complex mechanisms of cancers, and most of them achieve comparable or better results from previous single-modal methods. However, these multi-modal methods are restricted to a single task (e.g., survival analysis or grade classification), and thus neglect the correlation between different tasks. In this study, we present a multi-modal fusion framework based on multi-task correlation learning ({MultiCoFusion}) for survival analysis and cancer grade classification, which combines the power of multiple modalities and multiple tasks. Specifically, a pre-trained {ResNet}-152 and a sparse graph convolutional network ({SGCN}) are used to learn the representations of histopathological images and {mRNA} expression data respectively. Then these representations are fused by a fully connected neural network ({FCNN}), which is also a multi-task shared network. Finally, the results of survival analysis and cancer grade classification output simultaneously. The framework is trained by an alternate scheme. We systematically evaluate our framework using glioma datasets from The Cancer Genome Atlas ({TCGA}). Results demonstrate that {MultiCoFusion} learns better representations than traditional feature extraction methods. With the help of multi-task alternating learning, even simple multi-modal concatenation can achieve better performance than other deep learning and traditional methods. Multi-task learning can improve the performance of multiple tasks not just one of them, and it is effective in both single-modal and multi-modal data.},
  doi              = {10.1016/j.artmed.2022.102260},
  keywords         = {Cancer grade, Multi-modal fusion, Multi-task learning, {PAYWALL}, Survival analysis},
  langid           = {english},
  modificationdate = {2023-04-15T18:59:25},
  publisher        = {Elsevier {BV}},
  shortjournal     = {Artificial Intelligence in Medicine},
  url              = {https://www.sciencedirect.com/science/article/pii/S0933365722000252},
  urldate          = {2022-11-28},
}

@Article{Sundararajan2017Axiomatic,
  author           = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  journal          = {Proceedings of the 34 th International Conference on MachineLearning},
  title            = {Axiomatic Attribution for Deep Networks},
  year             = {2017},
  month            = mar,
  abstract         = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  doi              = {10.48550/ARXIV.1703.01365},
  eprint           = {1703.01365},
  keywords         = {Machine Learning (cs.LG), FOS: Computer and information sciences},
  modificationdate = {2023-04-15T18:59:25},
  primaryclass     = {cs.LG},
  publisher        = {arXiv},
  url              = {http://arxiv.org/pdf/1703.01365v2},
}

@Article{Steyaert2022Multimodal,
  author           = {Steyaert, Sandra and Qiu, Yeping Lina and Zheng, Yuanning and Mukherjee, Pritam and Vogel, Hannes and Gevaert, Olivier},
  title            = {Multimodal data fusion of adult and pediatric brain tumors with deep learning},
  year             = {2022},
  month            = sep,
  note             = {0 citations (Crossref) [2022-11-28] Pages: 2022.09.21.22280223},
  abstract         = {The introduction of deep learning in both imaging and genomics has significantly advanced the analysis of biomedical data. For complex diseases such as cancer different data modalities may reveal different disease characteristics, and the integration of imaging with genomic data has the potential to unravel additional information then when using these data sources in isolation. Here, we propose a {DL} framework that by combining histopathology images with gene expression profiles can predict prognosis of brain tumors. Using two separate cohorts of 783 adult and 305 pediatric brain tumors, the developed multimodal data models achieved better prediction results compared to the single data models, but also leads to the identification of more relevant biological pathways. Importantly, when testing our adult models on a third independent brain tumor dataset, we show our multimodal framework is able to generalize and performs better on new data from different cohorts. Furthermore, leveraging the concept of transfer learning, we demonstrate how our multimodal models pre-trained on pediatric glioma can be used to predict prognosis for two more rare (less available samples) pediatric brain tumors, i.e. ependymoma and medulloblastoma. To summarize, our study illustrates that a multimodal data fusion approach can be successfully implemented and customized to model clinical outcome of adult and pediatric brain tumors.
The introduction of deep learning in both imaging and genomics has significantly advanced the analysis of biomedical data. For complex diseases such as cancer different data modalities may reveal different disease characteristics, and the integration of imaging with genomic data has the potential to unravel additional information then when using these data sources in isolation. Here, we propose a {DL} framework that by combining histopathology images with gene expression profiles can predict prognosis of brain tumors. Using two separate cohorts of 783 adult and 305 pediatric brain tumors, the developed multimodal data models achieved better prediction results compared to the single data models, but also leads to the identification of more relevant biological pathways. Importantly, when testing our adult models on a third independent brain tumor dataset, we show our multimodal framework is able to generalize and performs better on new data from different cohorts. Furthermore, leveraging the concept of transfer learning, we demonstrate how our multimodal models pre-trained on pediatric glioma can be used to predict prognosis for two more rare (less available samples) pediatric brain tumors, i.e. ependymoma and medulloblastoma. To summarize, our study illustrates that a multimodal data fusion approach can be successfully implemented and customized to model clinical outcome of adult and pediatric brain tumors.},
  doi              = {10.1101/2022.09.21.22280223},
  langid           = {english},
  modificationdate = {2023-04-15T18:59:25},
  publisher        = {Cold Spring Harbor Laboratory},
  rights           = {{\copyright} 2022, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  urldate          = {2022-11-28},
}

@Article{Stahlschmidt2022Multimodal,
  author           = {Sren Richard Stahlschmidt and Benjamin Ulfenborg and Jane Synnergren},
  journal          = {Briefings in Bioinformatics},
  title            = {Multimodal deep learning for biomedical data fusion: a review},
  year             = {2022},
  month            = jan,
  number           = {2},
  volume           = {23},
  doi              = {10.1093/bib/bbab569},
  journaltitle     = {Briefings in Bioinformatics},
  modificationdate = {2023-04-15T18:59:25},
  publisher        = {Oxford University Press},
  x-fetchedfrom    = {Google Scholar},
}

@Article{Sirinukunwattana2021Image,
  author           = {Sirinukunwattana, Korsuk and Domingo, Enric and Richman, Susan D. and Redmond, Keara L. and Blake, Andrew and Verrill, Clare and Leedham, Simon J. and Chatzipli, Aikaterini and Hardy, Claire and Whalley, Celina M. and Wu, Chieh-hsi and Beggs, Andrew D. and {McDermott}, Ultan and Dunne, Philip D. and Meade, Angela and Walker, Steven M. and Murray, Graeme I. and Samuel, Leslie and Seymour, Matthew and Tomlinson, Ian and Quirke, Phil and Maughan, Timothy and Rittscher, Jens and Koelzer, Viktor H.},
  journal          = {Gut},
  title            = {Image-based consensus molecular subtype ({imCMS}) classification of colorectal cancer using deep learning},
  year             = {2021},
  month            = mar,
  note             = {70 citations (Crossref) [2022-11-10] Publisher: {BMJ} Publishing Group Section: Artificial intelligence/machine learning},
  number           = {3},
  pages            = {544--554},
  volume           = {70},
  abstract         = {Objective Complex phenotypes captured on histological slides represent the biological processes at play in individual cancers, but the link to underlying molecular classification has not been clarified or systematised. In colorectal cancer ({CRC}), histological grading is a poor predictor of disease progression, and consensus molecular subtypes ({CMSs}) cannot be distinguished without gene expression profiling. We hypothesise that image analysis is a cost-effective tool to associate complex features of tissue organisation with molecular and outcome data and to resolve unclassifiable or heterogeneous cases. In this study, we present an image-based approach to predict {CRC} {CMS} from standard H\&E sections using deep learning.
Design Training and evaluation of a neural network were performed using a total of n=1206 tissue sections with comprehensive multi-omic data from three independent datasets (training on {FOCUS} trial, n=278 patients; test on rectal cancer biopsies, {GRAMPIAN} cohort, n=144 patients; and The Cancer Genome Atlas ({TCGA}), n=430 patients). Ground truth {CMS} calls were ascertained by matching random forest and single sample predictions from {CMS} classifier.
Results Image-based {CMS} ({imCMS}) accurately classified slides in unseen datasets from {TCGA} (n=431 slides, {AUC})=0.84) and rectal cancer biopsies (n=265 slides, {AUC}=0.85). {imCMS} spatially resolved intratumoural heterogeneity and provided secondary calls correlating with bioinformatic prediction from molecular data. {imCMS} classified samples previously unclassifiable by {RNA} expression profiling, reproduced the expected correlations with genomic and epigenetic alterations and showed similar prognostic associations as transcriptomic {CMS}.
Conclusion This study shows that a prediction of {RNA} expression classifiers can be made from H\&E images, opening the door to simple, cheap and reliable biological stratification within routine workflows.},
  doi              = {10.1136/gutjnl-2019-319866},
  keywords         = {colorectal pathology, computerised image analysis, molecular pathology},
  langid           = {english},
  modificationdate = {2023-04-15T18:59:25},
  pmid             = {32690604},
  publisher        = {{BMJ}},
  rights           = {{\copyright} Author(s) (or their employer(s)) 2021. Re-use permitted under {CC} {BY}. Published by {BMJ}.. https://creativecommons.org/licenses/by/4.0/This is an open access article distributed in accordance with the Creative Commons Attribution 4.0 Unported ({CC} {BY} 4.0) license, which permits others to copy, redistribute, remix, transform and build upon this work for any purpose, provided the original work is properly cited, a link to the licence is given, and indication of whether changes were made. See: https://creativecommons.org/licenses/by/4.0/.},
  url              = {https://gut.bmj.com/content/70/3/544},
  urldate          = {2022-11-08},
}

@Article{Selvaraju2016Grad,
  author           = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  journal          = {International Journal of Computer Vision},
  title            = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization},
  year             = {2016},
  month            = oct,
  note             = {cite arxiv:1610.02391Comment: This version was published in International Journal of Computer Vision (IJCV) in 2019; A previous version of the paper was published at International Conference on Computer Vision (ICCV'17)},
  number           = {2},
  pages            = {336--359},
  volume           = {128},
  abstract         = {We propose a technique for producing "visual explanations" for decisions from
a large class of CNN-based models, making them more transparent. Our approach -
Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of
any target concept, flowing into the final convolutional layer to produce a
coarse localization map highlighting important regions in the image for
predicting the concept. Grad-CAM is applicable to a wide variety of CNN
model-families: (1) CNNs with fully-connected layers, (2) CNNs used for
structured outputs, (3) CNNs used in tasks with multimodal inputs or
reinforcement learning, without any architectural changes or re-training. We
combine Grad-CAM with fine-grained visualizations to create a high-resolution
class-discriminative visualization and apply it to off-the-shelf image
classification, captioning, and visual question answering (VQA) models,
including ResNet-based architectures. In the context of image classification
models, our visualizations (a) lend insights into their failure modes, (b) are
robust to adversarial images, (c) outperform previous methods on localization,
(d) are more faithful to the underlying model and (e) help achieve
generalization by identifying dataset bias. For captioning and VQA, we show
that even non-attention based models can localize inputs. We devise a way to
identify important neurons through Grad-CAM and combine it with neuron names to
provide textual explanations for model decisions. Finally, we design and
conduct human studies to measure if Grad-CAM helps users establish appropriate
trust in predictions from models and show that Grad-CAM helps untrained users
successfully discern a 'stronger' nodel from a 'weaker' one even when both make
identical predictions. Our code is available at
https://github.com/ramprs/grad-cam/, along with a demo at
http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
  added-at         = {2021-05-23T16:29:45.000+0200},
  biburl           = {https://www.bibsonomy.org/bibtex/2c0c80613cc68bea491ee999febdd3a78/cmcneile},
  description      = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization},
  doi              = {10.1007/s11263-019-01228-7},
  interhash        = {b8ba041501bafce876014ea7228c6812},
  intrahash        = {c0c80613cc68bea491ee999febdd3a78},
  keywords         = {ML},
  modificationdate = {2023-04-15T18:59:25},
  publisher        = {Springer Science and Business Media {LLC}},
  timestamp        = {2021-05-23T16:29:45.000+0200},
  url              = {http://arxiv.org/abs/1610.02391},
  x-fetchedfrom    = {Bibsonomy},
}

@Article{Schneider2022Integration,
  author           = {Lucas Schneider and Sara Laiouar-Pedari and Sara Kuntz and Eva Krieghoff-Henning and Achim Hekler and Jakob N. Kather and Timo Gaiser and Stefan Frhling and Titus J. Brinker},
  journal          = {{Eur J Cancer}},
  title            = {Integration of deep learning-based image analysis and genomic data in cancer pathology: A systematic review},
  year             = {2022},
  month            = jan,
  pages            = {80--91},
  volume           = {160},
  abstract         = {Over the past decade, the development of molecular high-throughput methods (omics) increased rapidly and provided new insights for cancer research. In parallel, deep learning approaches revealed the enormous potential for medical image analysis, especially in digital pathology. Combining image and omics data with deep learning tools may enable the discovery of new cancer biomarkers and a more precise prediction of patient prognosis. This systematic review addresses different multimodal fusion methods of convolutional neural network-based image analyses with omics data, focussing on the impact of data combination on the classification performance. PubMed was screened for peer-reviewed articles published in English between January 2015 and June 2021 by two independent researchers. Search terms related to deep learning, digital pathology, omics, and multimodal fusion were combined. We identified a total of 11 studies meeting the inclusion criteria, namely studies that used convolutional neural networks for haematoxylin and eosinimage analysis of patients with cancer in combination with integrated omics data. Publications were categorised according to their endpoints: 7 studies focused on survival analysis and 4 studies on prediction of cancer subtypes, malignancy or microsatellite instability with spatial analysis. Image-based classifiers already show high performances in prognostic and predictive cancer diagnostics. The integration of omics data led to improved performance in all studies described here. However, these are very early studies that still require external validation to demonstrate their generalisability and robustness. Further and more comprehensive studies with larger sample sizes are needed to evaluate performance and determine clinical benefits.},
  date             = {2022-01},
  doi              = {10.1016/j.ejca.2021.10.007},
  journaltitle     = {European Journal of Cancer},
  modificationdate = {2023-04-13T22:01:18},
  nlmuniqueid      = {9005373},
  pii              = {S0959-8049(21)01160-6},
  publisher        = {Elsevier {BV}},
  pubmed           = {34810047},
  x-fetchedfrom    = {PubMed},
}

@Article{Sammut2022Multi,
  author           = {Stephen-John Sammut and Mireia Crispin-Ortuzar and Suet-Feung Chin and Elena Provenzano and Helen A. Bardwell and Wenxin Ma and Wei Cope and Ali Dariush and Sarah-Jane Dawson and Jean E. Abraham and Janet Dunn and Louise Hiller and Jeremy Thomas and David A. Cameron and John M. S. Bartlett and Larry Hayward and Paul D. Pharoah and Florian Markowetz and Oscar M. Rueda and Helena M. Earl and Carlos Caldas},
  journal          = {Nature},
  title            = {Multi-omic machine learning predictor of breast cancer therapy response},
  year             = {2022},
  issn             = {1476-4687},
  month            = jan,
  note             = {41 citations (Crossref) [2022-12-02]},
  number           = {7894},
  pages            = {623--629},
  volume           = {601},
  abstract         = {Breast cancers are complex ecosystems of malignant cells and the tumour microenvironment1. The composition of these tumour ecosystems and interactions within them contribute to responses tocytotoxic therapy2. Efforts to build response predictors have not incorporated this knowledge. We collected clinical, digital pathology, genomic and transcriptomic profiles of pre-treatment biopsies of breast tumours from 168 patients treated with chemotherapy with or without {HER}2 (encoded by {ERBB}2)-targeted therapy before surgery. Pathology end points (complete response or residual disease) at surgery3 were then correlated with multi-omic features in these diagnostic biopsies. Here we show that response to treatment is modulated by the pre-treated tumour ecosystem, and its multi-omics landscape can be integrated in predictive models using machine learning. The degree of residual disease following therapy is monotonically associated with pre-therapy features, including tumour mutational and copy number landscapes, tumour proliferation, immune infiltration and T cell dysfunction and exclusion. Combining these features into a multi-omic machine learning model predicted apathological complete response in an external validation cohort (75 patients) with an area under the curve of 0.87. In conclusion, response to therapy is determined by the baseline characteristics of the totality of the tumour ecosystem captured through data integration and machine learning. This approach could be used to develop predictors for other cancers.},
  doi              = {10.1038/s41586-021-04278-5},
  keywords         = {Genomics, Therapy Response, Breast Neoplasms, Ecosystem, Female, Humans, Machine Learning, Neoadjuvant Therapy, Tumor Microenvironment},
  modificationdate = {2023-04-15T18:59:25},
  pmcid            = {PMC8791834},
  pmid             = {34875674},
  publisher        = {Springer Science and Business Media {LLC}},
  shortjournal     = {Nature},
}

@Article{Hancock2023Kidney,
  author           = {S. Brandon Hancock and Christos S. Georgiades},
  journal          = {Cancer J.},
  title            = {Kidney Cancer},
  year             = {2023},
  issn             = {1528-9117},
  month            = nov,
  number           = {6},
  pages            = {387--392},
  volume           = {22},
  doi              = {10.1097/ppo.0000000000000225},
  journaltitle     = {The Cancer Journal},
  modificationdate = {2023-04-15T18:59:24},
  publisher        = {Ovid Technologies (Wolters Kluwer Health)},
}

@Article{Feng2022Development,
  author           = {Feng, Lili and Liu, Zhenyu and Li, Chaofeng and Li, Zhenhui and Lou, Xiaoying and Shao, Lizhi and Wang, Yunlong and Huang, Yan and Chen, Haiyang and Pang, Xiaolin and Liu, Shuai and He, Fang and Zheng, Jian and Meng, Xiaochun and Xie, Peiyi and Yang, Guanyu and Ding, Yi and Wei, Mingbiao and Yun, Jingping and Hung, Mien-Chie and Zhou, Weihua and Wahl, Daniel R. and Lan, Ping and Tian, Jie and Wan, Xiangbo},
  journal          = {The Lancet Digital Health},
  title            = {Development and validation of a radiopathomics model to predict pathological complete response to neoadjuvant chemoradiotherapy in locally advanced rectal cancer: a multicentre observational study},
  year             = {2022},
  issn             = {2589-7500},
  month            = jan,
  note             = {15 citations (Crossref) [2022-12-02] Publisher: Elsevier},
  number           = {1},
  pages            = {e8--e17},
  volume           = {4},
  doi              = {10.1016/s2589-7500(21)00215-6},
  keywords         = {Therapy Response},
  modificationdate = {2023-04-15T18:59:24},
  pmid             = {34952679},
  publisher        = {Elsevier {BV}},
  shortjournal     = {The Lancet Digital Health},
  shorttitle       = {Development and validation of a radiopathomics model to predict pathological complete response to neoadjuvant chemoradiotherapy in locally advanced rectal cancer},
  url              = {https://www.thelancet.com/journals/landig/article/PIIS2589-7500(21)00215-6/fulltext},
  urldate          = {2022-12-02},
}

@Article{Faraggi1995neural,
  author           = {Faraggi, David and Simon, Richard},
  journal          = {Statistics in medicine},
  title            = {A neural network model for survival data},
  year             = {1995},
  month            = jan,
  number           = {1},
  pages            = {73--82},
  volume           = {14},
  doi              = {10.1002/sim.4780140108},
  journaltitle     = {Statistics in Medicine},
  modificationdate = {2023-04-15T18:59:24},
  publisher        = {Wiley},
  x-fetchedfrom    = {Google Scholar},
}

@Article{Esteva2019guide,
  author           = {Esteva, Andre and Robicquet, Alexandre and Ramsundar, Bharath and Kuleshov, Volodymyr and {DePristo}, Mark and Chou, Katherine and Cui, Claire and Corrado, Greg and Thrun, Sebastian and Dean, Jeff},
  journal          = {Nature Medicine},
  title            = {A guide to deep learning in healthcare},
  year             = {2019},
  issn             = {1546-170X},
  month            = jan,
  note             = {Number: 1 Publisher: Nature Publishing Group},
  number           = {1},
  pages            = {24--29},
  volume           = {25},
  abstract         = {Here we present deep-learning techniques for healthcare, centering our discussion on deep learning in computer vision, natural language processing, reinforcement learning, and generalized methods. We describe how these computational techniques can impact a few key areas of medicine and explore how to build end-to-end systems. Our discussion of computer vision focuses largely on medical imaging, and we describe the application of natural language processing to domains such as electronic health record data. Similarly, reinforcement learning is discussed in the context of robotic-assisted surgery, and generalized deep-learning methods for genomics are reviewed.},
  doi              = {10.1038/s41591-018-0316-z},
  keywords         = {Bioinformatics, Machine learning, Health care, Interesting},
  langid           = {english},
  modificationdate = {2023-04-15T18:59:24},
  publisher        = {Springer Science and Business Media {LLC}},
  rights           = {2019 Springer Nature America, Inc.},
  shortjournal     = {Nat Med},
  url              = {https://www.nature.com/articles/s41591-018-0316-z},
  urldate          = {2022-11-09},
}

@InProceedings{Dwivedi2022Multi,
  author           = {Dwivedi, Chaitanya and Nofallah, Shima and Pouryahya, Maryam and Iyer, Janani and Leidal, Kenneth and Chung, Chuhan and Watkins, Timothy and Billin, Andrew and Myers, Robert and Abel, John and Behrooz, Ali},
  booktitle        = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition Workshops ({CVPRW})},
  title            = {Multi stain graph fusion for multimodal integration in pathology},
  year             = {2022},
  address          = {New Orleans, {LA}, {USA}},
  month            = apr,
  note             = {0 citations (Crossref) [2022-11-28]},
  number           = {{arXiv}:2204.12541},
  pages            = {1834--1844},
  publisher        = {{IEEE}},
  abstract         = {In pathology, tissue samples are assessed using multiple staining techniques to enhance contrast in unique histologic features. In this paper, we introduce a multimodal {CNN}-{GNN} based graph fusion approach that leverages complementary information from multiple non-registered histopathology images to predict pathologic scores. We demonstrate this approach in nonalcoholic steatohepatitis ({NASH}) by predicting {CRN} fibrosis stage and {NAFLD} Activity Score ({NAS}). Primary assessment of {NASH} typically requires liver biopsy evaluation on two histological stains: Trichrome ({TC}) and hematoxylin and eosin (H\&E). Our multimodal approach learns to extract complementary information from {TC} and H\&E graphs corresponding to each stain while simultaneously learning an optimal policy to combine this information. We report up to 20\% improvement in predicting fibrosis stage and {NAS} component grades over single-stain modeling approaches, measured by computing linearly weighted Cohen's kappa between machine-derived vs. pathologist consensus scores. Broadly, this paper demonstrates the value of leveraging diverse pathology images for improved {ML}-powered histologic assessment.},
  archiveprefix    = {arxiv},
  doi              = {10.1109/cvprw56347.2022.00200},
  eprint           = {2204.12541 [cs, eess]},
  eventtitle       = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition Workshops ({CVPRW})},
  isbn             = {978-1-66548-739-9},
  keywords         = {Computer Science - Computer Vision and Pattern Recognition},
  langid           = {english},
  modificationdate = {2023-04-15T18:59:24},
  url              = {http://arxiv.org/abs/2204.12541; https://ieeexplore.ieee.org/document/9857316/},
  urldate          = {2022-11-28},
}

@Article{Dosovitskiy2020Image,
  author           = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  title            = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  year             = {2020},
  month            = oct,
  abstract         = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-12T08:39:11},
  doi              = {10.48550/ARXIV.2010.11929},
  eprint           = {2010.11929},
  file             = {:http\://arxiv.org/pdf/2010.11929v2:PDF},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences},
  modificationdate = {2023-04-15T18:59:24},
  primaryclass     = {cs.CV},
  publisher        = {arXiv},
}

@Article{Deo2021Survival,
  author           = {Salil Vasudeo Deo and Vaishali Deo and Varun Sundaram},
  journal          = {Indian Journal of Thoracic and Cardiovascular Surgery},
  title            = {Survival analysis{\textemdash}part 2: Cox proportional hazards model},
  year             = {2021},
  month            = jan,
  number           = {2},
  pages            = {229--233},
  volume           = {37},
  creationdate     = {2023-04-12T08:41:25},
  doi              = {10.1007/s12055-020-01108-7},
  modificationdate = {2023-04-15T18:59:24},
  publisher        = {Springer Science and Business Media {LLC}},
}

@Misc{Deng2022Omni,
  author           = {Deng, Ruining and Liu, Quan and Cui, Can and Yao, Tianyuan and Long, Jun and Asad, Zuhayr and Womick, R. Michael and Zhu, Zheyu and Fogo, Agnes B. and Zhao, Shilin and Yang, Haichun and Huo, Yuankai},
  month            = jun,
  title            = {Omni-Seg: A Scale-aware Dynamic Network for Renal Pathological Image Segmentation},
  year             = {2022},
  abstract         = {Comprehensive semantic segmentation on renal pathological images is challenging due to the heterogeneous scales of the objects. For example, on a whole slide image ({WSI}), the cross-sectional areas of glomeruli can be 64 times larger than that of the peritubular capillaries, making it impractical to segment both objects on the same patch, at the same scale. To handle this scaling issue, prior studies have typically trained multiple segmentation networks in order to match the optimal pixel resolution of heterogeneous tissue types. This multi-network solution is resource-intensive and fails to model the spatial relationship between tissue types. In this paper, we propose the Omni-Seg+ network, a scale-aware dynamic neural network that achieves multi-object (six tissue types) and multi-scale (5X to 40X scale) pathological image segmentation via a single neural network. The contribution of this paper is three-fold: (1) a novel scale-aware controller is proposed to generalize the dynamic neural network from single-scale to multi-scale; (2) semi-supervised consistency regularization of pseudo-labels is introduced to model the inter-scale correlation of unannotated tissue types into a single end-to-end learning paradigm; and (3) superior scale-aware generalization is evidenced by directly applying a model trained on human kidney images to mouse kidney images, without retraining. By learning from {\textasciitilde}150,000 human pathological image patches from six tissue types at three different resolutions, our approach achieved superior segmentation performance according to human visual assessment and evaluation of image-omics (i.e., spatial transcriptomics). The official implementation is available at https://github.com/ddrrnn123/Omni-Seg.},
  archiveprefix    = {arxiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-12T08:42:06},
  doi              = {10.48550/arXiv.2206.13632},
  eprint           = {2206.13632 [cs, eess]},
  keywords         = {Image and Video Processing (eess.IV), Computer Vision and Pattern Recognition (cs.CV), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  modificationdate = {2023-04-15T18:59:24},
  number           = {{arXiv}:2206.13632},
  publisher        = {arXiv},
  shorttitle       = {Omni-Seg+},
  url              = {http://arxiv.org/abs/2206.13632},
  urldate          = {2022-11-28},
}

@Article{Cohen2005Renal,
  author           = {Cohen, Herbert T. and McGovern, Francis J.},
  journal          = {N. Engl. J. Med.},
  title            = {Renal-Cell Carcinoma},
  year             = {2005},
  issn             = {1533-4406},
  month            = dec,
  number           = {23},
  pages            = {2477--2490},
  volume           = {353},
  creationdate     = {2023-04-12T08:44:02},
  date             = {2005-12},
  doi              = {10.1056/nejmra043172},
  eprint           = {16339096},
  journaltitle     = {New England Journal of Medicine},
  modificationdate = {2023-04-13T22:01:18},
  publisher        = {Massachusetts Medical Society},
}

@Article{Clark2003Survival,
  author           = {Clark, T. G. and Bradburn, M. J. and Love, S. B. and Altman, D. G.},
  journal          = {British Journal of Cancer},
  title            = {Survival Analysis Part I: Basic concepts and first analyses},
  year             = {2003},
  month            = jul,
  number           = {2},
  pages            = {232--238},
  volume           = {89},
  creationdate     = {2023-04-12T08:44:24},
  doi              = {10.1038/sj.bjc.6601118},
  modificationdate = {2023-04-15T18:59:24},
  publisher        = {Springer Science and Business Media {LLC}},
}

@InProceedings{Ciompi2017importance,
  author           = {Ciompi, Francesco and Geessink, Oscar and Bejnordi, Babak Ehteshami and de Souza, Gabriel Silva and Baidoshvili, Alexi and Litjens, Geert and van Ginneken, Bram and Nagtegaal, Iris and van der Laak, Jeroen},
  booktitle        = {2017 {IEEE} 14th International Symposium on Biomedical Imaging ({ISBI} 2017)},
  title            = {The importance of stain normalization in colorectal tissue classification with convolutional networks},
  year             = {2017},
  month            = apr,
  pages            = {160--163},
  publisher        = {{IEEE}},
  creationdate     = {2023-04-12T08:45:09},
  doi              = {10.1109/isbi.2017.7950492},
  modificationdate = {2023-04-15T18:59:24},
}

@Article{Christinat2015Integrated,
  author           = {Christinat, Yann and Krek, Wilhelm},
  journal          = {Oncotarget},
  title            = {Integrated genomic analysis identifies subclasses and prognosis signatures of kidney cancer},
  year             = {2015},
  issn             = {1949-2553},
  month            = apr,
  note             = {35 citations (Crossref) [2022-11-13]},
  number           = {12},
  pages            = {10521--10531},
  volume           = {6},
  abstract         = {{PURPOSE}: To define robust {miRNA}-based molecular classifiers for human clear cell renal cell carcinoma ({ccRCC}) subgrouping and prognostication.
{EXPERIMENTAL} {DESIGN}: Multidimensional data of over 500 clear cell renal cell carcinoma ({ccRCC}) patients were retrieved from The Cancer Genome Atlas ({TCGA}) archive. Data analysis was based on a novel computational approach that selectively considers patients with extreme expression values of {miRNAs} to detect survival-associated molecular signatures.
{RESULTS}: Our in silico analysis unveiled a novel {ccRCC}-specific 5-{miRNA} ({miR}-10b, {miR}-21, {miR}-143, {miR}-183, and {miR}-192) signature able, when combined with information from conventional {TNM} staging and the age of the patient, to prognosticate {ccRCC} outcome more accurately than known {ccRCC} {miRNA} signatures or {TNM} staging alone. Furthermore, our approach revealed the existence of 6 distinct subgroups of {ccRCC} characterized by discrete differences in overall survival, tumor stage, and mutational spectra in key {ccRCC} tumor suppressor genes. It also demonstrated that {BAP}1 mutations correlate with tumor progression rather than overall survival.
{CONCLUSION}: Integrated analysis of multidimensional data from the {TCGA} archive allowed to draw a portrait of distinct molecular subclasses of human {ccRCC} and to define signatures for prognosticating disease outcome. Together, these results offer new prospects for more accurate stratification and prognostication of {ccRCC}.},
  creationdate     = {2023-04-12T08:45:19},
  date             = {2015-04-30},
  doi              = {10.18632/oncotarget.3294},
  eprint           = {25826081},
  journaltitle     = {Oncotarget},
  keywords         = {Genomics},
  modificationdate = {2023-04-13T22:01:18},
  pmcid            = {PMC4496372},
  pmid             = {25826081},
  publisher        = {Impact Journals, {LLC}},
  shortjournal     = {Oncotarget},
}

@Article{Chowdhury2020Kidney,
  author           = {Chowdhury, Nivedita and Drake, Charles G.},
  journal          = {Urol. Clin. North Am.},
  title            = {Kidney Cancer: An Overview of Current Therapeutic Approaches},
  year             = {2020},
  issn             = {0094-0143},
  month            = nov,
  number           = {4},
  pages            = {419--431},
  volume           = {47},
  creationdate     = {2023-04-12T08:45:43},
  date             = {2020-11},
  doi              = {10.1016/j.ucl.2020.07.009},
  journaltitle     = {Urologic Clinics of North America},
  modificationdate = {2023-04-13T22:01:18},
  publisher        = {Elsevier {BV}},
}

@Article{Choi2019EmbraceNet,
  author           = {Choi, Jun-Ho and Lee, Jong-Seok},
  journal          = {Information Fusion 51 (2019) 259-270},
  title            = {EmbraceNet: A robust deep learning architecture for multimodal classification},
  year             = {2019},
  month            = nov,
  pages            = {259--270},
  volume           = {51},
  abstract         = {Classification using multimodal data arises in many machine learning applications. It is crucial not only to model cross-modal relationship effectively but also to ensure robustness against loss of part of data or modalities. In this paper, we propose a novel deep learning-based multimodal fusion architecture for classification tasks, which guarantees compatibility with any kind of learning models, deals with cross-modal information carefully, and prevents performance degradation due to partial absence of data. We employ two datasets for multimodal classification tasks, build models based on our architecture and other state-of-the-art models, and analyze their performance on various situations. The results show that our architecture outperforms the other multimodal fusion architectures when some parts of data are not available.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-12T08:46:44},
  date             = {2019-04-19},
  doi              = {10.1016/j.inffus.2019.02.010},
  eprint           = {1904.09078},
  file             = {:http\://arxiv.org/pdf/1904.09078v1:PDF},
  keywords         = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  modificationdate = {2023-04-15T18:59:24},
  primaryclass     = {cs.LG},
  publisher        = {Elsevier {BV}},
}

@Article{Chen2022Pan,
  author           = {Chen, Richard J. and Lu, Ming Y. and Williamson, Drew F. K. and Chen, Tiffany Y. and Lipkova, Jana and Noor, Zahra and Shaban, Muhammad and Shady, Maha and Williams, Mane and Joo, Bumjin and Mahmood, Faisal},
  journal          = {Cancer Cell},
  title            = {Pan-cancer integrative histology-genomic analysis via multimodal deep learning},
  year             = {2022},
  issn             = {1535-6108},
  month            = aug,
  note             = {6 citations (Crossref) [2022-12-02]},
  number           = {8},
  pages            = {865--878},
  volume           = {40},
  abstract         = {The rapidly emerging field of computational pathology has demonstrated promise in developing objective prognostic models from histology images. However, most prognostic models are either based on histology or genomics alone and do not address how these data sources can be integrated to develop joint image-omic prognostic models. Additionally, identifying explainable morphological and molecular descriptors from these models that govern such prognosis is of interest. We use multimodal deep learning to jointly examine pathology whole-slide images and molecular profile data from 14 cancer types. Our weakly supervised, multimodal deep-learning algorithm is able to fuse these heterogeneous modalities to predict outcomes and discover prognostic features that correlate with poor and favorable outcomes. We present all analyses for morphological and molecular correlates of patient prognosis across the 14 cancer types at both a disease and a patient level in an interactive open-access database to allow for further exploration, biomarker discovery, and feature assessment.},
  annote           = {Comment: Demo: http://pancancer.mahmoodlab.org},
  archiveprefix    = {arxiv},
  creationdate     = {2023-04-12T08:47:19},
  doi              = {10.1016/j.ccell.2022.07.004},
  eprint           = {2108.02278 [cs, q-bio]},
  keywords         = {deep learning},
  langid           = {english},
  modificationdate = {2023-04-15T18:59:23},
  publisher        = {Elsevier {BV}},
  shortjournal     = {Cancer Cell},
  url              = {https://www.sciencedirect.com/science/article/pii/S1535610822003178; http://arxiv.org/abs/2108.02278},
  urldate          = {2022-12-02},
}

@Article{Chen2021Whole,
  author           = {Chen, Richard J. and Lu, Ming Y. and Shaban, Muhammad and Chen, Chengkuan and Chen, Tiffany Y. and Williamson, Drew F. K. and Mahmood, Faisal},
  title            = {Whole Slide Images are 2D Point Clouds: Context-Aware Survival Prediction Using Patch-Based Graph Convolutional Networks},
  year             = {2021},
  month            = jul,
  pages            = {339--349},
  abstract         = {Cancer prognostication is a challenging task in computational pathology that requires context-aware representations of histology features to adequately infer patient survival. Despite the advancements made in weakly-supervised deep learning, many approaches are not context-aware and are unable to model important morphological feature interactions between cell identities and tissue types that are prognostic for patient survival. In this work, we present Patch-GCN, a context-aware, spatially-resolved patch-based graph convolutional network that hierarchically aggregates instance-level histology features to model local- and global-level topological structures in the tumor microenvironment. We validate Patch-GCN with 4,370 gigapixel WSIs across five different cancer types from the Cancer Genome Atlas (TCGA), and demonstrate that Patch-GCN outperforms all prior weakly-supervised approaches by 3.58-9.46\%. Our code and corresponding models are publicly available at https://github.com/mahmoodlab/Patch-GCN.},
  archiveprefix    = {arXiv},
  booktitle        = {Medical Image Computing and Computer Assisted Intervention {\textendash} {MICCAI} 2021},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-12T08:51:12},
  doi              = {10.1007/978-3-030-87237-3_33},
  eprint           = {2107.13048},
  file             = {:http\://arxiv.org/pdf/2107.13048v1:PDF},
  keywords         = {Image and Video Processing (eess.IV), Computer Vision and Pattern Recognition (cs.CV), Tissues and Organs (q-bio.TO), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Biological sciences},
  modificationdate = {2023-04-28T08:30:11},
  primaryclass     = {eess.IV},
  publisher        = {Springer International Publishing},
}

@InProceedings{Chen2022Scaling,
  author           = {Chen, Richard J. and Chen, Chengkuan and Li, Yicong and Chen, Tiffany Y. and Trister, Andrew D. and Krishnan, Rahul G. and Mahmood, Faisal},
  booktitle        = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  title            = {Scaling Vision Transformers to Gigapixel Images via Hierarchical Self-Supervised Learning},
  year             = {2022},
  month            = jun,
  pages            = {16123--16134},
  publisher        = {{IEEE}},
  abstract         = {Vision Transformers (ViTs) and their multi-scale and hierarchical variations have been successful at capturing image representations but their use has been generally studied for low-resolution images (e.g. 256 {\texttimes} 256, 384 {\texttimes} 384). For gigapixel whole-slide imaging (WSI) in computational pathology, WSIs can be as large as 150000 {\texttimes} 150000 pixels at 20 {\texttimes} magnification and exhibit a hierarchical structure of visual tokens across varying resolutions: from 16 {\texttimes} 16 images capturing individual cells, to 4096 {\texttimes} 4096 images characterizing interactions within the tissue microenvironment. We introduce a new ViT architecture called the Hierarchical Image Pyramid Transformer (HIPT), which leverages the natural hierarchical structure inherent in WSIs using two levels of self-supervised learning to learn high-resolution image representations. HIPT is pretrained across 33 cancer types using 10,678 gigapixel WSIs, 408,218 4096 {\texttimes} 4096 images, and 104M 256 {\texttimes} 256 images. We benchmark HIPT representations on 9 slide-level tasks, and demonstrate that: 1) HIPT with hierarchical pretraining outperforms current state-of-the-art methods for cancer subtyping and survival prediction, 2) self-supervised ViTs are able to model important inductive biases about the hierarchical structure of phenotypes in the tumor microenvironment.},
  creationdate     = {2023-04-12T08:51:24},
  date             = {2022-06},
  doi              = {10.1109/cvpr52688.2022.01567},
  isbn             = {978-1-6654-6947-0},
  issn             = {1063-6919},
  keywords         = {Training, Visualization, Self-supervised learning, Computer architecture, Image representation, Transformers, Workstations},
  modificationdate = {2023-04-13T22:03:39},
  url              = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=9880275; https://ieeexplore.ieee.org/document/9880275/},
  x-fetchedfrom    = {IEEEXplore},
}

@Article{Chen2022Pathomic,
  author           = {Richard J. Chen and Ming Y. Lu and Jingwen Wang and Drew F. K. Williamson and Scott J. Rodig and Neal I. Lindeman and Faisal Mahmood},
  journal          = {{IEEE Trans Med Imaging}},
  title            = {Pathomic Fusion: An Integrated Framework for Fusing Histopathology and Genomic Features for Cancer Diagnosis and Prognosis},
  year             = {2022},
  month            = apr,
  number           = {4},
  pages            = {757--770},
  volume           = {41},
  abstract         = {Cancer diagnosis, prognosis, mymargin and therapeutic response predictions are based on morphological information from histology slides and molecular profiles from genomic data. However, most deep learning-based objective outcome prediction and grading paradigms are based on histology or genomics alone and do not make use of the complementary information in an intuitive manner. In this work, we propose Pathomic Fusion, an interpretable strategy for end-to-end multimodal fusion of histology image and genomic (mutations, CNV, RNA-Seq) features for survival outcome prediction. Our approach models pairwise feature interactions across modalities by taking the Kronecker product of unimodal feature representations, and controls the expressiveness of each representation via a gating-based attention mechanism. Following supervised learning, we are able to interpret and saliently localize features across each modality, and understand how feature importance shifts when conditioning on multimodal input. We validate our approach using glioma and clear cell renal cell carcinoma datasets from the Cancer Genome Atlas (TCGA), which contains paired whole-slide image, genotype, and transcriptome data with ground truth survival and histologic grade labels. In a 15-fold cross-validation, our results demonstrate that the proposed multimodal fusion paradigm improves prognostic determinations from ground truth grading and molecular subtyping, as well as unimodal deep networks trained on histology and genomic data alone. The proposed method establishes insight and theory on how to train deep networks on multimodal biomedical data in an intuitive manner, which will be useful for other problems in medicine that seek to combine heterogeneous data streams for understanding diseases and predicting response and resistance to treatment. Code and trained models are made available at: https://github.com/mahmoodlab/PathomicFusion.},
  added-at         = {2020-01-03T00:00:00.000+0100},
  biburl           = {https://www.bibsonomy.org/bibtex/2f08def5d010dda1096529b797e6479a1/dblp},
  creationdate     = {2023-04-12T08:52:41},
  date             = {2022-04},
  doi              = {10.1109/tmi.2020.3021387},
  ee               = {http://arxiv.org/abs/1912.08937},
  interhash        = {e144a59bd840f4c4a55601cb5481190d},
  intrahash        = {f08def5d010dda1096529b797e6479a1},
  journaltitle     = {{IEEE} Transactions on Medical Imaging},
  modificationdate = {2023-04-14T23:04:21},
  nlmuniqueid      = {8310780},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
  pubmed           = {32881682},
  timestamp        = {2020-01-04T11:38:48.000+0100},
  x-fetchedfrom    = {PubMed},
}

@Article{Cheerla2019Deep,
  author           = {Cheerla, Anika and Gevaert, Olivier},
  journal          = {Bioinformatics},
  title            = {Deep learning with multimodal representation for pancancer prognosis prediction},
  year             = {2019},
  issn             = {1367-4803},
  month            = jul,
  note             = {115 citations (Crossref) [2022-11-10]},
  number           = {14},
  pages            = {i446--i454},
  volume           = {35},
  abstract         = {Estimating the future course of patients with cancer lesions is invaluable to physicians; however, current clinical methods fail to effectively use the vast amount of multimodal data that is available for cancer patients. To tackle this problem, we constructed a multimodal neural network-based model to predict the survival of patients for 20 different cancer types using clinical data, {mRNA} expression data, {microRNA} expression data and histopathology whole slide images ({WSIs}). We developed an unsupervised encoder to compress these four data modalities into a single feature vector for each patient, handling missing data through a resilient, multimodal dropout method. Encoding methods were tailored to each data type---using deep highway networks to extract features from clinical and genomic data, and convolutional neural networks to extract features from {WSIs}.We used pancancer data to train these feature encodings and predict single cancer and pancancer overall survival, achieving a C-index of 0.78 overall. This work shows that it is possible to build a pancancer model for prognosis that also predicts prognosis in single cancer sites. Furthermore, our model handles multiple data modalities, efficiently analyzes {WSIs} and represents patient multimodal data flexibly into an unsupervised, informative representation. We thus present a powerful automated tool to accurately determine prognosis, a key step towards personalized treatment for cancer patients.https://github.com/gevaertlab/{MultimodalPrognosis}},
  annotation2      = {Notes},
  annote           = {model type ``multimodal neural networkbased model'' (Cheerla and Gevaert, 2019, p. 446)},
  creationdate     = {2023-04-12T08:53:23},
  doi              = {10.1093/bioinformatics/btz342},
  keywords         = {done},
  modificationdate = {2023-04-15T18:59:23},
  publisher        = {Oxford University Press ({OUP})},
  shortjournal     = {Bioinformatics},
  urldate          = {2022-11-08},
}

@Article{Calderaro2022Artificial,
  author           = {Calderaro, Julien and Seraphin, Tobias Paul and Luedde, Tom and Simon, Tracey G.},
  journal          = {Journal of Hepatology},
  title            = {Artificial intelligence for the prevention and clinical management of hepatocellular carcinoma},
  year             = {2022},
  issn             = {0168-8278},
  month            = jun,
  note             = {14 citations (Crossref) [2022-11-28]},
  number           = {6},
  pages            = {1348--1361},
  volume           = {76},
  abstract         = {Hepatocellular carcinoma ({HCC}) currently represents the fifth most common malignancy and the third-leading cause of cancer-related death worldwide, with incidence and mortality rates that are increasing. Recently, artificial intelligence ({AI}) has emerged as a unique opportunity to improve the full spectrum of {HCC} clinical care, by improving {HCC} risk prediction, diagnosis, and prognostication. {AI} approaches include computational search algorithms, machine learning ({ML}) and deep learning ({DL}) models. {ML} consists of a computer running repeated iterations of models, in order to progressively improve performance of a specific task, such as classifying an outcome. {DL} models are a subtype of {ML}, based on neural network structures that are inspired by the neuroanatomy of the human brain. A growing body of recent data now apply {DL} models to diverse data sources -- including electronic health record data, imaging modalities, histopathology and molecular biomarkers -- to improve the accuracy of {HCC} risk prediction, detection and prediction of treatment response. Despite the promise of these early results, future research is still needed to standardise {AI} data, and to improve both the generalisability and interpretability of results. If such challenges can be overcome, {AI} has the potential to profoundly change the way in which care is provided to patients with or at risk of {HCC}.},
  creationdate     = {2023-04-12T08:53:32},
  doi              = {10.1016/j.jhep.2022.01.014},
  keywords         = {Deep learning, Machine learning, Artificial intelligence, Liver cancer, Simple},
  langid           = {english},
  modificationdate = {2023-04-15T18:59:23},
  publisher        = {Elsevier {BV}},
  series           = {Breakthroughs in Hepatology},
  shortjournal     = {Journal of Hepatology},
  url              = {https://www.sciencedirect.com/science/article/pii/S0168827822000277},
  urldate          = {2022-11-28},
}

@Article{Breslow1974Covariance,
  author           = {N. Breslow},
  journal          = {Biometrics},
  title            = {Covariance Analysis of Censored Survival Data},
  year             = {1974},
  month            = mar,
  number           = {1},
  pages            = {89},
  volume           = {30},
  creationdate     = {2023-04-12T08:57:01},
  doi              = {10.2307/2529620},
  journaltitle     = {Biometrics},
  modificationdate = {2023-04-15T18:59:23},
  publisher        = {{JSTOR}},
  x-fetchedfrom    = {Google Scholar},
}

@Article{Bejnordi2016Stain,
  author           = {Bejnordi, Babak Ehteshami and Litjens, Geert and Timofeeva, Nadya and Otte-Holler, Irene and Homeyer, Andre and Karssemeijer, Nico and van der Laak, Jeroen A. W. M.},
  journal          = {{IEEE} Transactions on Medical Imaging},
  title            = {Stain Specific Standardization of Whole-Slide Histopathological Images},
  year             = {2016},
  month            = feb,
  number           = {2},
  pages            = {404--415},
  volume           = {35},
  creationdate     = {2023-04-12T08:57:12},
  doi              = {10.1109/tmi.2015.2476509},
  modificationdate = {2023-04-15T18:59:23},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Thesis{Bao2022End,
  abstract         = {The pathogenesis of infectious and severe diseases including {COVID}-19, metabolic disorders, and cancer can be highly complicated because it involves abnormalities in genetic, metabolic, anatomical as well as functional levels. The deteriorative changes could be quantitatively monitored on biochemical markers, genome-wide assays as well as different imaging modalities including radiographic and pathological data. Multimodal medical data, involving three common and essential diagnostic disciplines, i.e., pathology, radiography, and genomics, are increasingly utilized to unravel the complexity of the diseases. High-throughput and deep features can be extracted from different types of medical data to characterize diseases in various quantitative aspects, e.g., compactness and flatness of tumors, and heterogeneity of tissues. 
State-of-the-art deep learning methods including convolutional neural networks ({CNNs}) and Transformer have achieved impressive results in analyses of natural image, text, and voice data through an intrinsic and latent manner. However, there are many obstacles and challenges when applying existing machine learning models that initially tuned on natural image and language data to clinical practice, such as shortage of labeled data, distribution and domain discrepancy, data heterogeneity and imbalance, etc. Moreover, those methods are not designed to harness multimodal data under a unified and end-to-end learning paradigm, making them heavily relying on expert involvement and more prone to be affected by intra- and inter-observer variability. 
To address those limitations, in this thesis, we present novel end-to-end machine learning methods to learn fused feature representations from multimodal medical data, and perform quantitative analyses to identify significant higher-level features from raw medical data in explanation of the characteristics and outcomes of the infectious and severe diseases. 
{\textbullet }	Starting from gold standard pathology images, we propose a bifocal weakly-supervised method which is able to complementarily and simultaneously capture two types of discriminative regions from both shorter and longer image tiles under a small amount of sparsely labeled data to improve recognition and cross-modality analyses of complex morphological and immunohistochemical structures in entire and adjacent multimodal histological slides. 
{\textbullet }	Then, we expand our research on data collected from non-invasive approaches, we present an end-to-end multitask learning model for automated and simultaneous diagnosis and severity assessment of infectious disease which obviates the need for expert involvement, and Shift3D and Random-weighted multitask loss function are two novel algorithm components proposed to learn shift-invariant and shareable representations from fused radiographic imaging and high-throughput numerical data to accelerate model convergence, improve joint learning performance, and resist the influence of intra- and inter-observer variability. 
{\textbullet }	Next, we further involve time-dimension data and invent the machine learning-based method to locate representative imaging features to tackle the problem of non-invasive diagnostic side effects, i.e., radiation, and the low-radiation and non-invasive solution can be used on progression analysis of metabolic disorders over time and evaluation of surgery-induced weight loss effects. 
{\textbullet }	Lastly, we investigate genomic data given genetic disorders can lead to diverse diseases, we build a machine learning pipeline for processing genomic data and analyzing disease prognosis by incorporating statistical power, biological rationale, and machine learning algorithms as a unified prognostic feature extractor. 
We carried out rigorous and extensive experiments on two large public datasets and two private cohorts covering various forms of medical data, e.g., biochemical markers, genomic profiles, radiomic features, radiological and pathological imaging data. The experiments demonstrated that our proposed machine learning approaches are able to achieve better performances compared to corresponding state-of-the-art methods, and subsequently improve the diagnostic and/or prognostic workflows of infectious and severe diseases including {COVID}-19, metabolic disorders, and cancer.},
  author           = {Bao, Guoqing},
  creationdate     = {2023-04-12T09:00:27},
  langid           = {english},
  modificationdate = {2023-04-15T18:59:23},
  note             = {Accepted: 2022-04-21T04:36:22Z},
  rights           = {The author retains copyright of this thesis. It may only be used for the purposes of research and study. It must not be used for any other purposes and may not be transmitted or shared with others without prior permission.},
  title            = {End-to-End Machine Learning Models for Multimodal Medical Data Analysis},
  type             = {Thesis},
  url              = {https://ses.library.usyd.edu.au/handle/2123/28153},
  urldate          = {2022-11-28},
  year             = {2022},
}

@Article{Athiwaratkun2018There,
  author           = {Athiwaratkun, Ben and Finzi, Marc and Izmailov, Pavel and Wilson, Andrew Gordon},
  journal          = {arXiv preprint arXiv:1806.05594},
  title            = {There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average},
  year             = {2018},
  month            = jun,
  abstract         = {Presently the most successful approaches to semi-supervised learning are based on consistency regularization, whereby a model is trained to be robust to small perturbations of its inputs and parameters. To understand consistency regularization, we conceptually explore how loss geometry interacts with training procedures. The consistency loss dramatically improves generalization performance over supervised-only training; however, we show that SGD struggles to converge on the consistency loss and continues to make large steps that lead to changes in predictions on the test data. Motivated by these observations, we propose to train consistency-based methods with Stochastic Weight Averaging (SWA), a recent approach which averages weights along the trajectory of SGD with a modified learning rate schedule. We also propose fast-SWA, which further accelerates convergence by averaging multiple points within each cycle of a cyclical learning rate schedule. With weight averaging, we achieve the best known semi-supervised results on CIFAR-10 and CIFAR-100, over many different quantities of labeled training data. For example, we achieve 5.0% error on CIFAR-10 with only 4000 labels, compared to the previous best result in the literature of 6.3%.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-12T09:01:17},
  doi              = {10.48550/ARXIV.1806.05594},
  eprint           = {1806.05594},
  file             = {:http\://arxiv.org/pdf/1806.05594v3:PDF},
  keywords         = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences},
  modificationdate = {2023-04-15T18:59:23},
  primaryclass     = {cs.LG},
  publisher        = {arXiv},
  x-fetchedfrom    = {Google Scholar},
}

@InBook{Li2021Dual,
  author           = {Li, Bin and Li, Yin and Eliceiri, Kevin},
  pages            = {14318--14328},
  publisher        = {IEEE},
  title            = {Dual-stream Multiple Instance Learning Network for Whole Slide Image Classification with Self-supervised Contrastive Learning},
  year             = {2021},
  month            = jun,
  booktitle        = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  creationdate     = {2023-04-12T22:03:38},
  doi              = {10.1109/cvpr46437.2021.01409},
  modificationdate = {2023-04-15T18:59:24},
}

@Article{Otsu1979Threshold,
  author           = {Nobuyuki Otsu},
  journal          = {{IEEE} Transactions on Systems, Man, and Cybernetics},
  title            = {A Threshold Selection Method from Gray-Level Histograms},
  year             = {1979},
  month            = jan,
  number           = {1},
  pages            = {62--66},
  volume           = {9},
  creationdate     = {2023-04-13T22:43:12},
  doi              = {10.1109/tsmc.1979.4310076},
  modificationdate = {2023-04-15T18:59:24},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Tagliavini2017Transprecision,
  author           = {Tagliavini, Giuseppe and Mach, Stefan and Rossi, Davide and Marongiu, Andrea and Benini, Luca},
  title            = {A Transprecision Floating-Point Platform for Ultra-Low Power Computing},
  year             = {2017},
  month            = nov,
  abstract         = {In modern low-power embedded platforms, floating-point (FP) operations emerge as a major contributor to the energy consumption of compute-intensive applications with large dynamic range. Experimental evidence shows that 50% of the energy consumed by a core and its data memory is related to FP computations. The adoption of FP formats requiring a lower number of bits is an interesting opportunity to reduce energy consumption, since it allows to simplify the arithmetic circuitry and to reduce the memory bandwidth between memory and registers by enabling vectorization. From a theoretical point of view, the adoption of multiple FP types perfectly fits with the principle of transprecision computing, allowing fine-grained control of approximation while meeting specified constraints on the precision of final results. In this paper we propose an extended FP type system with complete hardware support to enable transprecision computing on low-power embedded processors, including two standard formats (binary32 and binary16) and two new formats (binary8 and binary16alt). First, we introduce a software library that enables exploration of FP types by tuning both precision and dynamic range of program variables. Then, we present a methodology to integrate our library with an external tool for precision tuning, and experimental results that highlight the clear benefits of introducing the new formats. Finally, we present the design of a transprecision FP unit capable of handling 8-bit and 16-bit operations in addition to standard 32-bit operations. Experimental results on FP-intensive benchmarks show that up to 90% of FP operations can be safely scaled down to 8-bit or 16-bit formats. Thanks to precision tuning and vectorization, execution time is decreased by 12% and memory accesses are reduced by 27% on average, leading to a reduction of energy consumption up to 30%.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-13T23:01:53},
  doi              = {10.48550/ARXIV.1711.10374},
  eprint           = {1711.10374},
  file             = {:http\://arxiv.org/pdf/1711.10374v1:PDF},
  keywords         = {Hardware Architecture (cs.AR), FOS: Computer and information sciences},
  modificationdate = {2023-04-15T18:59:25},
  primaryclass     = {cs.AR},
  publisher        = {arXiv},
}

@Article{Micikevicius2017Mixed,
  author           = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
  title            = {Mixed Precision Training},
  year             = {2017},
  month            = oct,
  abstract         = {Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-13T23:02:46},
  doi              = {10.48550/ARXIV.1710.03740},
  eprint           = {1710.03740},
  file             = {:http\://arxiv.org/pdf/1710.03740v3:PDF},
  keywords         = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  modificationdate = {2023-04-15T18:59:24},
  primaryclass     = {cs.AI},
  publisher        = {arXiv},
}

@Article{Kalamkar2019Study,
  author           = {Kalamkar, Dhiraj and Mudigere, Dheevatsa and Mellempudi, Naveen and Das, Dipankar and Banerjee, Kunal and Avancha, Sasikanth and Vooturi, Dharma Teja and Jammalamadaka, Nataraj and Huang, Jianyu and Yuen, Hector and Yang, Jiyan and Park, Jongsoo and Heinecke, Alexander and Georganas, Evangelos and Srinivasan, Sudarshan and Kundu, Abhisek and Smelyanskiy, Misha and Kaul, Bharat and Dubey, Pradeep},
  title            = {A Study of BFLOAT16 for Deep Learning Training},
  year             = {2019},
  month            = may,
  abstract         = {This paper presents the first comprehensive empirical study demonstrating the efficacy of the Brain Floating Point (BFLOAT16) half-precision format for Deep Learning training across image classification, speech recognition, language modeling, generative networks and industrial recommendation systems. BFLOAT16 is attractive for Deep Learning training for two reasons: the range of values it can represent is the same as that of IEEE 754 floating-point format (FP32) and conversion to/from FP32 is simple. Maintaining the same range as FP32 is important to ensure that no hyper-parameter tuning is required for convergence; e.g., IEEE 754 compliant half-precision floating point (FP16) requires hyper-parameter tuning. In this paper, we discuss the flow of tensors and various key operations in mixed precision training, and delve into details of operations, such as the rounding modes for converting FP32 tensors to BFLOAT16. We have implemented a method to emulate BFLOAT16 operations in Tensorflow, Caffe2, IntelCaffe, and Neon for our experiments. Our results show that deep learning training using BFLOAT16 tensors achieves the same state-of-the-art (SOTA) results across domains as FP32 tensors in the same number of iterations and with no changes to hyper-parameters.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-13T23:03:05},
  doi              = {10.48550/ARXIV.1905.12322},
  eprint           = {1905.12322},
  file             = {:http\://arxiv.org/pdf/1905.12322v3:PDF},
  keywords         = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  modificationdate = {2023-04-15T18:59:24},
  primaryclass     = {cs.LG},
  publisher        = {arXiv},
}

@Misc{2022Fookes,
  title            = {Fookes and Lars Petersson},
  year             = {2022},
  creationdate     = {2023-04-14T22:31:55},
  journal          = {Comput. Medical Imaging Graph.},
  modificationdate = {2023-04-15T18:59:23},
  pages            = {102027},
  volume           = {95},
}

@Misc{2022Fookesa,
  title            = {Fookes and Lars Petersson},
  year             = {2022},
  creationdate     = {2023-04-14T22:32:05},
  journal          = {Comput. Medical Imaging Graph.},
  modificationdate = {2023-04-15T18:59:23},
  pages            = {102027},
  volume           = {95},
}

@Misc{2022Fookesb,
  title            = {Fookes and Lars Petersson},
  year             = {2022},
  creationdate     = {2023-04-14T22:33:07},
  journal          = {Comput. Medical Imaging Graph.},
  modificationdate = {2023-04-15T18:59:23},
  pages            = {102027},
  volume           = {95},
}

@Article{AhmedtAristizabal2022survey,
  author           = {David Ahmedt-Aristizabal and Mohammad Ali Armin and Simon Denman and Clinton Fookes and Lars Petersson},
  journal          = {Computerized Medical Imaging and Graphics},
  title            = {A survey on graph-based deep learning for computational histopathology},
  year             = {2022},
  month            = jan,
  pages            = {102027},
  volume           = {95},
  creationdate     = {2023-04-14T22:33:58},
  date             = {2022},
  doi              = {10.1016/j.compmedimag.2021.102027},
  modificationdate = {2023-04-15T19:08:35},
  publisher        = {Elsevier {BV}},
}

@Article{Ilse2018Attention,
  author           = {Ilse, Maximilian and Tomczak, Jakub M. and Welling, Max},
  title            = {Attention-based Deep Multiple Instance Learning},
  year             = {2018},
  month            = feb,
  abstract         = {Multiple instance learning (MIL) is a variation of supervised learning where a single class label is assigned to a bag of instances. In this paper, we state the MIL problem as learning the Bernoulli distribution of the bag label where the bag label probability is fully parameterized by neural networks. Furthermore, we propose a neural network-based permutation-invariant aggregation operator that corresponds to the attention mechanism. Notably, an application of the proposed attention-based operator provides insight into the contribution of each instance to the bag label. We show empirically that our approach achieves comparable performance to the best MIL methods on benchmark MIL datasets and it outperforms other methods on a MNIST-based MIL dataset and two real-life histopathology datasets without sacrificing interpretability.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-14T22:38:54},
  date             = {2018},
  doi              = {10.48550/ARXIV.1802.04712},
  eprint           = {1802.04712},
  file             = {:Ilse2018Attention - Attention Based Deep Multiple Instance Learning.pdf:PDF},
  keywords         = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  modificationdate = {2023-04-15T19:06:13},
  primaryclass     = {cs.LG},
  publisher        = {arXiv},
}

@Manual{AperioTechnologies2008Digital,
  title            = {Digital Slides andThird-Party DataInterchange},
  author           = {Aperio Technologies, Inc.},
  month            = dec,
  year             = {2008},
  creationdate     = {2023-04-15T11:14:21},
  date             = {2008-12-09},
  modificationdate = {2023-04-16T00:13:00},
  url              = {https://web.archive.org/web/20120420105738/http://www.aperio.com/documents/api/Aperio_Digital_Slides_and_Third-party_data_interchange.pdf},
  urldate          = {2023-04-15},
}

@Article{NanoStringTechnologies2017Gene,
  author           = {NanoString Technologies, Inc.},
  journal          = {Technical Document},
  title            = {Gene Expression Data Analysis Guidelines},
  year             = {2017},
  creationdate     = {2023-04-15T17:16:19},
  date             = {2017},
  issuetitle       = {MAN-C0011-04},
  modificationdate = {2023-04-15T19:04:36},
  url              = {https://nanostring.com/wp-content/uploads/Gene_Expression_Data_Analysis_Guidelines.pdf},
  urldate          = {2023-04-15},
}

@Article{Gorman2022IO,
  author           = {Kara Gorman},
  journal          = {NanoString},
  title            = {IO 360 Data Analysis Report},
  year             = {2022},
  month            = feb,
  annotator        = {Meghan Hogan},
  creationdate     = {2023-04-15T18:11:25},
  modificationdate = {2023-04-15T18:59:24},
}

@Article{Wissel2022SurvBoard,
  author           = {David Wissel and Nikita Janakarajan and Aayush Grover and Enrico Toniato and Mar{\'{\i}}a Rodr{\'{\i}}guez Mart{\'{\i}}nez and Valentina Boeva},
  journal          = {bioRxiv},
  title            = {{SurvBoard}: Standardised Benchmarking for Multi-omics Cancer Survival Models},
  year             = {2022},
  month            = nov,
  creationdate     = {2023-04-15T19:03:00},
  doi              = {10.1101/2022.11.18.517043},
  modificationdate = {2023-04-28T08:30:12},
  publisher        = {Cold Spring Harbor Laboratory},
}

@InProceedings{Chen2021Multimodal,
  author           = {Richard J. Chen and Ming Y. Lu and Wei-Hung Weng and Tiffany Y. Chen and Drew FK. Williamson and Trevor Manz and Maha Shady and Faisal Mahmood},
  booktitle        = {2021 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
  title            = {Multimodal Co-Attention Transformer for Survival Prediction in Gigapixel Whole Slide Images},
  year             = {2021},
  month            = oct,
  pages            = {3995--4005},
  publisher        = {{IEEE}},
  abstract         = {Survival outcome prediction is a challenging weakly-supervised and ordinal regression task in computational pathology that involves modeling complex interactions within the tumor microenvironment in gigapixel whole slide images (WSIs). Despite recent progress in formulating WSIs as bags for multiple instance learning (MIL), representation learning of entire WSIs remains an open and challenging problem, especially in overcoming: 1) the computational complexity of feature aggregation in large bags, and 2) the data heterogeneity gap in incorporating biological priors such as genomic measurements. In this work, we present a Multimodal Co-Attention Transformer (MCAT) framework that learns an interpretable, dense co-attention mapping between WSIs and genomic features formulated in an embedding space. Inspired by approaches in Visual Question Answering (VQA) that can attribute how word embed-dings attend to salient objects in an image when answering a question, MCAT learns how histology patches attend to genes when predicting patient survival. In addition to visualizing multimodal interactions, our co-attention trans-formation also reduces the space complexity of WSI bags, which enables the adaptation of Transformer layers as a general encoder backbone in MIL. We apply our proposed method on five different cancer datasets (4,730 WSIs, 67 million patches). Our experimental results demonstrate that the proposed method consistently achieves superior performance compared to the state-of-the-art methods.},
  creationdate     = {2023-04-15T19:10:49},
  date             = {2021-10},
  doi              = {10.1109/iccv48922.2021.00398},
  isbn             = {978-1-6654-2813-2},
  issn             = {1550-5499},
  keywords         = {Representation learning, Visualization, Histopathology, Genomics, Predictive models, Transformers, Biological information theory},
  modificationdate = {2023-04-15T19:10:49},
  url              = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=9710773; https://ieeexplore.ieee.org/document/9710773/},
  x-fetchedfrom    = {IEEEXplore},
}

@Article{Evans2022explainability,
  author           = {Theodore Evans and Carl Orge Retzlaff and Christian Gei{\ss}ler and Michaela Kargl and Markus Plass and Heimo Mller and Tim-Rasmus Kiehl and Norman Zerbe and Andreas Holzinger},
  journal          = {Future Generation Computer Systems},
  title            = {The explainability paradox: Challenges for {xAI} in digital pathology},
  year             = {2022},
  month            = aug,
  pages            = {281--296},
  volume           = {133},
  creationdate     = {2023-04-15T19:11:30},
  doi              = {10.1016/j.future.2022.03.009},
  modificationdate = {2023-04-28T08:30:12},
  publisher        = {Elsevier {BV}},
}

@TechReport{Butler2016GeoJSON,
  author           = {H. Butler and M. Daly and A. Doyle and S. Gillies and S. Hagen and T. Schaub},
  title            = {The {GeoJSON} Format},
  year             = {2016},
  month            = aug,
  creationdate     = {2023-04-16T00:10:29},
  doi              = {10.17487/rfc7946},
  modificationdate = {2023-04-28T08:30:08},
  publisher        = {{RFC} Editor},
}

@Article{Bandi2019Detection,
  author           = {Peter Bandi and Oscar Geessink and Quirine Manson and Marcory Van Dijk and Maschenka Balkenhol and Meyke Hermsen and Babak Ehteshami Bejnordi and Byungjae Lee and Kyunghyun Paeng and Aoxiao Zhong and Quanzheng Li and Farhad Ghazvinian Zanjani and Svitlana Zinger and Keisuke Fukuta and Daisuke Komura and Vlado Ovtcharov and Shenghua Cheng and Shaoqun Zeng and Jeppe Thagaard and Anders B. Dahl and Huangjing Lin and Hao Chen and Ludwig Jacobsson and Martin Hedlund and Melih cetin and Eren Halici and Hunter Jackson and Richard Chen and Fabian Both and Jorg Franke and Heidi Kusters-Vandevelde and Willem Vreuls and Peter Bult and Bram van Ginneken and Jeroen van der Laak and Geert Litjens},
  journal          = {{IEEE} Transactions on Medical Imaging},
  title            = {From Detection of Individual Metastases to Classification of Lymph Node Status at the Patient Level: The {CAMELYON}17 Challenge},
  year             = {2019},
  month            = feb,
  number           = {2},
  pages            = {550--560},
  volume           = {38},
  creationdate     = {2023-04-16T12:25:10},
  doi              = {10.1109/tmi.2018.2867350},
  modificationdate = {2023-04-28T08:30:10},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Lu2021Data,
  author           = {Ming Y. Lu and Drew F. K. Williamson and Tiffany Y. Chen and Richard J. Chen and Matteo Barbieri and Faisal Mahmood},
  journal          = {Nature Biomedical Engineering},
  title            = {Data-efficient and weakly supervised computational pathology on whole-slide images},
  year             = {2021},
  month            = mar,
  number           = {6},
  pages            = {555--570},
  volume           = {5},
  creationdate     = {2023-04-16T16:47:42},
  doi              = {10.1038/s41551-020-00682-w},
  modificationdate = {2023-04-28T08:30:12},
  publisher        = {Springer Science and Business Media {LLC}},
}

@Article{CreativeCommons2007Attribution,
  author           = {{Creative Commons}},
  title            = {Attribution-ShareAlike 3.0 Unported},
  year             = {2007},
  creationdate     = {2023-04-17T21:38:27},
  modificationdate = {2023-04-17T21:38:55},
  url              = {https://creativecommons.org/licenses/by-sa/3.0/},
}

@Article{Cmglee2015Illustration,
  author           = {Cmglee},
  title            = {Illustration of an image pyramid with 5 levels.},
  year             = {2015},
  month            = aug,
  creationdate     = {2023-04-17T21:39:08},
  howpublished     = {\url{https://commons.wikimedia.org/wiki/File:Image_pyramid.svg}},
  modificationdate = {2023-04-17T21:39:16},
  publisher        = {Wikimedia Commons},
}

@Article{InkscapeTeam2022Inkscape,
  author           = {{The Inkscape Team}},
  title            = {Inkscape},
  year             = {2022},
  creationdate     = {2023-04-17T21:39:35},
  modificationdate = {2023-04-17T21:42:11},
  organization     = {Inkscape},
  subtitle         = {Draw Freely},
  url              = {https://inkscape.org/},
  version          = {Inkscape 1.2.2},
}

@Article{Klambauer2017Self,
  author           = {Klambauer, Gnter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
  journal          = {Advances in Neural Information Processing Systems 30 (NIPS 2017)},
  title            = {Self-Normalizing Neural Networks},
  year             = {2017},
  month            = jun,
  abstract         = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-17T23:28:52},
  doi              = {10.48550/ARXIV.1706.02515},
  eprint           = {1706.02515},
  file             = {:http\://arxiv.org/pdf/1706.02515v5:PDF},
  keywords         = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:30:09},
  primaryclass     = {cs.LG},
  publisher        = {arXiv},
}

@Article{Roeder2023Netron,
  author           = {Lutz Roeder},
  title            = {{Netron}},
  year             = {2023},
  month            = apr,
  note             = {[Online; accessed 18. Apr. 2023]},
  creationdate     = {2023-04-18T19:37:57},
  modificationdate = {2023-04-18T19:39:11},
  url              = {https://netron.app},
}

@Article{He2015Deep,
  author           = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title            = {Deep Residual Learning for Image Recognition},
  year             = {2015},
  month            = dec,
  abstract         = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-18T20:09:46},
  doi              = {10.48550/ARXIV.1512.03385},
  eprint           = {1512.03385},
  file             = {:http\://arxiv.org/pdf/1512.03385v1:PDF},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:30:08},
  primaryclass     = {cs.CV},
  publisher        = {arXiv},
}

@Article{Zhang2021Dive,
  author           = {Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
  journal          = {arXiv preprint arXiv:2106.11342},
  title            = {Dive into Deep Learning},
  year             = {2021},
  creationdate     = {2023-04-18T20:30:08},
  modificationdate = {2023-04-18T20:30:23},
}

@Article{Li2017Visualizing,
  author           = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  title            = {Visualizing the Loss Landscape of Neural Nets},
  year             = {2017},
  month            = dec,
  abstract         = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-18T20:33:36},
  doi              = {10.48550/ARXIV.1712.09913},
  eprint           = {1712.09913},
  file             = {:http\://arxiv.org/pdf/1712.09913v3:PDF},
  keywords         = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:30:09},
  primaryclass     = {cs.LG},
  publisher        = {arXiv},
}

@Article{Togelius2023Choose,
  author           = {Togelius, Julian and Yannakakis, Georgios N.},
  title            = {Choose Your Weapon: Survival Strategies for Depressed AI Academics},
  year             = {2023},
  month            = mar,
  abstract         = {Are you an AI researcher at an academic institution? Are you anxious you are not coping with the current pace of AI advancements? Do you feel you have no (or very limited) access to the computational and human resources required for an AI research breakthrough? You are not alone; we feel the same way. A growing number of AI academics can no longer find the means and resources to compete at a global scale. This is a somewhat recent phenomenon, but an accelerating one, with private actors investing enormous compute resources into cutting edge AI research. Here, we discuss what you can do to stay competitive while remaining an academic. We also briefly discuss what universities and the private sector could do improve the situation, if they are so inclined. This is not an exhaustive list of strategies, and you may not agree with all of them, but it serves to start a discussion.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-18T21:02:22},
  doi              = {10.48550/ARXIV.2304.06035},
  eprint           = {2304.06035},
  file             = {:http\://arxiv.org/pdf/2304.06035v1:PDF},
  keywords         = {Other Computer Science (cs.OH), Computers and Society (cs.CY), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:30:13},
  primaryclass     = {cs.OH},
  publisher        = {arXiv},
}

@Article{GutierrezPerez2022StainCUT,
  author           = {Gutirrez Prez, Jos Carlos and Otero Baguer, Daniel and Maass, Peter},
  journal          = {Journal of Imaging},
  title            = {StainCUT: Stain Normalization with Contrastive Learning},
  year             = {2022},
  issn             = {2313-433X},
  number           = {7},
  volume           = {8},
  abstract         = {In recent years, numerous deep-learning approaches have been developed for the analysis of histopathology Whole Slide Images (WSI). A recurrent issue is the lack of generalization ability of a model that has been trained with images of one laboratory and then used to analyze images of a different laboratory. This occurs mainly due to the use of different scanners, laboratory procedures, and staining variations. This can produce strong color differences, which change not only the characteristics of the image, such as the contrast, brightness, and saturation, but also create more complex style variations. In this paper, we present a deep-learning solution based on contrastive learning to transfer from one staining style to another: StainCUT. This method eliminates the need to choose a reference frame and does not need paired images with different staining to learn the mapping between the stain distributions. Additionally, it does not rely on the CycleGAN approach, which makes the method efficient in terms of memory consumption and running time. We evaluate the model using two datasets that consist of the same specimens digitized with two different scanners. We also apply it as a preprocessing step for the semantic segmentation of metastases in lymph nodes. The model was trained on data from one of the laboratories and evaluated on data from another. The results validate the hypothesis that stain normalization indeed improves the performance of the model. Finally, we also investigate and compare the application of the stain normalization step during the training of the model and at inference.},
  article-number   = {202},
  creationdate     = {2023-04-18T21:47:07},
  date             = {2022},
  doi              = {10.3390/jimaging8070202},
  modificationdate = {2023-04-18T21:47:49},
  pubmedid         = {35877646},
  url              = {https://www.mdpi.com/2313-433X/8/7/202},
}

@Article{Ning2020Integrative,
  author           = {Ning, Zhenyuan and Pan, Weihao and Chen, Yuting and Xiao, Qing and Zhang, Xinsen and Luo, Jiaxiu and Wang, Jian and Zhang, Yu},
  journal          = {Bioinformatics},
  title            = {{Integrative analysis of cross-modal features for the prognosis prediction of clear cell renal cell carcinoma}},
  year             = {2020},
  issn             = {1367-4803},
  month            = jan,
  number           = {9},
  pages            = {2888--2895},
  volume           = {36},
  abstract         = {{As a highly heterogeneous disease, clear cell renal cell carcinoma (ccRCC) has quite variable clinical behaviors. The prognostic biomarkers play a crucial role in stratifying patients suffering from ccRCC to avoid over- and under-treatment. Researches based on hand-crafted features and single-modal data have been widely conducted to predict the prognosis of ccRCC. However, these experience-dependent methods, neglecting the synergy among multimodal data, have limited capacity to perform accurate prediction. Inspired by complementary information among multimodal data and the successful application of convolutional neural networks (CNNs) in medical image analysis, a novel framework was proposed to improve prediction performance.We proposed a cross-modal feature-based integrative framework, in which deep features extracted from computed tomography/histopathological images by using CNNs were combined with eigengenes generated from functional genomic data, to construct a prognostic model for ccRCC. Results showed that our proposed model can stratify high- and low-risk subgroups with significant difference (P-value \\&lt; 0.05) and outperform the predictive performance of those models based on single-modality features in the independent testing cohort [C-index, 0.808 (0.7280.888)]. In addition, we also explored the relationship between deep image features and eigengenes, and make an attempt to explain deep image features from the view of genomic data. Notably, the integrative framework is available to the task of prognosis prediction of other cancer with matched multimodal data.https://github.com/zhang-de-lab/zhang-lab? from=singlemessageSupplementary data are available at Bioinformatics online.}},
  creationdate     = {2023-04-18T22:52:41},
  date             = {2020-05-01},
  doi              = {10.1093/bioinformatics/btaa056},
  eprint           = {https://academic.oup.com/bioinformatics/article-pdf/36/9/2888/48984773/bioinformatics\_36\_9\_2888.pdf},
  modificationdate = {2023-04-28T08:30:11},
}

@Article{Gensheimer2019scalable,
  author           = {Michael F. Gensheimer and Balasubramanian Narasimhan},
  journal          = {{PeerJ}},
  title            = {A scalable discrete-time survival model for neural networks},
  year             = {2019},
  month            = jan,
  pages            = {e6257},
  volume           = {7},
  creationdate     = {2023-04-18T23:03:22},
  doi              = {10.7717/peerj.6257},
  modificationdate = {2023-04-28T08:30:10},
  publisher        = {{PeerJ}},
}

@Article{Tan2021EfficientNetV2,
  author           = {Tan, Mingxing and Le, Quoc V.},
  journal          = {International Conference on Machine Learning, 2021},
  title            = {EfficientNetV2: Smaller Models and Faster Training},
  year             = {2021},
  month            = apr,
  abstract         = {This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop this family of models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller. Our training can be further sped up by progressively increasing the image size during training, but it often causes a drop in accuracy. To compensate for this accuracy drop, we propose to adaptively adjust regularization (e.g., dropout and data augmentation) as well, such that we can achieve both fast training and good accuracy. With progressive learning, our EfficientNetV2 significantly outperforms previous models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on the same ImageNet21k, our EfficientNetV2 achieves 87.3% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0% accuracy while training 5x-11x faster using the same computing resources. Code will be available at https://github.com/google/automl/tree/master/efficientnetv2.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-19T22:39:15},
  doi              = {10.48550/ARXIV.2104.00298},
  eprint           = {2104.00298},
  file             = {:http\://arxiv.org/pdf/2104.00298v3:PDF},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:30:12},
  primaryclass     = {cs.CV},
  publisher        = {arXiv},
}

@Article{Tan2019EfficientNet,
  author           = {Tan, Mingxing and Le, Quoc V.},
  journal          = {International Conference on Machine Learning, 2019},
  title            = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  year             = {2019},
  month            = may,
  abstract         = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-19T22:39:29},
  doi              = {10.48550/ARXIV.1905.11946},
  eprint           = {1905.11946},
  file             = {:http\://arxiv.org/pdf/1905.11946v5:PDF},
  keywords         = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:30:10},
  primaryclass     = {cs.LG},
  publisher        = {arXiv},
}

@Article{Howard2017MobileNets,
  author           = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  title            = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
  year             = {2017},
  month            = apr,
  abstract         = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-19T22:42:22},
  doi              = {10.48550/ARXIV.1704.04861},
  eprint           = {1704.04861},
  file             = {:http\://arxiv.org/pdf/1704.04861v1:PDF},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:30:09},
  primaryclass     = {cs.CV},
  publisher        = {arXiv},
}

@Article{Sandler2018MobileNetV2,
  author           = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  journal          = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 4510-4520},
  title            = {MobileNetV2: Inverted Residuals and Linear Bottlenecks},
  year             = {2018},
  month            = jan,
  abstract         = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-19T22:42:41},
  doi              = {10.48550/ARXIV.1801.04381},
  eprint           = {1801.04381},
  file             = {:http\://arxiv.org/pdf/1801.04381v4:PDF},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:30:10},
  primaryclass     = {cs.CV},
  publisher        = {arXiv},
}

@Article{Iizuka2020Deep,
  author           = {Osamu Iizuka and Fahdi Kanavati and Kei Kato and Michael Rambeau and Koji Arihiro and Masayuki Tsuneki},
  journal          = {Scientific Reports},
  title            = {Deep Learning Models for Histopathological Classification of Gastric and Colonic Epithelial Tumours},
  year             = {2020},
  month            = jan,
  number           = {1},
  volume           = {10},
  creationdate     = {2023-04-19T22:45:26},
  doi              = {10.1038/s41598-020-58467-9},
  modificationdate = {2023-04-28T08:30:11},
  publisher        = {Springer Science and Business Media {LLC}},
}

@Article{Chistiakov2017CD68/macrosialin,
  author           = {Dimitry A Chistiakov and Murry C Killingsworth and Veronika A Myasoedova and Alexander N Orekhov and Yuri V Bobryshev},
  journal          = {Laboratory Investigation},
  title            = {{CD}68/macrosialin: not just a histochemical marker},
  year             = {2017},
  month            = jan,
  number           = {1},
  pages            = {4--13},
  volume           = {97},
  creationdate     = {2023-04-21T09:18:08},
  doi              = {10.1038/labinvest.2016.116},
  modificationdate = {2023-04-28T08:30:09},
  publisher        = {Elsevier {BV}},
}

@Article{Kelley2014Scavenger,
  author           = {Kelley, Jim L. and Ozment, Tammy R. and Li, Chuanfu and Schweitzer, John B. and Williams, David L.},
  journal          = {Critical reviews in immunology},
  title            = {Scavenger receptor-A (CD204): a two-edged sword in health and disease.},
  year             = {2014},
  issn             = {1040-8401},
  pages            = {241--261},
  volume           = {34},
  abstract         = {Scavenger receptor A (SR-A), also known as the macrophage scavenger receptor and cluster of differentiation 204 (CD204), plays roles in lipid metabolism, atherogenesis, and a number of metabolic processes. However, recent evidence points to important roles for SR-A in inflammation, innate immunity, host defense, sepsis, and ischemic injury. Herein, we review the role of SR-A in inflammation, innate immunity, host defense, sepsis, cardiac and cerebral ischemic injury, Alzheimer's disease, virus recognition and uptake, bone metabolism, and pulmonary injury. Interestingly, SR-A is reported to be host protective in some disease states, but there is also compelling evidence that SR-A plays a role in the pathophysiology of other diseases. These observations of both harmful and beneficial effects of SR-A are discussed here in the framework of inflammation, innate immunity, and endoplasmic reticulum stress.},
  chemicals        = {Scavenger Receptors, Class A},
  citation-subset  = {IM},
  completed        = {2015-01-23},
  country          = {United States},
  creationdate     = {2023-04-21T21:46:31},
  doi              = {10.1615/critrevimmunol.2014010267},
  issn-linking     = {1040-8401},
  issue            = {3},
  keywords         = {Animals; Atherosclerosis, etiology, metabolism; Humans; Immunity, Innate, physiology; Inflammation, etiology, metabolism; Intracellular Space, metabolism; Organ Specificity, genetics; Scavenger Receptors, Class A, chemistry, genetics, metabolism; Sepsis, etiology, metabolism; Signal Transduction; Virus Diseases, etiology, metabolism},
  mid              = {NIHMS626566},
  modificationdate = {2023-04-28T08:30:08},
  nlm-id           = {8914819},
  owner            = {NLM},
  pii              = {7f24896522e1c5c0,5f34f489651d60d0},
  pmc              = {PMC4191651},
  pmid             = {24941076},
  pubmodel         = {Print},
  pubstate         = {ppublish},
  revised          = {2022-03-17},
}

@Article{Yu2006CSR1,
  author           = {Guoying Yu and George C. Tseng and Yan Ping Yu and Tim Gavel and Joel Nelson and Alan Wells and George Michalopoulos and Demetrius Kokkinakis and Jian-Hua Luo},
  journal          = {The American Journal of Pathology},
  title            = {{CSR}1 Suppresses Tumor Growth and Metastasis of Prostate Cancer},
  year             = {2006},
  month            = feb,
  number           = {2},
  pages            = {597--607},
  volume           = {168},
  creationdate     = {2023-04-21T21:48:07},
  doi              = {10.2353/ajpath.2006.050620},
  modificationdate = {2023-04-28T08:30:07},
  publisher        = {Elsevier {BV}},
}

@Article{Zhu2008CSR1,
  author           = {Z-H Zhu and Y P Yu and Y-K Shi and J B Nelson and J-H Luo},
  journal          = {Oncogene},
  title            = {{CSR}1 induces cell death through inactivation of {CPSF}3},
  year             = {2008},
  month            = sep,
  number           = {1},
  pages            = {41--51},
  volume           = {28},
  creationdate     = {2023-04-21T21:48:24},
  doi              = {10.1038/onc.2008.359},
  modificationdate = {2023-04-28T08:30:07},
  publisher        = {Springer Science and Business Media {LLC}},
}

@Article{Raskov2020Cytotoxic,
  author           = {Hans Raskov and Adile Orhan and Jan Pravsgaard Christensen and Ismail Ggenur},
  journal          = {British Journal of Cancer},
  title            = {Cytotoxic {CD}8\textsuperscript{+} T cells in cancer and cancer immunotherapy},
  year             = {2020},
  month            = sep,
  number           = {2},
  pages            = {359--367},
  volume           = {124},
  creationdate     = {2023-04-21T22:03:52},
  doi              = {10.1038/s41416-020-01048-4},
  modificationdate = {2023-04-28T08:30:11},
  publisher        = {Springer Science and Business Media {LLC}},
}

@Book{Murphy2011Janeways,
  author           = {Murphy, Kenneth P. and Murphy, Kenneth},
  publisher        = {Garland Science},
  title            = {Janeway's immunobiology},
  year             = {2011},
  isbn             = {9780815342434},
  creationdate     = {2023-04-21T22:20:03},
  modificationdate = {2023-04-28T08:30:07},
}

@Article{Oh2021Cytotoxic,
  author           = {David Y. Oh and Lawrence Fong},
  journal          = {Immunity},
  title            = {Cytotoxic {CD}4\textsuperscript{+} T~cells in cancer: Expanding the immune effector toolbox},
  year             = {2021},
  month            = dec,
  number           = {12},
  pages            = {2701--2711},
  volume           = {54},
  creationdate     = {2023-04-21T22:25:29},
  doi              = {10.1016/j.immuni.2021.11.015},
  modificationdate = {2023-04-28T08:30:12},
  publisher        = {Elsevier {BV}},
}

@Article{Yang2017FOXP3,
  author           = {Shucai Yang and Yi Liu and Ming-Yue Li and Calvin S. H. Ng and Sheng-li Yang and Shanshan Wang and Chang Zou and Yujuan Dong and Jing Du and Xiang Long and Li-Zhong Liu and Innes Y. P. Wan and Tony Mok and Malcolm J. Underwood and George G. Chen},
  journal          = {Molecular Cancer},
  title            = {{FOXP}3 promotes tumor growth and metastasis by activating Wnt-catenin signaling pathway and {EMT} in non-small cell lung cancer},
  year             = {2017},
  month            = jul,
  number           = {1},
  volume           = {16},
  creationdate     = {2023-04-21T22:31:37},
  doi              = {10.1186/s12943-017-0700-1},
  modificationdate = {2023-04-28T08:30:09},
  publisher        = {Springer Science and Business Media {LLC}},
}

@Article{Tanaka2016Regulatory,
  author           = {Atsushi Tanaka and Shimon Sakaguchi},
  journal          = {Cell Research},
  title            = {Regulatory T cells in cancer immunotherapy},
  year             = {2016},
  month            = dec,
  number           = {1},
  pages            = {109--118},
  volume           = {27},
  creationdate     = {2023-04-21T22:34:45},
  doi              = {10.1038/cr.2016.151},
  modificationdate = {2023-04-28T08:30:08},
  publisher        = {Springer Science and Business Media {LLC}},
}

@Article{Simonyan2013Deep,
  author           = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  title            = {Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps},
  year             = {2013},
  month            = dec,
  abstract         = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-22T11:38:01},
  doi              = {10.48550/ARXIV.1312.6034},
  eprint           = {1312.6034},
  file             = {:http\://arxiv.org/pdf/1312.6034v2:PDF},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:30:08},
  primaryclass     = {cs.CV},
  publisher        = {arXiv},
}

@Article{Shrikumar2016Not,
  author           = {Shrikumar, Avanti and Greenside, Peyton and Shcherbina, Anna and Kundaje, Anshul},
  title            = {Not Just a Black Box: Learning Important Features Through Propagating Activation Differences},
  year             = {2016},
  month            = may,
  abstract         = {Note: This paper describes an older version of DeepLIFT. See https://arxiv.org/abs/1704.02685 for the newer version. Original abstract follows: The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Learning Important FeaTures), an efficient and effective method for computing importance scores in a neural network. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. We apply DeepLIFT to models trained on natural images and genomic data, and show significant advantages over gradient-based methods.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-22T11:47:06},
  doi              = {10.48550/ARXIV.1605.01713},
  eprint           = {1605.01713},
  file             = {:http\://arxiv.org/pdf/1605.01713v3:PDF},
  keywords         = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:30:08},
  primaryclass     = {cs.LG},
  publisher        = {arXiv},
}

@Article{Bach2015Pixel,
  author           = {Sebastian Bach and Alexander Binder and Gr{\'{e}}goire Montavon and Frederick Klauschen and Klaus-Robert Mller and Wojciech Samek},
  journal          = {{PLOS} {ONE}},
  title            = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
  year             = {2015},
  month            = jul,
  number           = {7},
  pages            = {e0130140},
  volume           = {10},
  creationdate     = {2023-04-22T14:05:54},
  doi              = {10.1371/journal.pone.0130140},
  editor           = {Oscar Deniz Suarez},
  modificationdate = {2023-04-28T08:30:08},
  publisher        = {Public Library of Science ({PLoS})},
}

@Article{Springenberg2014Striving,
  author           = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
  title            = {Striving for Simplicity: The All Convolutional Net},
  year             = {2014},
  month            = dec,
  abstract         = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-22T17:24:28},
  doi              = {10.48550/ARXIV.1412.6806},
  eprint           = {1412.6806},
  file             = {:http\://arxiv.org/pdf/1412.6806v3:PDF},
  keywords         = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:30:08},
  primaryclass     = {cs.LG},
  publisher        = {arXiv},
}

@Article{Kondo2022Two,
  author           = {Kondo, Satoshi and Kasai, Satoshi and Hirasawa, Kousuke},
  title            = {A Two Step Approach for Whole Slide Image Registration},
  year             = {2022},
  month            = aug,
  abstract         = {Multi-stain whole-slide-image (WSI) registration is an active field of research. It is unclear, however, how the current WSI registration methods would perform on a real-world data set. AutomatiC Registration Of Breast cAncer Tissue (ACROBAT) challenge is held to verify the performance of the current WSI registration methods by using a new dataset that originates from routine diagnostics to assess real-world applicability. In this report, we present our solution for the ACROBAT challenge. We employ a two-step approach including rigid and non-rigid transforms. The experimental results show that the median 90th percentile is 1,250 um for the validation dataset.},
  archiveprefix    = {arXiv},
  copyright        = {Creative Commons Attribution Share Alike 4.0 International},
  creationdate     = {2023-04-22T21:58:16},
  doi              = {10.48550/ARXIV.2208.12635},
  eprint           = {2208.12635},
  file             = {:http\://arxiv.org/pdf/2208.12635v1:PDF},
  keywords         = {Image and Video Processing (eess.IV), Computer Vision and Pattern Recognition (cs.CV), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:30:13},
  primaryclass     = {eess.IV},
  publisher        = {arXiv},
}

@Article{Solorzano2019Whole,
  author           = {Solorzano, Leslie and Almeida, Gabriela M. and Mesquita, Brbara and Martins, Diana and Oliveira, Carla and Whlby, Carolina},
  journal          = {vol 11039, 2018, p95-102},
  title            = {Whole slide image registration for the study of tumor heterogeneity},
  year             = {2019},
  month            = jan,
  pages            = {95--102},
  abstract         = {Consecutive thin sections of tissue samples make it possible to study local variation in e.g. protein expression and tumor heterogeneity by staining for a new protein in each section. In order to compare and correlate patterns of different proteins, the images have to be registered with high accuracy. The problem we want to solve is registration of gigapixel whole slide images (WSI). This presents 3 challenges: (i) Images are very large; (ii) Thin sections result in artifacts that make global affine registration prone to very large local errors; (iii) Local affine registration is required to preserve correct tissue morphology (local size, shape and texture). In our approach we compare WSI registration based on automatic and manual feature selection on either the full image or natural sub-regions (as opposed to square tiles). Working with natural sub-regions, in an interactive tool makes it possible to exclude regions containing scientifically irrelevant information. We also present a new way to visualize local registration quality by a Registration Confidence Map (RCM). With this method, intra-tumor heterogeneity and charateristics of the tumor microenvironment can be observed and quantified.},
  archiveprefix    = {arXiv},
  booktitle        = {Computational Pathology and Ophthalmic Medical Image Analysis},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-22T22:06:42},
  doi              = {10.1007/978-3-030-00949-6_12},
  eprint           = {1901.08317},
  file             = {:http\://arxiv.org/pdf/1901.08317v1:PDF},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), Computers and Society (cs.CY), FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:30:11},
  primaryclass     = {cs.CV},
  publisher        = {Springer International Publishing},
}

@Article{Khened2021generalized,
  author           = {Mahendra Khened and Avinash Kori and Haran Rajkumar and Ganapathy Krishnamurthi and Balaji Srinivasan},
  journal          = {Scientific Reports},
  title            = {A generalized deep learning framework for whole-slide image segmentation and analysis},
  year             = {2021},
  month            = jun,
  number           = {1},
  volume           = {11},
  creationdate     = {2023-04-22T22:15:33},
  doi              = {10.1038/s41598-021-90444-8},
  modificationdate = {2023-04-28T08:30:12},
  publisher        = {Springer Science and Business Media {LLC}},
}

@Article{Levy2020PathFlow,
  author           = {Joshua J. Levy and Christopher R. Jackson and Christian C. Haudenschild and Brock C. Christensen and Louis J. Vaickus},
  title            = {{PathFlow}-{MixMatch} for Whole Slide Image Registration: An Investigation of a Segment-Based Scalable Image Registration Method},
  year             = {2020},
  month            = mar,
  creationdate     = {2023-04-22T22:16:11},
  doi              = {10.1101/2020.03.22.002402},
  modificationdate = {2023-04-28T08:30:11},
  publisher        = {Cold Spring Harbor Laboratory},
}

@Article{Chiaruttini2022Open,
  author           = {Nicolas Chiaruttini and Olivier Burri and Peter Haub and Romain Guiet and Jessica Sordet-Dessimoz and Arne Seitz},
  journal          = {Frontiers in Computer Science},
  title            = {An Open-Source Whole Slide Image Registration Workflow at Cellular Precision Using Fiji, {QuPath} and Elastix},
  year             = {2022},
  month            = jan,
  volume           = {3},
  creationdate     = {2023-04-22T22:16:52},
  doi              = {10.3389/fcomp.2021.780026},
  modificationdate = {2023-04-28T08:30:13},
  publisher        = {Frontiers Media {SA}},
}

@Article{Hoque2022Whole,
  author           = {Md. Ziaul Hoque and Anja Keskinarkaus and Pia Nyberg and Taneli Mattila and Tapio Seppnen},
  journal          = {Computers in Biology and Medicine},
  title            = {Whole slide image registration via multi-stained feature matching},
  year             = {2022},
  month            = may,
  pages            = {105301},
  volume           = {144},
  creationdate     = {2023-04-22T22:17:56},
  doi              = {10.1016/j.compbiomed.2022.105301},
  modificationdate = {2023-04-28T08:30:13},
  publisher        = {Elsevier {BV}},
}

@Misc{Marzahl2023qt,
  author           = {Marzahl, Christian and Aubreville, Marc},
  month            = apr,
  note             = {[Online; accessed 22. Apr. 2023]},
  title            = {{qt-wsi-registration}},
  year             = {2023},
  creationdate     = {2023-04-22T22:31:47},
  journal          = {PyPI},
  modificationdate = {2023-04-22T22:32:52},
  url              = {https://pypi.org/project/qt-wsi-registration},
}

@Article{Celik2021Extracting,
  author           = {Yusuf {\c{C}}elik and Murat Karabatak},
  journal          = {Expert Systems},
  title            = {Extracting low dimensional representations from large size whole slide images using deep convolutional autoencoders},
  year             = {2021},
  month            = sep,
  number           = {4},
  volume           = {40},
  creationdate     = {2023-04-23T14:45:33},
  doi              = {10.1111/exsy.12819},
  modificationdate = {2023-04-28T08:30:12},
  publisher        = {Wiley},
}

@Article{Quan2022Global,
  author           = {Quan, Hao and Li, Xingyu and Chen, Weixing and Bai, Qun and Zou, Mingchen and Yang, Ruijie and Zheng, Tingting and Qi, Ruiqun and Gao, Xinghua and Cui, Xiaoyu},
  title            = {Global Contrast Masked Autoencoders Are Powerful Pathological Representation Learners},
  year             = {2022},
  month            = may,
  abstract         = {Based on digital whole slide scanning technique, artificial intelligence algorithms represented by deep learning have achieved remarkable results in the field of computational pathology. Compared with other medical images such as Computed Tomography (CT) or Magnetic Resonance Imaging (MRI), pathological images are more difficult to annotate, thus there is an extreme lack of data sets that can be used for supervised learning. In this study, a self-supervised learning (SSL) model, Global Contrast Masked Autoencoders (GCMAE), is proposed, which has the ability to represent both global and local domain-specific features of whole slide image (WSI), as well as excellent cross-data transfer ability. The Camelyon16 and NCTCRC datasets are used to evaluate the performance of our model. When dealing with transfer learning tasks with different data sets, the experimental results show that GCMAE has better linear classification accuracy than MAE, which can reach 81.10% and 89.22% respectively. Our method outperforms the previous state-of-the-art algorithm and even surpass supervised learning (improved by 3.86% on NCTCRC data sets). The source code of this paper is publicly available at https://github.com/StarUniversus/gcmae},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-23T15:26:04},
  doi              = {10.48550/ARXIV.2205.09048},
  eprint           = {2205.09048},
  file             = {:http\://arxiv.org/pdf/2205.09048v2:PDF},
  keywords         = {Image and Video Processing (eess.IV), Computer Vision and Pattern Recognition (cs.CV), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:30:13},
  primaryclass     = {eess.IV},
  publisher        = {arXiv},
}

@Article{Hecht2020Disentangled,
  author           = {Helge Hecht and Mhd Hasan Sarhan and Vlad Popovici},
  journal          = {Applied Sciences},
  title            = {Disentangled Autoencoder for Cross-Stain Feature Extraction in Pathology Image Analysis},
  year             = {2020},
  month            = sep,
  number           = {18},
  pages            = {6427},
  volume           = {10},
  creationdate     = {2023-04-23T15:26:42},
  doi              = {10.3390/app10186427},
  modificationdate = {2023-04-28T08:30:11},
  publisher        = {{MDPI} {AG}},
}

@Article{Sun2022Deep,
  author           = {Caixia Sun and Bingbing Li and Genxia Wei and Weihao Qiu and Danyi Li and Xiangzhao Li and Xiangyu Liu and Wei Wei and Shuo Wang and Zhenyu Liu and Jie Tian and Li Liang},
  journal          = {Computer Methods and Programs in Biomedicine},
  title            = {Deep learning with whole slide images can improve the prognostic risk stratification with stage {III} colorectal cancer},
  year             = {2022},
  month            = jun,
  pages            = {106914},
  volume           = {221},
  creationdate     = {2023-04-23T15:29:50},
  doi              = {10.1016/j.cmpb.2022.106914},
  modificationdate = {2023-04-28T08:30:13},
  publisher        = {Elsevier {BV}},
}

@Article{Roy2021Convolutional,
  author           = {Mousumi Roy and Jun Kong and Satyananda Kashyap and Vito Paolo Pastore and Fusheng Wang and Ken C. L. Wong and Vandana Mukherjee},
  journal          = {Scientific Reports},
  title            = {Convolutional autoencoder based model {HistoCAE} for segmentation of viable tumor regions in liver whole-slide images},
  year             = {2021},
  month            = jan,
  number           = {1},
  volume           = {11},
  creationdate     = {2023-04-23T15:34:16},
  doi              = {10.1038/s41598-020-80610-9},
  modificationdate = {2023-04-28T08:30:12},
  publisher        = {Springer Science and Business Media {LLC}},
}

@Article{Wilson2003general,
  author           = {D.Randall Wilson and Tony R. Martinez},
  journal          = {Neural Networks},
  title            = {The general inefficiency of batch training for gradient descent learning},
  year             = {2003},
  month            = dec,
  number           = {10},
  pages            = {1429--1451},
  volume           = {16},
  creationdate     = {2023-04-23T15:50:25},
  doi              = {10.1016/s0893-6080(03)00138-2},
  modificationdate = {2023-04-28T08:30:07},
  publisher        = {Elsevier {BV}},
}

@Article{Smith2017Dont,
  author           = {Smith, Samuel L. and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V.},
  title            = {Don't Decay the Learning Rate, Increase the Batch Size},
  year             = {2017},
  month            = nov,
  abstract         = {It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate $\epsilon$ and scaling the batch size $B \propto \epsilon$. Finally, one can increase the momentum coefficient $m$ and scale $B \propto 1/(1-m)$, although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train ResNet-50 on ImageNet to $76.1\%$ validation accuracy in under 30 minutes.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-23T15:51:21},
  doi              = {10.48550/ARXIV.1711.00489},
  eprint           = {1711.00489},
  file             = {:http\://arxiv.org/pdf/1711.00489v2:PDF},
  keywords         = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Distributed / Parallel / Cluster Computing (cs.DC), Machine Learning (stat.ML), FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:30:10},
  primaryclass     = {cs.LG},
  publisher        = {arXiv},
}

@Article{Oord2017Neural,
  author           = {Oord, Aaron van den and Vinyals, Oriol and Kavukcuoglu, Koray},
  title            = {Neural Discrete Representation Learning},
  year             = {2017},
  month            = nov,
  abstract         = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-23T15:55:52},
  doi              = {10.48550/ARXIV.1711.00937},
  eprint           = {1711.00937},
  file             = {:http\://arxiv.org/pdf/1711.00937v2:PDF},
  keywords         = {Machine Learning (cs.LG), FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:30:10},
  primaryclass     = {cs.LG},
  publisher        = {arXiv},
}

@Article{Razavi2019Generating,
  author           = {Razavi, Ali and Oord, Aaron van den and Vinyals, Oriol},
  title            = {Generating Diverse High-Fidelity Images with VQ-VAE-2},
  year             = {2019},
  month            = jun,
  abstract         = {We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-23T16:10:36},
  doi              = {10.48550/ARXIV.1906.00446},
  eprint           = {1906.00446},
  file             = {:http\://arxiv.org/pdf/1906.00446v1:PDF},
  keywords         = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:30:11},
  primaryclass     = {cs.LG},
  publisher        = {arXiv},
}

@Article{Rajpurkar2017CheXNet,
  author           = {Rajpurkar, Pranav and Irvin, Jeremy and Zhu, Kaylie and Yang, Brandon and Mehta, Hershel and Duan, Tony and Ding, Daisy and Bagul, Aarti and Langlotz, Curtis and Shpanskaya, Katie and Lungren, Matthew P. and Ng, Andrew Y.},
  title            = {CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning},
  year             = {2017},
  month            = nov,
  abstract         = {We develop an algorithm that can detect pneumonia from chest X-rays at a level exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer convolutional neural network trained on ChestX-ray14, currently the largest publicly available chest X-ray dataset, containing over 100,000 frontal-view X-ray images with 14 diseases. Four practicing academic radiologists annotate a test set, on which we compare the performance of CheXNet to that of radiologists. We find that CheXNet exceeds average radiologist performance on the F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and achieve state of the art results on all 14 diseases.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-23T16:24:57},
  doi              = {10.48550/ARXIV.1711.05225},
  eprint           = {1711.05225},
  file             = {:http\://arxiv.org/pdf/1711.05225v3:PDF},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:30:10},
  primaryclass     = {cs.CV},
  publisher        = {arXiv},
}

@Article{Chen2022Fasta,
  author           = {Chengkuan Chen and Ming Y. Lu and Drew F. K. Williamson and Tiffany Y. Chen and Andrew J. Schaumberg and Faisal Mahmood},
  journal          = {Nature Biomedical Engineering},
  title            = {Fast and scalable search of whole-slide images via self-supervised deep learning},
  year             = {2022},
  month            = oct,
  number           = {12},
  pages            = {1420--1434},
  volume           = {6},
  creationdate     = {2023-04-23T16:26:28},
  doi              = {10.1038/s41551-022-00929-8},
  modificationdate = {2023-04-28T08:30:13},
  publisher        = {Springer Science and Business Media {LLC}},
}

@Article{Pinckaers2019Streaming,
  author           = {Pinckaers, Hans and van Ginneken, Bram and Litjens, Geert},
  journal          = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
  title            = {Streaming convolutional neural networks for end-to-end learning with multi-megapixel images},
  year             = {2019},
  month            = mar,
  number           = {3},
  pages            = {1581--1590},
  volume           = {44},
  abstract         = {Due to memory constraints on current hardware, most convolution neural networks (CNN) are trained on sub-megapixel images. For example, most popular datasets in computer vision contain images much less than a megapixel in size (0.09MP for ImageNet and 0.001MP for CIFAR-10). In some domains such as medical imaging, multi-megapixel images are needed to identify the presence of disease accurately. We propose a novel method to directly train convolutional neural networks using any input image size end-to-end. This method exploits the locality of most operations in modern convolutional neural networks by performing the forward and backward pass on smaller tiles of the image. In this work, we show a proof of concept using images of up to 66-megapixels (8192x8192), saving approximately 50GB of memory per image. Using two public challenge datasets, we demonstrate that CNNs can learn to extract relevant information from these large images and benefit from increasing resolution. We improved the area under the receiver-operating characteristic curve from 0.580 (4MP) to 0.706 (66MP) for metastasis detection in breast cancer (CAMELYON17). We also obtained a Spearman correlation metric approaching state-of-the-art performance on the TUPAC16 dataset, from 0.485 (1MP) to 0.570 (16MP). Code to reproduce a subset of the experiments is available at https://github.com/DIAGNijmegen/StreamingCNN.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-23T16:52:52},
  date             = {2019-11-11},
  doi              = {10.1109/tpami.2020.3019563},
  eprint           = {1911.04432},
  file             = {:http\://arxiv.org/pdf/1911.04432v1:PDF},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:30:11},
  primaryclass     = {cs.CV},
  publisher        = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Akiba2019Optuna,
  author           = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  title            = {Optuna: A Next-generation Hyperparameter Optimization Framework},
  year             = {2019},
  month            = jul,
  abstract         = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-23T17:50:56},
  doi              = {10.48550/ARXIV.1907.10902},
  eprint           = {1907.10902},
  file             = {:http\://arxiv.org/pdf/1907.10902v1:PDF},
  keywords         = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:30:11},
  primaryclass     = {cs.LG},
  publisher        = {arXiv},
}

@Article{Liao2022Empirical,
  author           = {Lizhi Liao and Heng Li and Weiyi Shang and Lei Ma},
  journal          = {{ACM} Transactions on Software Engineering and Methodology},
  title            = {An Empirical Study of the Impact of Hyperparameter Tuning and Model Optimization on the Performance Properties of Deep Neural Networks},
  year             = {2022},
  month            = apr,
  number           = {3},
  pages            = {1--40},
  volume           = {31},
  creationdate     = {2023-04-23T18:02:41},
  doi              = {10.1145/3506695},
  modificationdate = {2023-04-28T08:30:13},
  publisher        = {Association for Computing Machinery ({ACM})},
}

@Article{Hansen2016CMA,
  author           = {Hansen, Nikolaus},
  title            = {The CMA Evolution Strategy: A Tutorial},
  year             = {2016},
  month            = apr,
  abstract         = {This tutorial introduces the CMA Evolution Strategy (ES), where CMA stands for Covariance Matrix Adaptation. The CMA-ES is a stochastic, or randomized, method for real-parameter (continuous domain) optimization of non-linear, non-convex functions. We try to motivate and derive the algorithm from intuitive concepts and from requirements of non-linear, non-convex search in continuous domain.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-23T18:11:22},
  doi              = {10.48550/ARXIV.1604.00772},
  eprint           = {1604.00772},
  file             = {:http\://arxiv.org/pdf/1604.00772v2:PDF},
  keywords         = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:30:09},
  primaryclass     = {cs.LG},
  publisher        = {arXiv},
}

@InCollection{Feurer2019Hyperparameter,
  author           = {Matthias Feurer and Frank Hutter},
  booktitle        = {Automated Machine Learning},
  publisher        = {Springer International Publishing},
  title            = {Hyperparameter Optimization},
  year             = {2019},
  pages            = {3--33},
  creationdate     = {2023-04-23T19:55:32},
  doi              = {10.1007/978-3-030-05318-5_1},
  modificationdate = {2023-04-28T08:30:11},
}

@Article{Hertel2020Quantity,
  author           = {Hertel, Lars and Baldi, Pierre and Gillen, Daniel L.},
  title            = {Quantity vs. Quality: On Hyperparameter Optimization for Deep Reinforcement Learning},
  year             = {2020},
  month            = jul,
  abstract         = {Reinforcement learning algorithms can show strong variation in performance between training runs with different random seeds. In this paper we explore how this affects hyperparameter optimization when the goal is to find hyperparameter settings that perform well across random seeds. In particular, we benchmark whether it is better to explore a large quantity of hyperparameter settings via pruning of bad performers, or if it is better to aim for quality of collected results by using repetitions. For this we consider the Successive Halving, Random Search, and Bayesian Optimization algorithms, the latter two with and without repetitions. We apply these to tuning the PPO2 algorithm on the Cartpole balancing task and the Inverted Pendulum Swing-up task. We demonstrate that pruning may negatively affect the optimization and that repeated sampling does not help in finding hyperparameter settings that perform better across random seeds. From our experiments we conclude that Bayesian optimization with a noise robust acquisition function is the best choice for hyperparameter optimization in reinforcement learning tasks.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-23T20:41:58},
  doi              = {10.48550/ARXIV.2007.14604},
  eprint           = {2007.14604},
  file             = {:http\://arxiv.org/pdf/2007.14604v2:PDF},
  keywords         = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:30:11},
  primaryclass     = {cs.LG},
  publisher        = {arXiv},
}

@Article{Yu2020Hyper,
  author           = {Yu, Tong and Zhu, Hong},
  title            = {Hyper-Parameter Optimization: A Review of Algorithms and Applications},
  year             = {2020},
  month            = mar,
  abstract         = {Since deep neural networks were developed, they have made huge contributions to everyday lives. Machine learning provides more rational advice than humans are capable of in almost every aspect of daily life. However, despite this achievement, the design and training of neural networks are still challenging and unpredictable procedures. To lower the technical thresholds for common users, automated hyper-parameter optimization (HPO) has become a popular topic in both academic and industrial areas. This paper provides a review of the most essential topics on HPO. The first section introduces the key hyper-parameters related to model training and structure, and discusses their importance and methods to define the value range. Then, the research focuses on major optimization algorithms and their applicability, covering their efficiency and accuracy especially for deep learning networks. This study next reviews major services and toolkits for HPO, comparing their support for state-of-the-art searching algorithms, feasibility with major deep learning frameworks, and extensibility for new modules designed by users. The paper concludes with problems that exist when HPO is applied to deep learning, a comparison between optimization algorithms, and prominent approaches for model evaluation with limited computational resources.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-23T21:07:17},
  doi              = {10.48550/ARXIV.2003.05689},
  eprint           = {2003.05689},
  file             = {:http\://arxiv.org/pdf/2003.05689v1:PDF},
  keywords         = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:30:11},
  primaryclass     = {cs.LG},
  publisher        = {arXiv},
}

@Article{Zeiler2013Visualizing,
  author           = {Zeiler, Matthew D and Fergus, Rob},
  title            = {Visualizing and Understanding Convolutional Networks},
  year             = {2013},
  month            = nov,
  abstract         = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-25T23:23:24},
  doi              = {10.48550/ARXIV.1311.2901},
  eprint           = {1311.2901},
  file             = {:http\://arxiv.org/pdf/1311.2901v3:PDF},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:30:08},
  primaryclass     = {cs.CV},
  publisher        = {arXiv},
}

@Article{Bankhead2017QuPath,
  author           = {Peter Bankhead and Maurice B. Loughrey and Jos{\'{e}} A. Fern{\'{a}}ndez and Yvonne Dombrowski and Darragh G. McArt and Philip D. Dunne and Stephen McQuaid and Ronan T. Gray and Liam J. Murray and Helen G. Coleman and Jacqueline A. James and Manuel Salto-Tellez and Peter W. Hamilton},
  journal          = {Scientific Reports},
  title            = {{QuPath}: Open source software for digital pathology image analysis},
  year             = {2017},
  month            = dec,
  number           = {1},
  volume           = {7},
  creationdate     = {2023-04-16T00:03:37},
  date             = {2017-12},
  doi              = {10.1038/s41598-017-17204-5},
  journaltitle     = {Scientific Reports},
  modificationdate = {2023-04-28T08:30:09},
  publisher        = {Springer Science and Business Media {LLC}},
}

@Article{Thung2017Multi,
  author           = {Thung, Kim-Han and Yap, Pew-Thian and Shen, Dinggang},
  journal          = {Deep learning in medical image analysis and multimodal learning for clinical decision support: Third International Workshop, {DLMIA} 2017, and 7th International Workshop, {ML}-{CDS} 2017, held in conjunction with {MICCAI} 2017 Quebec City, {QC},...},
  title            = {Multi-stage Diagnosis of Alzheimer's Disease with Incomplete Multimodal Data via Multi-task Deep Learning},
  year             = {2017},
  month            = sep,
  note             = {17 citations (Crossref) [2022-11-29]},
  pages            = {160--168},
  volume           = {10553},
  abstract         = {Utilization of biomedical data from multiple modalities improves the diagnostic accuracy of neurodegenerative diseases. However, multi-modality data are often incomplete because not all data can be collected for every individual. When using such incomplete data for diagnosis, current approaches for addressing the problem of missing data, such as imputation, matrix completion and multi-task learning, implicitly assume linear data-to-label relationship, therefore limiting their performances. We thus propose multi-task deep learning for incomplete data, where prediction tasks that are associated with different modality combinations are learnt jointly to improve the performance of each task. Specifically, we devise a multi-input multi-output deep learning framework, and train our deep network subnet-wise, partially updating its weights based on the availability of modality data. The experimental results using the {ADNI} dataset show that our method outperforms the state-of-the-art methods.},
  booktitle        = {Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support},
  date             = {2017-09},
  doi              = {10.1007/978-3-319-67558-9_19},
  modificationdate = {2023-04-28T08:30:09},
  pmcid            = {PMC5666687},
  pmid             = {29104963},
  publisher        = {Springer International Publishing},
  shortjournal     = {Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2017)},
}

@InBook{Ash2017Authors,
  author           = {Rick Ash and David A. Morton and Sheryl A. Scott},
  publisher        = {McGraw-Hill Medical},
  title            = {About The Authors},
  year             = {2017},
  address          = {New York, NY},
  booktitle        = {The Big Picture: Histology},
  creationdate     = {2023-04-28T08:43:18},
  modificationdate = {2023-04-28T08:43:36},
  url              = {accessbiomedicalscience.mhmedical.com/content.aspx?aid=1141044529},
}

@Article{Zhuo2017Alternative,
  author           = {Li Zhuo and Haifeng Wang and Dapeng Chen and Haitao Lu and Guming Zou and Wenge Li},
  journal          = {International Urology and Nephrology},
  title            = {Alternative renal biopsies: past and present},
  year             = {2017},
  month            = jul,
  number           = {3},
  pages            = {475--479},
  volume           = {50},
  creationdate     = {2023-04-28T08:44:01},
  doi              = {10.1007/s11255-017-1668-x},
  modificationdate = {2023-04-28T08:44:06},
  publisher        = {Springer Science and Business Media {LLC}},
}

@Article{Herr2008Surgical,
  author           = {Harry W. Herr},
  journal          = {Urologic Clinics of North America},
  title            = {Surgical Management of Renal Tumors: A Historical Perspective},
  year             = {2008},
  month            = nov,
  number           = {4},
  pages            = {543--549},
  volume           = {35},
  creationdate     = {2023-04-28T08:44:17},
  doi              = {10.1016/j.ucl.2008.07.010},
  modificationdate = {2023-04-28T08:44:35},
  publisher        = {Elsevier {BV}},
}

@Article{Gansner2000Open,
  author           = {Gansner, Emden R. and North, Stephen C.},
  journal          = {Softw. Pract. Exper.},
  title            = {An Open Graph Visualization System and Its Applications to Software Engineering},
  year             = {2000},
  issn             = {0038-0644},
  month            = sep,
  number           = {11},
  pages            = {1203--1233},
  volume           = {30},
  address          = {USA},
  creationdate     = {2023-04-28T08:46:43},
  issue_date       = {Sept. 2000},
  keywords         = {graph visualization, software engineering, open systems},
  modificationdate = {2023-04-28T08:46:58},
  numpages         = {31},
  publisher        = {John Wiley &amp; Sons, Inc.},
}

@Article{Tenenbaum2000Separating,
  author           = {Joshua B. Tenenbaum and William T. Freeman},
  journal          = {Neural Computation},
  title            = {Separating Style and Content with Bilinear Models},
  year             = {2000},
  month            = jun,
  number           = {6},
  pages            = {1247--1283},
  volume           = {12},
  creationdate     = {2023-04-28T08:47:13},
  doi              = {10.1162/089976600300015349},
  modificationdate = {2023-04-28T08:47:26},
  publisher        = {{MIT} Press - Journals},
}

@Article{Reda2018Deep,
  author           = {Reda, Islam and Khalil, Ashraf and Elmogy, Mohammed and Abou El-Fetouh, Ahmed and Shalaby, Ahmed and Abou El-Ghar, Mohamed and Elmaghraby, Adel and Ghazal, Mohammed and El-Baz, Ayman},
  journal          = {Technology in cancer research \& treatment},
  title            = {Deep learning role in early diagnosis of prostate cancer},
  year             = {2018},
  pages            = {1533034618775530},
  volume           = {17},
  creationdate     = {2023-04-28T08:47:50},
  modificationdate = {2023-04-28T08:48:06},
  publisher        = {SAGE Publications Sage CA: Los Angeles, CA},
}

@Article{Morvant2014Majority,
  author           = {Morvant, Emilie and Habrard, Amaury and Ayache, Stphane},
  title            = {Majority Vote of Diverse Classifiers for Late Fusion},
  year             = {2014},
  month            = apr,
  abstract         = {In the past few years, a lot of attention has been devoted to multimedia indexing by fusing multimodal informations. Two kinds of fusion schemes are generally considered: The early fusion and the late fusion. We focus on late classifier fusion, where one combines the scores of each modality at the decision level. To tackle this problem, we investigate a recent and elegant well-founded quadratic program named MinCq coming from the machine learning PAC-Bayesian theory. MinCq looks for the weighted combination, over a set of real-valued functions seen as voters, leading to the lowest misclassification rate, while maximizing the voters' diversity. We propose an extension of MinCq tailored to multimedia indexing. Our method is based on an order-preserving pairwise loss adapted to ranking that allows us to improve Mean Averaged Precision measure while taking into account the diversity of the voters that we want to fuse. We provide evidence that this method is naturally adapted to late fusion procedures and confirm the good behavior of our approach on the challenging PASCAL VOC'07 benchmark.},
  archiveprefix    = {arXiv},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2023-04-28T08:48:29},
  doi              = {10.48550/ARXIV.1404.7796},
  eprint           = {1404.7796},
  file             = {:http\://arxiv.org/pdf/1404.7796v2:PDF},
  keywords         = {Machine Learning (stat.ML), Machine Learning (cs.LG), Multimedia (cs.MM), FOS: Computer and information sciences},
  modificationdate = {2023-04-28T08:48:29},
  primaryclass     = {stat.ML},
  publisher        = {arXiv},
}

@Article{Huang2020Multimodal,
  author           = {Shih-Cheng Huang and Anuj Pareek and Roham Zamanian and Imon Banerjee and Matthew P. Lungren},
  journal          = {Scientific Reports},
  title            = {Multimodal fusion with deep neural networks for leveraging {CT} imaging and electronic health record: a case-study in pulmonary embolism detection},
  year             = {2020},
  month            = dec,
  number           = {1},
  volume           = {10},
  creationdate     = {2023-04-28T08:49:08},
  doi              = {10.1038/s41598-020-78888-w},
  modificationdate = {2023-04-28T08:49:16},
  publisher        = {Springer Science and Business Media {LLC}},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: saveActions:enabled;
all-text-fields[identity]
date[normalize_date]
month[normalize_month]
pages[normalize_page_numbers]
;}

@article{Capitanio2016Feb,
	author = {Capitanio, Umberto and Montorsi, Francesco},
	title = {{Renal cancer}},
	journal = {Lancet},
	volume = {387},
	number = {10021},
	pages = {894--906},
	year = {2016},
	month = feb,
	issn = {1474-547X},
	publisher = {Elsevier Ltd},
	eprint = {26318520},
	doi = {10.1016/S0140-6736(15)00046-X}
}

@article {Cesano42,
	author = {Alessandra Cesano},
	title = {nCounter{\textregistered} PanCancer Immune Profiling Panel (NanoString Technologies, Inc., Seattle, WA)},
	volume = {3},
	number = {1},
	elocation-id = {42},
	year = {2015},
	doi = {10.1186/s40425-015-0088-7},
	publisher = {BMJ Specialist Journals},
	URL = {https://jitc.bmj.com/content/3/1/42},
	eprint = {https://jitc.bmj.com/content/3/1/42.full.pdf},
	journal = {Journal for ImmunoTherapy of Cancer}
}
