\chapter{Related Work}\label{related}

This section aims to give an overview over various neural network techniques that analyse whole slide images and perform survival analysis, while going into further detail for a few cases, where multimodal fusion models have been employed. Finally, some preliminary findings related to clear cell renal cell carcinoma are summarised. 

\section{Survival Analysis and Neural Networks}
The first adaption of survival analysis, based on the Cox proportional hazards model, dates back to 1995. Farragi and Simon used a network with a single hidden layer and varying numbers of hidden nodes to maximise a partial likelihood function derived from the CPH formulation. The C-Index was chosen as the performance metric. With only 3 covariates as input (age, weight, and stage of the disease), they were the first to overcome the linearity constraint of the CPH model, but were unable to significantly outperform the traditional model, achieving a c-index of 0.661. \cite{Faraggi1995neural}

The authors of DeepSurv claim to be the first to apply modern deep learning techniques to the Cox proportional hazards loss function. Unlike Farragi and Simon, they manage to outperform the linear CPH model as well as Random survival forests (RSF), a machine learning method, that had been adapted to survival analysis in the meantime. DeepSurv is a multilayer network that alternates between fully connected and dropout layers, with a linear layer that predicts the output of the log-risk function. Furthermore, they develop a treatment recommender system that uses DeepSurv's results. \cite{Katzman2018DeepSurv}

The DeepHit model is a multitask network in the sense that it can handle multiple potential causes of the event smoothly. This is done my partitioning the network in a shared and $K$ cause-specific subnetworks. 
The fully connected head of the network then performs a prediction based on the output of each subnetwork as well as on the original covariates. Additionally, they use the time-dependent concordance index ($C^{td}$ Index) for measuring the performance. In comparison with the ordinary concordance index, the $C^{td}$ index is capable of reevaluating the risk of an individual over time; the c-index is computed only at the initial time of observation. Ultimately, this achieves to lift the proportionality constraint of the classical CPH model. \cite{Lee2018DeepHit}

\section{Deep learning on WSIs}

Due to the high size of these images, it is unlikely that they will fit into GPU memory at full resolution. Hence, there are two general approaches to dealing with these images. 
Either they are downsampled to a lower resolution or they are divided into patches. In the latter case, the patches can then be processed separately or only a subset of them is used at all.

\subsection{Convolutional Neural Networks (CNN)}

The most straightforward approach is to use convolutional neural networks (CNN). In general, they are commonly made up of multiple blocks of convolutional, pooling and non-linear activation layers. This section's purpose is to extract features from the input. At the “head” of such a network, usually there are few fully connected layers. These sections aim to perform the actual task of the network, some form of prediction. However, these approaches are usually only effective when tissue-annotations are available at the time of training. One issue with the use of CNNs is that it is rather difficult to interpret the meaning of the extracted regions. \cite{Lipkova2022Artificial}
In contrast to CNNs, the following methods tend to not require strong supervision to work well and slide level annotations are sufficient. 

\subsection{Graph Convolutional Networks (GCN)}

A graph convolutional network can be constructed from parts of the images. These can be detected cells, tissue regions, or patches of the WSI. The distance of these objects is then used to define the length of the respective edges. \cite{Chen2022Pathomic, AhmedtAristizabal2022survey, Chen2021Whole}
This approach enables us to incorporate global context in the input to the network. In contrast to a regular CNN, these networks operate on unstructured graphs. 
\cite{Lipkova2022Artificial} Images can be considered graphs with 4 or 9 edges, depending on the definition.
Compared to downsampling the WSI for a CNN, we can choose the structure to be considered in the construction of the graph. Although the approach of construction is also a caveat of these networks, as it adds a collection of new hyperparameters that need to be tuned. 

 \subsection{Multiple Instance learning (MIL)}

In multiple instance learning, multiple inputs are grouped as 'bags'. The label to be predicted is assigned to the bag as a whole. It is to be noted that usually not all members of a bag have to conform to their bag's label. Such bags are sometimes referred to as 'negative' and the bags that only contain samples correctly associated are called 'positive'. A useful addition to this approach is to use an attention mechanism to determine the importance of patches toward their respective bag labels. \cite{Ilse2018Attention}
One such approach is introduced by Li \textit{et al.} in 2021. Here, the authors utilise the principle to detect tumours in image patches extracted from WSIs. They do this in a two-step process. The extracted patches are fed into a CNN to perform self-supervised contrastive learning. The pre-trained model is then re-used to obtain instance embedding of these patches. Interestingly, this is done twice, with the only difference being the scale of magnification of the WSIs. Embeddings obtained at 5x and 20x resolution are generated, each 20x patch has a corresponding lower resolution 5x patch. Then, each 20x embedding is concatenated to the corresponding 5x embedding and processed by the MIL aggregator. This aggregator consists of a masked non-local block and a max-pooling block. While the latter determines critical instances based on their individual scores, the former measures the distance of each bag-member to the critical instance and produces a bag embedding. Both aggregated and critical instances are individually scored, and their scores are then averaged. For classification, this approach seems to work very well. On the Camelyon16 dataset, an accuracy of 0.89 is reached. \cite{Lu2021Data}
This approach might not be suitable for survival analysis, since we would like to consider all the information in the image. For classification this might be sufficient because the slide class can be predicted based on a small subset of each WSI.

\subsection{Vision Transformers}

These models use attention-based learning. Context awareness allows accounting for correlations between image patches and thus learn positional encodings. After learning spatial structures of patches, usually there are self-attention layers to determine the importance of the individual patches. This can be followed by a Multi-head attention layer, which uses multiple self-attention blocks in parallel to account for different kinds of interactions between the patches.
This works by assigning multiple tokens to each patch, one for the positional information and another for the true outcome (e.g. class). Together with their respective tokens, these tokens are simultaneously used as input to the network. \cite{Lipkova2022Artificial} There are quite a few implementations of these kinds of networks already. \cite{Chen2022Scaling, Chen2021Multimodal, Dosovitskiy2020Image}

\subsection{Autoencoders}

Autoencoders are self-supervised models that do not need labels for training, they find a joint latent representation of the input. Thus, they present an alternative to direct modelling. \cite{Stahlschmidt2022Multimodal} The most simple variant consists of an Encoder and a Decoder. The Encoder is a CNN that learns a lower-dimensional representation of the input. The Decoder uses the inverse operations of the Encoder and learns to reproduce the original image using the latent code as input. Methods based on this simple principle can be used for various appliances: generating artificial training data \cite{Chen2022Fast}, recreating missing image patches based on their neighbouring patches \cite{Lipkova2022Artificial}, or generally improving the performance of downstream tasks that are heavily based on the performance of a CNN. 
Another way to use these methods is as a means of compression. Since the full WSIs are too big to be loaded at once, individual patches can be compressed and their embedding vectors are used for the downstream task instead. Tellez \textit{et al.} trained, among others, a Variational Autoencoder (VAE) and a Bidirectional Generative Adversarial Network to compress patches into 1-dimensional feature vectors. They then compressed these vectors into a 3D matrix, with each vector's position corresponding to that of the original image. With this new tensor, a CNN was trained to predict tumour metastasis and tumour proliferation speed. \cite{Tellez2019Neural}
Variational Autoencoders (VAE) provide significant improvement with little change. Instead of learning a direct representation of each input, by adding noise, the encoder learns to model the latent space as a Gaussian distribution. Hence, the network learns the mean and variance of the data. The Gaussian distribution characteristic is ensured by the Kullback–Leibler (KL) divergence, which is added to the regular reconstruction loss. The KL divergence measures the difference between latent space and a standard Gaussian distribution. \cite{Kingma2019Introduction}

\section{Multimodal deep learning}

As seen in the previous section, there are a variety of available techniques. Their composable nature leads to an even greater variety of pipelines that are employed in research, to the point that even exact replications of parts of a previous analysis are rare. Here, we look at some examples incorporating multimodal fusion.
\subsection{Siamese Network and Multimodal dropout} 
In 2019 Cheerla \textit{et al.} applied late fusion using four different datasets. These were gene expression data, miRNA data, clinical data and whole slide images. Moreover, they used three very different architectures, with only the subnetwork for gene expression and miRNA data sharing similarities. They calculate the colour balance of patches to remove the empty ones. Then, they select a subset of patches for each sample to be used for training. As not all modalities are available for each sample, they introduce what they call multimodal dropout. This technique trains the DNN to deal with missing modalities. By purposely removing whole modalities prior to fusion with probability $P=0.25$. This way, the network is forced to create representations that are robust to missing data. The fusion is guided by the sum of the pairwise similarity loss. Furthermore, by pre-training their fusion method using a loss inspired by the cosine similarity, they force same-patient feature representations from different modalities to resemble each other.
This work achieved a c-index of 0.73 for RCC cases from the TCGA-KIRC dataset, where the use of multimodal dropout did not affect this metric. TCGA-KIRC was the only one of the 20 Cancer data sets analysed where this was the case. Also, with only two exceptions, multimodal dropout improved the c-index by a one-digit percentage. It only decreased for TCGA-KICH and TCGA-THCA, but both already showed a very high c-index $\ge$ 0.90 which they managed to maintain despite their drop in performance.
Also, noticeable are the general variations in Performance on different TCGA cancer datasets. Although the TCGA-KICH dataset achieved a c-index of 0.93 (with multimodal dropout), for the TCGA-LUSC dataset only 66\% of predictions were concordant. The autors  found that molecular data alone already provides relatively high performance. \cite{Cheerla2019Deep}
\subsection{PathomicFusion} 
Another approach was proposed by Chen \textit{et al.} in 2020. They combined WSIs, genomic features and a graph constructed from the WSIs. The interesting parts are the use of Self-Normalizing Networks (SNN) for the processing of the genomic information, as well as the graph network. For the former, the network uses scaled exponential linear units (SeLU) over the more common ReLU as activation function. This has the effect that outputs are pushed towards zero mean and unit variance. In combination with alpha dropout, self-normalising property is maintained. The aim behind this setup is to lower the risk of overfitting, which would be quite high otherwise. 
The graph is constructed by detection and localisation of cells via nuclei segmentation. Then, using the K-nearest-neighbours algorithm, the edges were drawn to the five closest neighbours of each cell. The underlying assumption is that cell-cell interactions are most relevant between those that are adjacent. Each cell in the graph is then associated with up to 24 pre-defined features. This network is then used to learn a representation of the WSI. Obviously, this approach cannot contain the same information the WSI does, which is why both are used in the fusion. Fusion is handled by computing the Kronecker product to model pairwise feature interactions of all inputs. The resulting vector is then used for the Cox regression. In addition, the authors use GRAD-CAM \cite{Selvaraju2016Grad} and Integrated Gradients \cite{Sundararajan2017Axiomatic} to inspect the feature attribution of WSIs and genomic features respectively. \cite{Chen2022Pathomic}
\subsection{Multimodal Co-Attention Transformer (MCAT)} 
In 2021, Chen \textit{et al.} proposed another approach that utilizes early fusion between histology and genomic data. For this, both inputs are processed first by a CNN or fully connected layers respectively. This produces embeddings of genomic features and WSI patches. They are then processed in a co-attention mechanism guided by the genomic embeddings to learn pairwise interactions between the instance-level histology patches and genomic embeddings. Then a so-called set-based MIL transformer is used to obtain risk scores based on both embeddings. The co-attention mechanisms make visualisation of high- and low-risk cases possible, based on the attention score of each image patch. Heatmaps are overlaid onto the WSI, locating regions of high attention with granular detail. Although the approach seems very interesting, the c-indices for different datasets are mostly low. Although the Glioblastoma \& Lower Grade Glioma (GBMLGG) dataset scored a c-index of 0.82, none of the other four datasets surpassed 0.62. That being said, the authors compare their results to other MIL approaches and receive the best results with only one exception. \cite{Chen2021Multimodal}
Perhaps, MIL is not well suited for survival analysis in general, as these methods usually do not provide a “full picture” to the network and, unlike autoencoders, do not control the content of the latent space. 
\subsection{Long short-term memory (LTSM)} 
Ren \textit{et al.} use long short-term memory (LTSM) networks, a subtype of recurrent neural networks (RNN), to model the features of the multimodal input and for survival analysis on the recurrence-free survival (RFS) in months. For this, the authors incorporated ROI-extracted image patches and pathway activity data in a multimodal manner. Therefore, genomic data were first analysed with statistical methods to obtain so-called pathway scores. These were then fed together with the selected image patches into the neural network via simple concatenation of vectors, which then served as input into the LTSM. With this architecture, a c-index of 0.74 was reached. \cite{Ren2018Recurrence}
\subsection{MultiSurv} 
The MultiSurv model is another end-to-end multimodal network where the authors compare six different fusion methods. The feature vectors of each modality are fused using concatenation, row-wise summation, multiplication or taking the row-wise maximum. The last two approaches are more involved. First, an attention mechanism is used. This uses a fully connected linear layers followed by the hyperbolic tangent activation function and SoftMax rescaling. \cite{ValeSilva2021Long} Such a layer does not reduce the size of its input, but rather learns the importance of each node for the outcome of the downstream and balances them accordingly. The last approach is based on another publication. EmbraceNet uses so-called docking and embracement layers to fuse individual feature vectors. The docking layers take an output vector of an independent modality-specific network and resizes them to a common size. The embracement layers then pool individual values from each input to produce a vector of the same size as one of the input vectors. This is achieved by using the summed Hadamard product between a vector that has been preprocessed by the docking layers and a vector drawn from a multinomial distribution, this ensures that all values add up to one. The sum of such a Hadamard product then forms a single value in the final feature vector. \cite{Choi2019EmbraceNet}
Also, they use a discrete-time survival model to be able to adjust the predictions after one or more of these discrete time intervals have passed. This is done in order to remove the proportionality constraint 
On unimodal data, Multisurv manages to outperform the linear CPH model, RSFs, Deepsurv and DeepHit for a majority of modalities. Interestingly, the best results were achieved when not all six available modalities were used. In the author's direct comparison of MultiSurv and DeepSurv they achieved a $C^{td}$ index of 0.801, which is higher than the c-index of DeepSurv on the same dataset. \cite{ValeSilva2021Long}

\clearpage

\section{Clear cell renal cell carcinoma (ccRCC)}

Chen \textit{et al.} came to an interesting conclusion with respect to ccRCC. They found that a decrease in CYP3A7 expression as well as an increase in the expression of PITX2, DDX43 and XIST are correlated with the risk of developing cancer. Furthermore, they conclude that vasculature and cell atypia are important features for survival outcome prediction. They were able to show that cells with indiscernible nucleoli are more common in patients with longer survival, while large cells with clear nucleoli are indicative of patients with shorter survival. \cite{Chen2022Pathomic}

In 2015, Christinat \textit{et al.} used microRNA (miRNA) for classification and were able to identify subgroups of ccRCC. They found six distinct patient clusters, each with different clinical outcomes, pathological features and somatic mutation profiles associated to them. These groups were identified by applying the log-rank test to the miRNA expression levels. \cite{Christinat2015Integrated} This work illustrates the importance of tumour heterogeneity. Although we classify two patients as having the same disease, the cellular characteristics can be completely different. This nonlinearity threatens that miss-treatments occur and the overlook of individual factors. Thus, we should ask whether a more fine-grained classification of cancer types is needed to properly address the full spectrum.



