{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WSI registration\n",
    "\n",
    "methods to transform slide images of different stains that their coord systems overlay (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir_path: str = \"/home/sean/Desktop/MastherThesisDaten/DigiStrucMed_Braesen/Scans für QupathProjekt RCC (3 Fälle) 10062021/\"\n",
    "images: list[str] = []\n",
    "\n",
    "for file in os.listdir(dir_path):\n",
    "   filename = os.fsdecode(file)\n",
    "   if file.endswith(\".svs\"):\n",
    "      # print(os.fsencode(dir_path+filename))\n",
    "      images.append(os.fsencode(dir_path+filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cucim import CuImage\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "import numpy as np\n",
    "test_pics = []\n",
    "\n",
    "for image in images[0:3]:\n",
    "    pics.append(\n",
    "        np.asarray(\n",
    "            rgb2gray(\n",
    "            CuImage(image).read_region(level=3)\n",
    "            )))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cucim - Daisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cucim import CuImage\n",
    "# import pprint\n",
    "# import numpy as np\n",
    "# from cucim.skimage.feature import match_descriptors, plot_matches, daisy\n",
    "\n",
    "# import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img1, img2, img3 = test_pics\n",
    "# descriptor_extractor = SIFT()\n",
    "\n",
    "# descriptor_extractor.detect_and_extract(img1)\n",
    "# keypoints1 = descriptor_extractor.keypoints\n",
    "# descriptors1 = descriptor_extractor.descriptors\n",
    "\n",
    "# descriptor_extractor.detect_and_extract(img2)\n",
    "# keypoints2 = descriptor_extractor.keypoints\n",
    "# descriptors2 = descriptor_extractor.descriptors\n",
    "\n",
    "# # descriptor_extractor.detect_and_extract(img3)\n",
    "# # keypoints3 = descriptor_extractor.keypoints\n",
    "# # descriptors3 = descriptor_extractor.descriptors\n",
    "\n",
    "# matches12 = match_descriptors(descriptors1, descriptors2, max_ratio=0.6,\n",
    "#                               cross_check=True)\n",
    "# # matches13 = match_descriptors(descriptors1, descriptors3, max_ratio=0.6,\n",
    "#                             #   cross_check=True)\n",
    "# fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(11, 8))\n",
    "\n",
    "# plt.gray()\n",
    "\n",
    "# plot_matches(ax[0, 0], img1, img2, keypoints1, keypoints2, matches12)\n",
    "# ax[0, 0].axis('off')\n",
    "# ax[0, 0].set_title(\"Original Image vs. Flipped Image\\n\"\n",
    "#                    \"(all keypoints and matches)\")\n",
    "\n",
    "# # plot_matches(ax[1, 0], img1, img3, keypoints1, keypoints3, matches13)\n",
    "# # ax[1, 0].axis('off')\n",
    "# # ax[1, 0].set_title(\"Original Image vs. Transformed Image\\n\"\n",
    "# #                    \"(all keypoints and matches)\")\n",
    "\n",
    "# plot_matches(ax[0, 1], img1, img2, keypoints1, keypoints2, matches12[::15],\n",
    "#              only_matches=True)\n",
    "# ax[0, 1].axis('off')\n",
    "# ax[0, 1].set_title(\"Original Image vs. Flipped Image\\n\"\n",
    "#                    \"(subset of matches for visibility)\")\n",
    "\n",
    "# # plot_matches(ax[1, 1], img1, img3, keypoints1, keypoints3, matches13[::15],\n",
    "# #              only_matches=True)\n",
    "# ax[1, 1].axis('off')\n",
    "# ax[1, 1].set_title(\"Original Image vs. Transformed Image\\n\"\n",
    "#                    \"(subset of matches for visibility)\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-image Sift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ransac' from 'skimage.feature' (/home/sean/mambaforge/envs/monai/lib/python3.9/site-packages/skimage/feature/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mskimage\u001b[39;00m \u001b[39mimport\u001b[39;00m transform\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mskimage\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolor\u001b[39;00m \u001b[39mimport\u001b[39;00m rgb2gray\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mskimage\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature\u001b[39;00m \u001b[39mimport\u001b[39;00m match_descriptors, plot_matches, SIFT, ransac\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ransac' from 'skimage.feature' (/home/sean/mambaforge/envs/monai/lib/python3.9/site-packages/skimage/feature/__init__.py)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import match_descriptors, plot_matches, SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1, img2, img3 = test_pics\n",
    "descriptor_extractor = SIFT()\n",
    "\n",
    "descriptor_extractor.detect_and_extract(img1)\n",
    "keypoints1 = descriptor_extractor.keypoints\n",
    "descriptors1 = descriptor_extractor.descriptors\n",
    "\n",
    "descriptor_extractor.detect_and_extract(img2)\n",
    "keypoints2 = descriptor_extractor.keypoints\n",
    "descriptors2 = descriptor_extractor.descriptors\n",
    "\n",
    "# descriptor_extractor.detect_and_extract(img3)\n",
    "# keypoints3 = descriptor_extractor.keypoints\n",
    "# descriptors3 = descriptor_extractor.descriptors\n",
    "\n",
    "matches12 = match_descriptors(descriptors1, descriptors2, max_ratio=0.6,\n",
    "                              cross_check=True)\n",
    "# matches13 = match_descriptors(descriptors1, descriptors3, max_ratio=0.6,\n",
    "                            #   cross_check=True)\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(11, 8))\n",
    "\n",
    "plt.gray()\n",
    "\n",
    "plot_matches(ax[0, 0], img1, img2, keypoints1, keypoints2, matches12)\n",
    "ax[0, 0].axis('off')\n",
    "ax[0, 0].set_title(\"Original Image vs. Flipped Image\\n\"\n",
    "                   \"(all keypoints and matches)\")\n",
    "\n",
    "# plot_matches(ax[1, 0], img1, img3, keypoints1, keypoints3, matches13)\n",
    "# ax[1, 0].axis('off')\n",
    "# ax[1, 0].set_title(\"Original Image vs. Transformed Image\\n\"\n",
    "#                    \"(all keypoints and matches)\")\n",
    "\n",
    "plot_matches(ax[0, 1], img1, img2, keypoints1, keypoints2, matches12[::15],\n",
    "             only_matches=True)\n",
    "ax[0, 1].axis('off')\n",
    "ax[0, 1].set_title(\"Original Image vs. Flipped Image\\n\"\n",
    "                   \"(subset of matches for visibility)\")\n",
    "\n",
    "# plot_matches(ax[1, 1], img1, img3, keypoints1, keypoints3, matches13[::15],\n",
    "#              only_matches=True)\n",
    "ax[1, 1].axis('off')\n",
    "ax[1, 1].set_title(\"Original Image vs. Transformed Image\\n\"\n",
    "                   \"(subset of matches for visibility)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-image - Ransac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from skimage import data\n",
    "from skimage.util import img_as_float\n",
    "from skimage.feature import (corner_harris, corner_subpix, corner_peaks,\n",
    "                             plot_matches)\n",
    "from skimage.transform import warp, AffineTransform\n",
    "from skimage.exposure import rescale_intensity\n",
    "from skimage.measure import ransac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_orig = corner_peaks(corner_harris(img_orig_gray), threshold_rel=0.001,\n",
    "                           min_distance=5)\n",
    "coords_warped = corner_peaks(corner_harris(img_warped_gray),\n",
    "                             threshold_rel=0.001, min_distance=5)\n",
    "\n",
    "# determine sub-pixel corner position\n",
    "coords_orig_subpix = corner_subpix(img_orig_gray, coords_orig, window_size=9)\n",
    "coords_warped_subpix = corner_subpix(img_warped_gray, coords_warped,\n",
    "                                     window_size=9)\n",
    "\n",
    "\n",
    "def gaussian_weights(window_ext, sigma=1):\n",
    "    y, x = np.mgrid[-window_ext:window_ext+1, -window_ext:window_ext+1]\n",
    "    g = np.zeros(y.shape, dtype=np.double)\n",
    "    g[:] = np.exp(-0.5 * (x**2 / sigma**2 + y**2 / sigma**2))\n",
    "    g /= 2 * np.pi * sigma * sigma\n",
    "    return g\n",
    "\n",
    "\n",
    "def match_corner(coord, window_ext=5):\n",
    "    r, c = np.round(coord).astype(np.intp)\n",
    "    window_orig = img_orig[r-window_ext:r+window_ext+1,\n",
    "                           c-window_ext:c+window_ext+1, :]\n",
    "\n",
    "    # weight pixels depending on distance to center pixel\n",
    "    weights = gaussian_weights(window_ext, 3)\n",
    "    weights = np.dstack((weights, weights, weights))\n",
    "\n",
    "    # compute sum of squared differences to all corners in warped image\n",
    "    SSDs = []\n",
    "    for cr, cc in coords_warped:\n",
    "        window_warped = img_warped[cr-window_ext:cr+window_ext+1,\n",
    "                                   cc-window_ext:cc+window_ext+1, :]\n",
    "        SSD = np.sum(weights * (window_orig - window_warped)**2)\n",
    "        SSDs.append(SSD)\n",
    "\n",
    "    # use corner with minimum SSD as correspondence\n",
    "    min_idx = np.argmin(SSDs)\n",
    "    return coords_warped_subpix[min_idx]\n",
    "\n",
    "\n",
    "# find correspondences using simple weighted sum of squared differences\n",
    "src = []\n",
    "dst = []\n",
    "for coord in coords_orig_subpix:\n",
    "    src.append(coord)\n",
    "    dst.append(match_corner(coord))\n",
    "src = np.array(src)\n",
    "dst = np.array(dst)\n",
    "\n",
    "\n",
    "# estimate affine transform model using all coordinates\n",
    "model = AffineTransform()\n",
    "model.estimate(src, dst)\n",
    "\n",
    "# robustly estimate affine transform model with RANSAC\n",
    "model_robust, inliers = ransac((src, dst), AffineTransform, min_samples=3,\n",
    "                               residual_threshold=2, max_trials=100)\n",
    "outliers = inliers == False\n",
    "\n",
    "\n",
    "# compare \"true\" and estimated transform parameters\n",
    "print(\"Ground truth:\")\n",
    "print(f'Scale: ({tform.scale[1]:.4f}, {tform.scale[0]:.4f}), '\n",
    "      f'Translation: ({tform.translation[1]:.4f}, '\n",
    "      f'{tform.translation[0]:.4f}), '\n",
    "      f'Rotation: {-tform.rotation:.4f}')\n",
    "print(\"Affine transform:\")\n",
    "print(f'Scale: ({model.scale[0]:.4f}, {model.scale[1]:.4f}), '\n",
    "      f'Translation: ({model.translation[0]:.4f}, '\n",
    "      f'{model.translation[1]:.4f}), '\n",
    "      f'Rotation: {model.rotation:.4f}')\n",
    "print(\"RANSAC:\")\n",
    "print(f'Scale: ({model_robust.scale[0]:.4f}, {model_robust.scale[1]:.4f}), '\n",
    "      f'Translation: ({model_robust.translation[0]:.4f}, '\n",
    "      f'{model_robust.translation[1]:.4f}), '\n",
    "      f'Rotation: {model_robust.rotation:.4f}')\n",
    "\n",
    "# visualize correspondence\n",
    "fig, ax = plt.subplots(nrows=2, ncols=1)\n",
    "\n",
    "plt.gray()\n",
    "\n",
    "inlier_idxs = np.nonzero(inliers)[0]\n",
    "plot_matches(ax[0], img_orig_gray, img_warped_gray, src, dst,\n",
    "             np.column_stack((inlier_idxs, inlier_idxs)), matches_color='b')\n",
    "ax[0].axis('off')\n",
    "ax[0].set_title('Correct correspondences')\n",
    "\n",
    "outlier_idxs = np.nonzero(outliers)[0]\n",
    "plot_matches(ax[1], img_orig_gray, img_warped_gray, src, dst,\n",
    "             np.column_stack((outlier_idxs, outlier_idxs)), matches_color='r')\n",
    "ax[1].axis('off')\n",
    "ax[1].set_title('Faulty correspondences')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "80db6464d06f8e5c620ae59b6e0c808104b7a57447c875540741f5d1460ee201"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
