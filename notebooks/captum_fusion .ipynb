{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.networks.pathology import CoxResNet\n",
    "\n",
    "\n",
    "model = CoxResNet(input_channels=8)\n",
    "\n",
    "# load model from file\n",
    "model.load_state_dict(torch.load('/data2/projects/DigiStrudMed_sklein/huggingface/2023-04-05_None_no_scheduler_CI.pth'))\n",
    "# model.load_state_dict(torch.load('/data2/projects/DigiStrudMed_sklein/huggingface/2023-04-05_None_no_scheduler_CPH.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from captum.attr import LayerIntegratedGradients, MultiInputGradientShap\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "# Define your multimodal model that takes in a dictionary of inputs\n",
    "class MultimodalModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.image_model = models.resnet18(pretrained=True)\n",
    "        self.fc1 = torch.nn.Linear(1000+1, 512)\n",
    "        self.fc2 = torch.nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        img = inputs['image']\n",
    "        num = inputs['numerical']\n",
    "        img_feat = self.image_model(img)\n",
    "        combined_feat = torch.cat((img_feat, num), dim=1)\n",
    "        out = torch.relu(self.fc1(combined_feat))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Create an instance of your multimodal model\n",
    "model = MultimodalModel()\n",
    "\n",
    "# Define your input dictionary\n",
    "inputs = {\n",
    "    'image': torch.randn(1, 3, 224, 224), # batch size x channels x height x width\n",
    "    'numerical': torch.tensor([0.5]) # a single numerical value\n",
    "}\n",
    "\n",
    "# Create an instance of the LayerIntegratedGradients algorithm for the image input\n",
    "lig = LayerIntegratedGradients(model, model.image_model.conv1)\n",
    "\n",
    "# Compute the attribution scores for the image input\n",
    "attr_image = lig.attribute(inputs['image'], target=0)\n",
    "\n",
    "# Create an instance of the MultiInputGradientShap algorithm for the multimodal inputs\n",
    "migs = MultiInputGradientShap(model)\n",
    "\n",
    "# Compute the attribution scores for the multimodal inputs\n",
    "attr_multimodal = migs.attribute(inputs=(inputs['image'], inputs['numerical']), target=0)\n",
    "\n",
    "# Visualize the attribution scores for the image input\n",
    "viz.visualize_image_attr(attr_image.squeeze(), inputs['image'].squeeze())\n",
    "\n",
    "# Visualize the attribution scores for the multimodal inputs\n",
    "viz.visualize_image_attr_multiple(attr_multimodal[0].squeeze(), attr_multimodal[1].squeeze(), ['image', 'numerical'], inputs['image'].squeeze())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklein",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
